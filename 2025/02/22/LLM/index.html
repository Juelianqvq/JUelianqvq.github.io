<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="– [1.LLM](# 1.LLM) – [2.vLLM](# 2.vLLM) – [3.Attention 优化](# 3.Attention 优化) – [4.TensorRT-LLM](# 4.TensorRT-LLM) – [5.Coroutine &amp; Thread &amp; Process](# 5.Coroutine &amp; Thread &amp; Process) –">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM">
<meta property="og:url" content="http://example.com/2025/02/22/LLM/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="– [1.LLM](# 1.LLM) – [2.vLLM](# 2.vLLM) – [3.Attention 优化](# 3.Attention 优化) – [4.TensorRT-LLM](# 4.TensorRT-LLM) – [5.Coroutine &amp; Thread &amp; Process](# 5.Coroutine &amp; Thread &amp; Process) –">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/home/kwx/blog/source/_posts/assets/v2-00480748742764c41b4d2ce2463a0ac6_r.jpg">
<meta property="article:published_time" content="2025-02-22T11:58:23.000Z">
<meta property="article:modified_time" content="2025-02-22T12:08:59.041Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/home/kwx/blog/source/_posts/assets/v2-00480748742764c41b4d2ce2463a0ac6_r.jpg">

<link rel="canonical" href="http://example.com/2025/02/22/LLM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>LLM | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/LLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:58:23 / Modified: 20:08:59" itemprop="dateCreated datePublished" datetime="2025-02-22T19:58:23+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>– [1.LLM](# 1.LLM)</p>
<p>– [2.vLLM](# 2.vLLM)</p>
<p>– [3.Attention 优化](# 3.Attention 优化)</p>
<p>– [4.TensorRT-LLM](# 4.TensorRT-LLM)</p>
<p>– [5.Coroutine &amp; Thread &amp; Process](# 5.Coroutine &amp; Thread &amp; Process)</p>
<p>– [3.Speculative Decoding](# 3.Speculative Decoding)</p>
<p>– [4.Disaggregated Inference](# 4.Disaggregated Inference)</p>
<p>– [6.Triton](# 6.Triton)</p>
<p>– [7.CUDA](# 7.CUDA)</p>
<p>– [8.Quantization](# 8.Quantization)</p>
<p>– [9.DiT](# 9.DiT)</p>
<p>Deepseek：</p>
<p>什么水平：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/10666202502/answer/89907687711">https://www.zhihu.com/question/10666202502/answer/89907687711</a></p>
<p>直观参数量：<a target="_blank" rel="noopener" href="https://yangwenbo.com/articles/deepseek-v3-parameter-size.html">https://yangwenbo.com/articles/deepseek-v3-parameter-size.html</a></p>
<p>EP TP 通信量：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/13997146226">https://zhuanlan.zhihu.com/p/13997146226</a></p>
<p>通信分析 + 训练时 grouped gemm 优化建议</p>
<p>ring_reduce &#x3D; reduce_scatter + all_gather：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/617133971">https://zhuanlan.zhihu.com/p/617133971</a></p>
<p>所以 TP 的同步 all reduce 需要先从每张卡上求和得到自己卡上的数据，再同步其余卡上的求和结果</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/16704076805">https://zhuanlan.zhihu.com/p/16704076805</a></p>
<p>SGL walk through：</p>
<p>![截图 2025-02-22 20-02-56](&#x2F;home&#x2F;kwx&#x2F;blog&#x2F;source&#x2F;_posts&#x2F;assets&#x2F;截图 2025-02-22 20-02-56.png)</p>
<p>P:D &#x3D; 1:10：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/8531371805/answer/70643107756">https://www.zhihu.com/question/8531371805/answer/70643107756</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/8531371805/answer/80649923584">https://www.zhihu.com/question/8531371805/answer/80649923584</a></p>
<p> 显存会先于算力达到瓶颈点，而且是显存带宽，而非显存容量</p>
<p>什么是 Grouped GEMM：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/484319691">https://zhuanlan.zhihu.com/p/484319691</a></p>
<p>部署提炼：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23640350617">https://zhuanlan.zhihu.com/p/23640350617</a></p>
<p>FP8：</p>
<p><img src="/home/kwx/blog/source/_posts/assets/v2-00480748742764c41b4d2ce2463a0ac6_r.jpg" alt="v2-00480748742764c41b4d2ce2463a0ac6_r"></p>
<p>Triton：</p>
<p>如果调用 JIT 编译的函数入参具有 .data_ptr() 方法和 .dtype 属性，则会隐式转换为指针。</p>
<p>exp2：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/6874512980/answer/85865475090">https://www.zhihu.com/question/6874512980/answer/85865475090</a> 参看其他回答可能还能防止精度下溢</p>
<p>指针块：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/639423790">https://zhuanlan.zhihu.com/p/639423790</a></p>
<h4 id="1-LLM"><a href="#1-LLM" class="headerlink" title="1.LLM"></a>1.LLM</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>special tokens：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/17052593700">https://zhuanlan.zhihu.com/p/17052593700</a></p>
<p>TPS QPS 并发数 吞吐量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337708438">https://zhuanlan.zhihu.com/p/337708438</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">并发数 = QPS*平均响应时间</span><br></pre></td></tr></table></figure>

<p>采样：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631847634">https://zhuanlan.zhihu.com/p/631847634</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653926703">https://zhuanlan.zhihu.com/p/653926703</a></p>
<p>分词：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/652520262">https://zhuanlan.zhihu.com/p/652520262</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Scaling law： 假设transformer参数量为N，数据集tokens个数为D，那么模型的计算量C≈6N*D </span><br><span class="line">C确定后，模型性能就基本确定，它的决策变量只有N和D，跟模型的具体结构诸如层数、 深度、 attention头个数（宽度）基本无关，相关性非常小，性能在2%的区间内</span><br></pre></td></tr></table></figure>

<p>encoder decoder：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/427311331">https://zhuanlan.zhihu.com/p/427311331</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoder： 给一个向量，输出一个同样长度的向量</span><br><span class="line">decoder： 输出一个向量，长度为 词表 长度，该向量是一个概率分布，表示取得对应词的概率。</span><br><span class="line">自回归解码器 将解码器自己当前步的输出加入下一步的输入，融合所有已经输入的向量来输出下一个向量，越往后的输出考虑了越多输入，在一定程度上自己预测自己</span><br><span class="line">解码器和编码器的另一个区别是：解码器block的第一个自注意力是 masked MHA</span><br><span class="line">训练阶段，其输出序列的所有位置（时间步）的标记都是已知的；但预测阶段，其输出序列的标记是逐个生成的。因此，在任何解码器时间步中，只有生成的标记才能用于解码器的自注意力计算中</span><br></pre></td></tr></table></figure>

<p>Decoder only 完美回答：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3357252612">https://www.zhihu.com/question/588325646/answer/3357252612</a></p>
<p>Deepnorm： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/480783670">https://zhuanlan.zhihu.com/p/480783670</a></p>
<p>参数微调： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/635710004">https://zhuanlan.zhihu.com/p/635710004</a></p>
<p>位置编码性质：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662489503">https://zhuanlan.zhihu.com/p/662489503</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">正余弦交替的位置编码只与偏移量k有关，这意味着两个正弦位置嵌入的点乘可以反应两个tokens的距离，且该距离对称</span><br></pre></td></tr></table></figure>

<p>Left padding： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646852375">https://zhuanlan.zhihu.com/p/646852375</a></p>
<p>bos_token： <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/tokenizer">https://huggingface.co/docs/transformers/main_classes/tokenizer</a></p>
<p>cpu &amp; tensor core 算力估算： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661062002">https://zhuanlan.zhihu.com/p/661062002</a></p>
<p>Gpipe： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613196255">https://zhuanlan.zhihu.com/p/613196255</a> 流水线并行 压缩空闲气泡 切分更细碎的batch</p>
<p>forward的时候不存中间结果 bp 的时候重新计算</p>
<p>ZeRO： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/618865052">https://zhuanlan.zhihu.com/p/618865052</a> 数据并行 ZeRO 优化器 梯度 模型参数 分散各卡 减少通讯</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Prefix LM：前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务。</span><br><span class="line">Causal LM：因果语言模型是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于文本生成、语言建模等任务。</span><br></pre></td></tr></table></figure>

<p>Layernorm + Linear： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692679244">https://zhuanlan.zhihu.com/p/692679244</a></p>
<p>BN vs LN： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/395811291/answer/3103148309">https://www.zhihu.com/question/395811291/answer/3103148309</a></p>
<p>BN 是 所有 token 的每个特征 ；LN 是每个 token 的所有特征</p>
<p>CUDA Core or Tensor Core： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/636533414/answer/3345355574">https://www.zhihu.com/question/636533414/answer/3345355574</a></p>
<p>MFU &amp; HFU： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671537309">https://zhuanlan.zhihu.com/p/671537309</a></p>
<p>NCCL 环境变量：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/598730156/answer/3187578933">https://www.zhihu.com/question/598730156/answer/3187578933</a></p>
<p>NVLink 提升：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/654832546/answer/71647384740">https://www.zhihu.com/question/654832546/answer/71647384740</a></p>
<p>4090 快于 A100：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/615946801/answer/90445392759">https://www.zhihu.com/question/615946801/answer/90445392759</a></p>
<p>避免隐式 CUDA 同步：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/67209417/answer/3459890650">https://www.zhihu.com/question/67209417/answer/3459890650</a></p>
<p>A800 训练不收敛： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701623664">https://zhuanlan.zhihu.com/p/701623664</a></p>
<p>昇腾精度 debug：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/15940824171">https://zhuanlan.zhihu.com/p/15940824171</a></p>
<p>千卡超时：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/712505336">https://zhuanlan.zhihu.com/p/712505336</a></p>
<p>continuous batching： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688551989">https://zhuanlan.zhihu.com/p/688551989</a></p>
<p>bandwidthTest：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/703039225">https://zhuanlan.zhihu.com/p/703039225</a></p>
<p>Pinned &amp; Pageable memory：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/662795350/answer/3576429708">https://www.zhihu.com/question/662795350/answer/3576429708</a></p>
<p>面经：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658609960">https://zhuanlan.zhihu.com/p/658609960</a> （Nvidia）</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/634557901">https://zhuanlan.zhihu.com/p/634557901</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663917237">https://zhuanlan.zhihu.com/p/663917237</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/716021841">https://zhuanlan.zhihu.com/p/716021841</a></p>
<p>sft 细节：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/6497090767">https://zhuanlan.zhihu.com/p/6497090767</a></p>
<p>O1 复现：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/8102196012">https://zhuanlan.zhihu.com/p/8102196012</a></p>
<h5 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h5><ul>
<li><p>split k： <a target="_blank" rel="noopener" href="https://blog.csdn.net/u013701860/article/details/128674224">https://blog.csdn.net/u013701860/article/details/128674224</a></p>
</li>
<li><p>Cache miss &amp; 分块矩阵一致性： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342923482">https://zhuanlan.zhihu.com/p/342923482</a></p>
</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为a[0][0]本身就不在缓存中，所以肯定有一次 cache miss 啊</span><br></pre></td></tr></table></figure>

<p>row&#x2F;col parallel &amp; tp&#x2F;dp 通讯量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/622212228">https://zhuanlan.zhihu.com/p/622212228</a></p>
<p>int4 分块 &amp; fp8 模拟 ： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653735572">https://zhuanlan.zhihu.com/p/653735572</a></p>
<p>LLM 部署代价评估： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658868628">https://zhuanlan.zhihu.com/p/658868628</a></p>
<p>Transformer 时空复杂度： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/606514058/answer/3078324182">https://www.zhihu.com/question/606514058/answer/3078324182</a></p>
<p>Transformer 参数量计算量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a></p>
<p>手推 FLOPs 和 50% 利用率下的推理时间：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648988727">https://zhuanlan.zhihu.com/p/648988727</a></p>
<p>RoPE： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642884818">https://zhuanlan.zhihu.com/p/642884818</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">假设Ra表示角度为a的旋转矩阵，那么R具有如下性质：</span><br><span class="line">1. Ra^T = R(-a)</span><br><span class="line">2. Ra Rb = R(a+b)</span><br><span class="line"></span><br><span class="line">回到旋转位置编码，我们可以去证明 &lt;RaX, RbY&gt; = &lt;X, R(b-a)Y&gt; ，证明如下：</span><br><span class="line">&lt;RaX, RbY&gt;</span><br><span class="line">= (RaX)^T RbY</span><br><span class="line">= X^T Ra^T RbY</span><br><span class="line">= X^T R(b-a) Y</span><br><span class="line">= &lt;X, R(b-a)Y&gt;</span><br></pre></td></tr></table></figure>

<p>KV Cache 清晰图解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p>
<p>KV Cache 优化 MQA GQA CLA： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/697311739">https://zhuanlan.zhihu.com/p/697311739</a></p>
<p>为什么没 Q Cache： Q 只在当前 token 的计算中有用到 而 KV 在后续所有 token 的推计算中都得用</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/653658936/answer/3545520807">https://www.zhihu.com/question/653658936/answer/3545520807</a></p>
<p>KV Cache 稀疏化： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701580870">https://zhuanlan.zhihu.com/p/701580870</a></p>
<p>KV Cache 压缩评测： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/709684237">https://zhuanlan.zhihu.com/p/709684237</a></p>
<p>fa变长使用： <a target="_blank" rel="noopener" href="https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/">https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/</a></p>
<p>量化：</p>
<p>GPTQ 推导：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692338716">https://zhuanlan.zhihu.com/p/692338716</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690834228">https://zhuanlan.zhihu.com/p/690834228</a></p>
<p>AutoGPTQ bug： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/680047456">https://zhuanlan.zhihu.com/p/680047456</a></p>
<p>AWQ一看就会： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/685867596">https://zhuanlan.zhihu.com/p/685867596</a></p>
<h4 id="2-CUDA"><a href="#2-CUDA" class="headerlink" title="2.CUDA"></a>2.CUDA</h4><p>cuobjdump nvdisasm cu++filt nvprune：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html</a></p>
<p>CUDA 全局坐标运算： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/675603584">https://zhuanlan.zhihu.com/p/675603584</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x,y,z三维坐标系，先确定z，x*y是平面面积大小</span><br></pre></td></tr></table></figure>

<p>多维数组和一维数组的映射关系指的是当出现 threadIdx.y&#x2F;z 的时候映射回线程块内的线程索引 tid： $tid &#x3D; threadIdx.z \times blockDim.x \times blockDim.y +threadIdx.y \times blockDim.x + threadIdx.x $</p>
<p>双重指针含义： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648578880">https://zhuanlan.zhihu.com/p/648578880</a></p>
<p>不注水的显卡指标： <a target="_blank" rel="noopener" href="https://moonshot.feishu.cn/sheets/OaBLsKXazhMXjLt9dw5cC4Gvnzc?sheet=1357cc">https://moonshot.feishu.cn/sheets/OaBLsKXazhMXjLt9dw5cC4Gvnzc?sheet=1357cc</a></p>
<p>ptx sass code arch： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673833765">https://zhuanlan.zhihu.com/p/673833765</a></p>
<p>优化思路：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/356661099/answer/2449633440">https://www.zhihu.com/question/356661099/answer/2449633440</a></p>
<p>while (0)： <a target="_blank" rel="noopener" href="https://blog.csdn.net/dldw8816/article/details/86519575">https://blog.csdn.net/dldw8816/article/details/86519575</a></p>
<p>单线程 print 发生了什么： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/658620196/answer/3527314467">https://www.zhihu.com/question/658620196/answer/3527314467</a></p>
<p>浮点移位对齐： <a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv130411/">https://www.bilibili.com/read/cv130411/</a></p>
<p>__ldg： 只能用于读取全局内存中的单个数据，不能用于读取数组或结构体中的数据</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42536162/article/details/129892382">https://blog.csdn.net/qq_42536162/article/details/129892382</a></p>
<p>位运算取余： <a target="_blank" rel="noopener" href="https://perkins4j2.github.io/posts/19120/">https://perkins4j2.github.io/posts/19120/</a></p>
<p><em>_restrict</em>_： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28090573/article/details/89497788">https://blog.csdn.net/qq_28090573/article/details/89497788</a></p>
<p>cuda 编译： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690717002">https://zhuanlan.zhihu.com/p/690717002</a></p>
<p>float4 作用： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/574968879/answer/3005751704">https://www.zhihu.com/question/574968879/answer/3005751704</a></p>
<p>grid block size： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688610975">https://zhuanlan.zhihu.com/p/688610975</a></p>
<p><strong>Instruction：</strong></p>
<p>ld.cg：直接从 L2 缓存中读取数据，不会访问 L1 缓存，L1 cache hit rate &#x3D; 0</p>
<p><strong>cuda-mode：</strong></p>
<p>class 1 profile 概述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706469164">https://zhuanlan.zhihu.com/p/706469164</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load_inline</span><br><span class="line">@triton.jit(interprete=True)</span><br></pre></td></tr></table></figure>

<p><strong>Profile：</strong></p>
<p>简单访存效率对比示例： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660559591">https://zhuanlan.zhihu.com/p/660559591</a></p>
<p>warp 内产生bank conflict 造成的负面影响，要大于单 thread 内 cache miss 造成的影响</p>
<p>warp stall： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646523089">https://zhuanlan.zhihu.com/p/646523089</a></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464172074">https://zhuanlan.zhihu.com/p/464172074</a></p>
<p>warp state statistics：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671741797">https://zhuanlan.zhihu.com/p/671741797</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Stall Long Scoreboard：warp等待数据在L1TEX中的记分牌依赖（local，global，surface，tex）而停顿的平均周期数。为了减少等待L1TEX数据访问的周期数，可以考虑做的优化有：1）验证核函数的内存访问模式是否在目标GPU架构上最优；2）尝试增大cache命中率来提高数据局部性；3）改变L1 cache配置，考虑将频繁使用的数据移入SMEM。</span><br><span class="line">注：L1TEX即L1 Cache，可以认为它包括L1D，L1T等。</span><br><span class="line">注：Global memory是49-bit的虚拟地址，对GPU中的所有线程可见，它缓存在SM L1和GPU L2中。</span><br><span class="line">Stall Short Scoreboard：warp等待记分牌依赖MIO(memory input/output)操作(不会访问L1TEX)而等停顿的周期数。主要原因是启用SMEM并且发生了bank争用，次要原因包括频繁地执行特殊算数指令或动态分支指令。</span><br><span class="line">Stall Wait：warp等待固定延迟的执行依赖而停顿的周期数。可以考虑做的优化有：增大活跃的warp数。</span><br><span class="line">注：推测这种情况下访问的数据是在寄存器或SMEM(没有发生bank争用)中，所以可以认为是固定延迟。</span><br><span class="line">注：warp发生执行依赖只与它的前一条指令有关。</span><br><span class="line">Stall Branch Resolving：warp等待计算分支目标地址和更新warp PC而停顿的周期数。过多的分支指令可能导致这种停顿。</span><br><span class="line">Selected：warp调度器从warp池选择一条warp指令发射所需的周期数，固定为1个周期。</span><br><span class="line">Stall No Instruction：warp等待取指或等待ICache miss而停顿的周期数。</span><br><span class="line">注：wave：在给定GPU上并发运行的CTA总数称为wave。wave的大小与GPU可用SM数量以及核函数的占用率成比例关系。</span><br></pre></td></tr></table></figure>

<p>nsight compute 解读：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707107808">https://zhuanlan.zhihu.com/p/707107808</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/709873278">https://zhuanlan.zhihu.com/p/709873278</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">SOL：</span><br><span class="line">1.看内存吞吐量和计算吞吐量，表明计算密集or访存密集？</span><br><span class="line">2.L1/L2吞吐量优化空间</span><br><span class="line">3.DRAM吞吐量和总体内存吞吐量一致的话，表明主要内存操作是直接与DRAM交互</span><br><span class="line"></span><br><span class="line">Memory Workload Analysis：</span><br><span class="line">1.memory throughput：访问 DRAM 的字节数 </span><br><span class="line">2.mem busy：缓存和 DRAM 内部活动的吞吐量，峰值持续速率的百分比</span><br><span class="line"></span><br><span class="line">Source Counters：</span><br><span class="line">1.分支指令</span><br><span class="line">2.未合并的全局访问</span><br><span class="line">3.warp stall sampling：warp 停滞的主要原因和位置</span><br><span class="line"></span><br><span class="line">Warp State Statistics：</span><br><span class="line">某种 stall 状态下消耗的周期数越多，影响性能的可能性就越大</span><br><span class="line"></span><br><span class="line">Compute Workload Analysis</span><br><span class="line"></span><br><span class="line">Launch Statistics：观察有无 tail effect</span><br><span class="line"></span><br><span class="line">Scheduler Statitics：对 warp stall 现象的解释</span><br><span class="line">1.每个调度器每个周期都可以发出一条指令</span><br><span class="line">2.每个调度器最多分配12个warps，内核分配了active warps per scheduler，每个周期里只有 eligible 是可以发出指令的</span><br><span class="line">2.提高可以发出指令的warps数量，减少活跃warps阻塞时间，参看Warp State Statistics &amp; Source Counters</span><br><span class="line"></span><br><span class="line">Occupancy：</span><br><span class="line">1.Achieved active warps per SM / Theoretical active warps per SM</span><br><span class="line">2.block size / smem size / reg per thread 对性能的影响</span><br></pre></td></tr></table></figure>

<p>ncu 各参数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673770855">https://zhuanlan.zhihu.com/p/673770855</a></p>
<p>nsys nvtx：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1909450324">https://zhuanlan.zhihu.com/p/1909450324</a></p>
<p>nsys cuda kernel timeline：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/691307737">https://zhuanlan.zhihu.com/p/691307737</a></p>
<p>nsys ncu 使用流程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/721868054">https://zhuanlan.zhihu.com/p/721868054</a></p>
<p>nsys profile –gpu-metrics-device&#x3D;0 -t cuda,nvtx –cuda-memory-usage&#x3D;true .&#x2F;sgemm</p>
<p>ncu -f –set full –target-processes all -o profile .&#x2F;sgemm –kernel-name Sgemmv3</p>
<p><strong>Reduce 优化：</strong></p>
<p>Reduce 优化： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">https://zhuanlan.zhihu.com/p/654027980</a></p>
<p>深入浅出优化 Reduce： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">https://zhuanlan.zhihu.com/p/426978026</a></p>
<p>BBuf 细致解读： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688610091">https://zhuanlan.zhihu.com/p/688610091</a></p>
<p>blockReduceSum： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/584936904">https://zhuanlan.zhihu.com/p/584936904</a></p>
<p>warpReduce：All Thread + shared[WarpID]： LaneID&#x3D;0 只需要每个线程束里的第一个线程来存取</p>
<p>&amp; warpReduce： WarpID&#x3D;0 只需要32个线程就够了</p>
<p><strong>SGEMM 优化：</strong></p>
<p>Gemm 优化精讲：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/584236348">https://zhuanlan.zhihu.com/p/584236348</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">访存覆盖：block tiling，局部性原理</span><br><span class="line">1.避免内存访问瓶颈的内存访问带宽1：dtype Tflops / 计算访存比</span><br><span class="line">2.避免内存访问瓶颈的加权内存访问带宽2，假设 L2 cache 命中率 20%：</span><br><span class="line">0.8 * DRAM bandwidth + 0.2 * L2 bandwidth</span><br><span class="line">3.更大的分块需要用到更多的寄存器，一个SM允许使用的寄存器数量和线程数数量是固定的，一个SM能够同时承载的线程数就会减少 e.g. 65536 / 1536 = 42</span><br><span class="line">4.occupancy指的是每个SM能够同时调度的线程数量除以一个SM最大可调度的线程数量</span><br><span class="line">一个warp stall时，就会切换到另一个warp执行指令，occupancy越低可供切换的线程就越少</span><br><span class="line">但较高的occupancy不意味着高性能，每个线程本身就能通过更多的寄存器来达到延迟覆盖的目的，也就不需要SM来做这</span><br><span class="line">件事了，反倒是一些线程内的延迟甚至无法被SM切换所覆盖而得不偿失</span><br><span class="line"></span><br><span class="line">延迟覆盖：thread 级优化</span><br><span class="line">1.向量内积 or 外积的选择，外积搭配寄存器的细节，double buffer需要外积</span><br><span class="line">2.(load from g -&gt; store2s) g2s -&gt; sync -&gt; s2r -&gt; reg ffma -&gt; sync</span><br><span class="line"></span><br><span class="line">访存覆盖：warp 级优化 </span><br><span class="line">1.4*8 or 8*4 合并访问，wrap的m和n越接近，计算访存比越大，减少memory transaction</span><br><span class="line">2.float4</span><br><span class="line">3.一个warp搬运连续地址</span><br><span class="line"></span><br><span class="line">提升 L2 cache 命中率：block 级优化</span><br><span class="line">1.wav &amp; L2 cache hit rate 的计算，注意wave_gpu与wav_m,wav_n不可约</span><br><span class="line">2.wav_m, wav_n计算还不太明白</span><br><span class="line">假设一个wave是一横条，那么wave_m=block_m=128，wave_n=block_n*wave_gpu=128*46*2。46是sm个数，2是一个sm能跑的block数</span><br><span class="line">A的一行B的一列，横条wav，所以A就是wavm,blockm,B就相当于是最大wav乘以blockn</span><br><span class="line">3.横条变竖条后的 L2 cache miss 也还不明白</span><br><span class="line">需要明白到底哪种更好，图例到底指的是哪种</span><br><span class="line"></span><br><span class="line">内积不适合预取</span><br></pre></td></tr></table></figure>

<p>Gemm 优化指标：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687176254">https://zhuanlan.zhihu.com/p/687176254</a></p>
<p>一共 $m\times n$ 次向量乘法操作，每个向量乘进行 $k$ 次 FMA 操作，每次 FMA 需要一对乘加两个 op，对于 FP16，</p>
<p>$Arithmetic\space Intensity &#x3D; \frac{num\space of\space FLOPs}{byte\space accesses}&#x3D;\frac{2\cdot M\cdot N\cdot K}{2\cdot (M\cdot N+M\cdot K+N\cdot K)} $，易得 GEMV 时 AI &lt; 1，memory bound</p>
<p>vs cublas：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/518857175">https://zhuanlan.zhihu.com/p/518857175</a></p>
<p>配套代码： <a target="_blank" rel="noopener" href="https://github.com/nicolaswilde/cuda-sgemm">https://github.com/nicolaswilde/cuda-sgemm</a></p>
<p>register 单拍即可访问，smem 大约 20-30 拍，gmem 300 多拍</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以 A100 为例，网上的测评给出的访问 shared memory 的 latency 大概是 22.4 ns，GPU 时钟频率是 1.5GHz，算下来单次访问就是 22.4 / (1 / 1.5GHz) = 33.6 cycle</span><br></pre></td></tr></table></figure>

<p>每个时钟周期，SM 对应的若干个 block 都会尝试选择一个可以调度的 warp 去发射执行</p>
<p>naive 需要 M*N 个线程和 K 次乘累加，两次 load gmem 才能完成一次乘法运算</p>
<p>一共需要 M * N * 2K 次对 gmem 的访问，2 指的是 gmem 中的两个数，计算访存比低，数据复用性差</p>
<p>对 smem 分块后仅需 $$M &#x2F; BM * N &#x2F; BN * K &#x2F; BK * (BM * BK + BN * BK) &#x3D; (\frac{1}{BM}+\frac{1}{BN})*MNK $$</p>
<p>每个 block 一次从 gmem 中读取 1024A 和1024 B，每个线程 1 次读 4A 4B</p>
<p>gmem 搬运到 smem &amp; 计算各需要一次 syncthreads，以防没算完执行到下一次循环修改了 smem 中对应数据</p>
<p>对 bank conflict 的分析参考： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683465339">https://zhuanlan.zhihu.com/p/683465339</a></p>
<p>一个 smem 分为 32 bank，可以容纳 32 个int32 &#x2F; fp32，尽可能让同一个线程取到同一列 bank</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">smem到register也可以做double buffer，但看SASS，编译器已经自动优化了</span><br><span class="line">处理器核发射完访存请求，就会继续执行后面的指令，直到碰到scoreboard机制处理hazard，这是编译器设置在指令编码里发下去的，SASS中需要手动设置</span><br><span class="line">gmem和smem的访问都是可以复用的，不需要每次都去gmem或smem里面取数。对于sgemm这种用cuda core计算的，从寄存器堆里面取两个32-bit数可以做1次计算，一个sm的subcore一拍可以取128个数（32threads*4banks）但是只能算16次计算，所以限制性能的是cuda core的数目，只要cuda程序写得好访存带宽占用是比较低的</span><br><span class="line">因为在gemm这个负载下，一个SM里只有一个thread block，而且这个thread block里的所有线程在计算所需的数据在shared memory中准备好之后需要barrier的，所以warp scheduler没啥用</span><br><span class="line">smem是32个bank，每个bank数据宽度是32 bits，所以在没有bank conflict的情况下一拍只能出128B的数</span><br><span class="line">gpu SM processing block是顺序的处理器核，不会像CPU核那样可以乱序执行。gpu里碰到数据相关是一定会阻塞的，例如现在有三条指令：load r0 addr，add r1 r0 r0，load r2 addr，第一条load发出去之后，第二条add跟第一条load有写后读的真相关，所以第二条add不会发射，那么第三条load虽然没有相关可以发射，但是在顺序流水线里第三条load也不会跨过第二条add指令提前发射，需要等到第一个load的数据读回来，第二条add才能发射执行，然后第三条load才能发射执行。现在我们给这三条指令换个顺序，变成load r0 addr，load r2 addr，add r1 r0 r0，这样第一条load发出去之后，第二条load可以继续发射执行，然后才阻塞在第三条add指令，这也就是双缓冲里下一次访存和本次计算不同的相对位置会有不同的效果。</span><br></pre></td></tr></table></figure>

<p>向量化访存下的 bank conflict： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690052715">https://zhuanlan.zhihu.com/p/690052715</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对于 quarter/half/Warp 内所有活跃的第 i 号线程，第 i xor 1 号线程不活跃或者访存地址和其一致；(i.e. T0==T1, T2==T3, T4==T5, T6==T7, T8 == T9, ......, T30 == T31, etc.)</span><br><span class="line">对于 Warp 内所有活跃的第 i 号线程，第 i xor 2 号线程不活跃或者访存地址和其一致；(i.e. T0==T2, T1==T3, T4==T6, T5==T7 etc.)</span><br></pre></td></tr></table></figure>

<p>解决 bank conflict： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/722286440">https://zhuanlan.zhihu.com/p/722286440</a></p>
<p>gmem load to smem： smem[tid.y][tid.x] &#x3D; … x↑ y 行 x 列</p>
<p>smem write to gmem： out &#x3D; smem[tid.x][tid.y] x↑ x 行 y 列</p>
<p>padding 按照 32 bank 排列</p>
<p>0,…,31,</p>
<p>32,0,…,30,</p>
<p>31,32,0…</p>
<p>行列之和取余</p>
<p>(x + y) % 32 x列时32线程随着x↑y不变各自错开，x行时随着x↑也各自错开，是个一一映射</p>
<p>0,1,…,31,</p>
<p>1,…,31,0,</p>
<p>2,…,31,0,1</p>
<p>深入浅出 Gemm：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/435908830">https://zhuanlan.zhihu.com/p/435908830</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果一条指令的的源寄存器有2个以上来自同一bank，就会产生冲突。指令会重发射，浪费一个cycle</span><br><span class="line">内积外积计算各有优劣。内积计算不需要使用很多的寄存器，直接一个for-k循环，但需要多次访问shared mem，虽然shared mem在片上，但是访存开销也是几十个cycle, (rm + rn) * bk；外积在计算rm*rn这个小矩阵过程当中，对应的是A的列和B的行，这些元素其实只从smem访问一次，但是用到了很多寄存器, rm * rn * bk</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442930482">https://zhuanlan.zhihu.com/p/442930482</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">将数据从shared mem放到寄存器也是相对耗时的，这个双缓冲是让本轮计算和下一轮shared mem-&gt;register的过程错开，让计算指令和访存指令发射的时候尽量避免stall，这样的话，计算的流水和访存的流水可以打得更满</span><br><span class="line">加载到寄存器，指的就是赋值给kernel中的局部变量</span><br><span class="line">从global mem搬运到shared mem中间需要经过register，安培架构（async copy）前，只有从global到寄存器的指令，然后再用一条指令把寄存器的数放到share mem。</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/481600052">https://zhuanlan.zhihu.com/p/481600052</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先计算warp_id=tid/32，随后计算lane_id=tid%32，通过warp_id和lane_id来算出，对应128个元素当中的哪一个。先算(warp_id%4)×16，假设是warp2，就是上图左侧的第2个（从0算）warp。前面有2个warp，跳过了2*16=32个元素。然后再看看当前lane_id。0-15在左半边，16-31在右半边。所以lane_id/16，先看是左半边还是右半边。右半边的话，先跳过8个元素。最后再看lane_id的奇偶数，如果奇数的话，就再跳一个四个元素</span><br><span class="line">remapping reuse</span><br><span class="line">比如之前的一句有R0.reuse，.reuse把R0的值keep在reuse cache中，那后一句的R0虽然没加reuse，但是reuse cache里还是存着R0的值，相当于cache命中了，然后最后这句没有reuse，R0的值不再keep而是invalidate了。.reuse的含义并不是立即使用了reuse cache，而是把reg的值pass到了reuse cache（如果不在cache中）并且keep住</span><br><span class="line">volta/turing因为register file改成了2 bank 2port，更难出现bank conflict了，所以再增加reuse cache的利用率性能提升并不明显，更多是降功耗。如果把所有的reuse flag全去掉，大概影响10%的性能，这时候原因主要是寄存器带宽的压力比较大（fma把寄存器带宽用满了，但同时LDS又在写reg），只要每条fma有一个src operand是从reuse cache读的，寄存器带宽也不是瓶颈了，自然你再增加reuse也不会增加什么性能</span><br><span class="line">寄存器映射的关键在于C，不能让src2和src0 1同bank产生conflict，但是只映射解决不了src0和src1的冲突，就得引入reuse，当然也需要通过reuse解决寄存器带宽的压力；volta只要不是3src同奇偶就不会conflict，纯沿用maxwell的映射本身就已经没conflict，加reuse纯起到解决寄存器带宽压力和降功耗的作用，然后只要每条fma有一个reuse就已经没有寄存器带宽的问题，在此基础上再提升reuse的利用率也不能提高性能了</span><br><span class="line">turing给reg file的sram做成了2port，2RW，同一bank也一次能同时读两次，只是turing 2port地址线分奇偶，所以只要fma的3src不是同奇偶，就不会有conflict</span><br></pre></td></tr></table></figure>

<p>double buffer： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/696844342">https://zhuanlan.zhihu.com/p/696844342</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">访存和计算对应着不同的硬件单元, 可以并行执行。代码执行顺序执行取决于编译后硬件指令发射顺序。 指令的发射过程虽然是顺序的, 但发射速度很快, 而指令发出后需要一段时间才能执行完成, 这也就对应着某个指令需要相应的时钟周期才能完成, 访存的延迟也就是访存指令相比于计算指令有更长的时钟周期。</span><br><span class="line">在线程块层面, 由于少了一次同步, GPU可以提前发射后面线程分片的计算指令, 从而掩盖从 GMEM 加载到 SMEM 的访存延迟。</span><br><span class="line">在线程层面, 加载和计算的是两个线程分片, 指令上没有依赖关系, 计算指令可以无需等待数据加载完成就可以发射, 从而掩盖从 SMEM 加载到寄存器的访存延迟。</span><br></pre></td></tr></table></figure>

<p>初探 register bank conflict &amp; reuse：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410278370">https://zhuanlan.zhihu.com/p/410278370</a></p>
<p><strong>CUDA Graph</strong></p>
<ul>
<li>cuda graph &amp; multi-stream： <a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1391695">https://developer.aliyun.com/article/1391695</a></li>
</ul>
<p>kernel launch 开销拆分： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/3339296238/answer/24864699000">https://www.zhihu.com/question/3339296238/answer/24864699000</a></p>
<p>kernel launch 的时间也就是把每个操作提交给GPU花费的时间，一般为微秒级，但如果大量launch，总耗时就不可忽略</p>
<p>每次内核执行的空隙代表GPU处于闲置状态，可以用同步语句来模拟 等待内核完全在 GPU 上启动并完成</p>
<p>cuda context： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/694214348">https://zhuanlan.zhihu.com/p/694214348</a></p>
<p>cuda graph：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/700224642">https://zhuanlan.zhihu.com/p/700224642</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/705785807">https://zhuanlan.zhihu.com/p/705785807</a></p>
<p>只是对固定的计算任务提交到CPU的速度做优化，不会改变其他逻辑</p>
<p>理想状态下CPU总是能及时把计算任务提交到GPU，GPU利用率可以维持比较高的水平。但对于小算子，CPU提交的速度甚至不如GPU执行的速度快，这时GPU侧不可避免会出现空洞</p>
<p>单次提交的优化是有上限的，执行计算图包含了大量的单次提交，重复运行计算图，执行的单次提交也都是重复的。cuda graph 后续每次图的执行只有一次提交的开销</p>
<p>cuda event &amp; cuda stream： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/699754357">https://zhuanlan.zhihu.com/p/699754357</a></p>
<p>简易使用示例：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/715863693">https://zhuanlan.zhihu.com/p/715863693</a></p>
<p><strong>Structure releated</strong></p>
<p>片上调度： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/713114525">https://zhuanlan.zhihu.com/p/713114525</a></p>
<p>gpu 包含多个sm，一个sm对应一个及以上block；sm包含多个sp，一个sp对应一个及以上线程</p>
<p>一个sm运行多个block时，多个block共用一块smem，每个block分到的就少了</p>
<p>一个warp陷入等待时，就切换到另一组warp继续计算，这样一个warp的内存延迟就被另一个warp的计算延迟隐藏起来</p>
<p>所以对于使用寄存器较少访存为主的核函数（vector add） 使用大blockdim 反之可尝试分配较小的线程数</p>
<p>每个SM被划分为4个partition，对应4个warp scheduler，一个时钟周期可调度128个线程</p>
<p>同一时刻同一个partition的多个warp串行执行，同一时刻不同partition间可以并行</p>
<p>sp闲置： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/445590537/answer/3549844270">https://www.zhihu.com/question/445590537/answer/3549844270</a></p>
<p><strong>Tensor Core</strong></p>
<p>mma 计算过程： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614429902">https://zhuanlan.zhihu.com/p/614429902</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一个warp内所有thread所取的数据全部完成计算，算作一个硬件指令，虽然这些thread可能会分组串行执行耗费多个周期，也叫作一条硬件指令。</span><br><span class="line">需要 fetch register的就是一条硬件指令；复用寄存器的操作，但是需要两个周期计算（寄存器交换）叫做step</span><br><span class="line">一条硬件指令可能会把thread分成4组，每个set由8个thread联合提供数据计算,每个set会分成若干个step执行，随着数制变化而变化</span><br><span class="line"></span><br><span class="line">32个thread一次所取回的数据能够完成M16N8K16的矩阵乘，但是tensorCore的大小是8x4x8，所以需要一次取回的数据，需要分成8个step完成</span><br><span class="line">Ampere完成一个16x16x16需要两次hmma，一共28次寄存器访问（读+写），一次hmma=2*((3.5+0.5)+(2.5+0.5))=14次</span><br><span class="line">B的上半部分已经在第一轮被r4读过了</span><br></pre></td></tr></table></figure>

<p>cublas： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666391239">https://zhuanlan.zhihu.com/p/666391239</a></p>
<h4 id="3-Speculative-Decoding"><a href="#3-Speculative-Decoding" class="headerlink" title="3.Speculative Decoding"></a>3.Speculative Decoding</h4><p>两种实现： <a target="_blank" rel="noopener" href="https://github.com/feifeibear/LLMSpeculativeSampling">https://github.com/feifeibear/LLMSpeculativeSampling</a></p>
<p>投机采样简介： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/651359908">https://zhuanlan.zhihu.com/p/651359908</a></p>
<p>投机采样核心问答： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/656374338">https://zhuanlan.zhihu.com/p/656374338</a></p>
<p>投机采样并行验证： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653935025">https://zhuanlan.zhihu.com/p/653935025</a></p>
<p>Medusa head简介： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658544279">https://zhuanlan.zhihu.com/p/658544279</a></p>
<p>Medusa 代码浅析： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/655809033">https://zhuanlan.zhihu.com/p/655809033</a></p>
<p>特征层：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/704755926">https://zhuanlan.zhihu.com/p/704755926</a></p>
<p>优化树结构： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707596864">https://zhuanlan.zhihu.com/p/707596864</a></p>
<p>论文合订： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/684217993">https://zhuanlan.zhihu.com/p/684217993</a></p>
<h4 id="4-Disaggregated-Inference"><a href="#4-Disaggregated-Inference" class="headerlink" title="4.Disaggregated Inference"></a>4.Disaggregated Inference</h4><p>Splitwise： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701772045">https://zhuanlan.zhihu.com/p/701772045</a></p>
<p>Distserve： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706761664">https://zhuanlan.zhihu.com/p/706761664</a></p>
<p>Chunked Prefill：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/710165390">https://zhuanlan.zhihu.com/p/710165390</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/718715866">https://zhuanlan.zhihu.com/p/718715866</a></p>
<p>Distserve &amp; Tetrilnfer &amp; Mooncake： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706469785">https://zhuanlan.zhihu.com/p/706469785</a></p>
<p>Mooncake 碎碎念：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/710706218">https://zhuanlan.zhihu.com/p/710706218</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/705754254">https://zhuanlan.zhihu.com/p/705754254</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706204757">https://zhuanlan.zhihu.com/p/706204757</a></p>
<p>更多思考： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707199343">https://zhuanlan.zhihu.com/p/707199343</a></p>
<p>Mooncake 技术笔记： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706097807">https://zhuanlan.zhihu.com/p/706097807</a></p>
<p>请求调度： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/713712371">https://zhuanlan.zhihu.com/p/713712371</a></p>
<h4 id="2-vLLM"><a href="#2-vLLM" class="headerlink" title="2.vLLM"></a>2.vLLM</h4><ul>
<li>Paged_AttentionV2： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/pull/1348">https://github.com/vllm-project/vllm/pull/1348</a></li>
<li>+34%： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/issues/421">https://github.com/vllm-project/vllm/issues/421</a></li>
</ul>
<p>完全弄懂 prefix caching： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/693556044">https://zhuanlan.zhihu.com/p/693556044</a></p>
<p>Automatic Prefix Caching： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/pull/2762">https://github.com/vllm-project/vllm/pull/2762</a></p>
<p>Warp-level Primitives： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/572820783">https://zhuanlan.zhihu.com/p/572820783</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(unsigned mask, T var, int srcLane, int width=warpSize);</span><br><span class="line">T __shfl_xor_sync(unsigned mask, T var, int laneMask, int width=warpSize);</span><br></pre></td></tr></table></figure>

<p>Transformer-4： Paged Attention kernel</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663632255">https://zhuanlan.zhihu.com/p/663632255</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663719053">https://zhuanlan.zhihu.com/p/663719053</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一个线程组处理16个字节，一个线程8个字节相当于两个fp16</span><br><span class="line">K_vec 和 Q_vec 都表示这样的 vector：类型是fp16，大小为4</span><br><span class="line"></span><br><span class="line">加载q向量： q 是传入的关于整体 q 所在的显存起始地址，q_ptr 是当前 head 下当前 seq 的 q 的地址</span><br><span class="line">q_stride = num_head*head_size，表示要越过前面所有 seq 所有 head 的 q，当前处理的 head 的 idx 是 head_idx，那么在此 head 之前有关于当前 seq 的 head_idx 个 HEAD_SIZE 数量，也要越过，才是当前 seq 在当前 head 下的 q 的显存地址 q_ptr</span><br><span class="line"></span><br><span class="line">vec_idx * VEC_SIZE 求出包含的 fp16 个数，vec_idx 实际为 0 ~ 63</span><br><span class="line">const int offset1 = (vec_idx * VEC_SIZE) / x;  (0,4,8,12,...,252) / 8 = (0,0,1,1,...,31,31)</span><br><span class="line">const int offset2 = (vec_idx * VEC_SIZE) % x;  (0,0,0,0,4,4,4,4,...)</span><br></pre></td></tr></table></figure>

<p>Transformer-10： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671344566">https://zhuanlan.zhihu.com/p/671344566</a> SchedulerOutputs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_batched_tokens = len(new_seq_lens) * max(new_seq_lens)</span><br><span class="line">当前运行的 seq 总数 x 最大长度不能超过 profile 预设</span><br><span class="line">有新加入 scheduled 的 seq 就做 prompt run 并提前 return</span><br><span class="line">在 not self.swapped 的情况下，从 waiting 中拿 seq group 构造的 SchedulerOutputs 是promopt 阶段的调度；反之是在 running 和可以 swap_in 的 list 中拿数据，是generator阶段的调度</span><br><span class="line"></span><br><span class="line">generation 阶段的 每个 seq 都只持有一个 token slot，所以此后的 num_batched_tokens == running 中的 seq 数目</span><br><span class="line">两种抢占策略</span><br></pre></td></tr></table></figure>

<p>Paged Attention：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662900859">https://zhuanlan.zhihu.com/p/662900859</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660192497">https://zhuanlan.zhihu.com/p/660192497</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/664276902">https://zhuanlan.zhihu.com/p/664276902</a></p>
<p>Paged Attention 源码解析：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663632255">https://zhuanlan.zhihu.com/p/663632255</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663719053">https://zhuanlan.zhihu.com/p/663719053</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">当前 head 下的 seq 的 q 向量，load到共享内存 q_vecs 中，即 256 个 fp16 的数据，使用第一个 warp load 进来</span><br><span class="line">for (int i = thread_group_idx; i &lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) &#123;</span><br><span class="line">    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;</span><br><span class="line">    q_vecs[thread_group_offset][i] = *reinterpret_cast&lt;const Q_vec*&gt;(q_ptr + vec_idx * VEC_SIZE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">关于NUM_TOKENS_PER_THREAD_GROUP的循环：</span><br><span class="line">for (int i = 0; i &lt; NUM_TOKENS_PER_THREAD_GROUP; i++) &#123;  // 按照线程组处理的 token 数量循环，每1个线程组也就是2个线程处理1个token</span><br><span class="line">    const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;</span><br><span class="line">    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;  </span><br><span class="line">    ... </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">根据 offset 获得当前线程负责的 token 的 k 向量  j = 0, 1</span><br><span class="line">const int offset1 = (vec_idx * VEC_SIZE) / x;  256个元素，两组128，offset1=0~15,offset2=1,3,5,7/0,2,4,6</span><br><span class="line">const int offset2 = (vec_idx * VEC_SIZE) % x;  </span><br><span class="line">类似于展平成一行后重新寻址 一共256个数放到容量为32的寄存器中 每8个数放一起 一个线程放几个 一个线程组一次传输16byte 也就是一个线程4个数</span><br><span class="line"></span><br><span class="line">kv_head_stride 本质上是一个 block 中所有的 token 乘以 head_size，只算1个头，所占的 fp16 的数量</span><br><span class="line">    </span><br><span class="line">thread_group_offset为0的线程，收集的是当前 group（1个group负责1个token的1个head的head_size个数据）中最大的 qk值</span><br><span class="line"></span><br><span class="line">处理和传输带宽要区分开</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665609491">https://zhuanlan.zhihu.com/p/665609491</a></p>
<ul>
<li>源码分析： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/641999400">https://zhuanlan.zhihu.com/p/641999400</a></li>
</ul>
<p>vLLM 推理流程梳理：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/649974825">https://zhuanlan.zhihu.com/p/649974825</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/649977422">https://zhuanlan.zhihu.com/p/649977422</a></p>
<p>Paged Attention 核心： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/655561941">https://zhuanlan.zhihu.com/p/655561941</a></p>
<p>一致性 penalty： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658780653">https://zhuanlan.zhihu.com/p/658780653</a></p>
<ul>
<li><p>vLLM top down 概览： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645251151">https://zhuanlan.zhihu.com/p/645251151</a></p>
</li>
<li><p>fastapi 部署： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/6256971">https://zhuanlan.zhihu.com/p/6256971</a></p>
</li>
</ul>
<p>  fastapi async single threaded： <a target="_blank" rel="noopener" href="https://github.com/tiangolo/fastapi/issues/4265">https://github.com/tiangolo/fastapi/issues/4265</a></p>
<h4 id="3-Attention-优化"><a href="#3-Attention-优化" class="headerlink" title="3.Attention 优化"></a>3.Attention 优化</h4><ul>
<li>FlashAttention：</li>
</ul>
<p>  从 online softmax 开始推导：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/9339626150">https://zhuanlan.zhihu.com/p/9339626150</a></p>
<p>  配图解读，十分清晰：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/693843979">https://zhuanlan.zhihu.com/p/693843979</a></p>
<p>  只看图： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663932651">https://zhuanlan.zhihu.com/p/663932651</a></p>
<p>  值得和2最后细看： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156">https://zhuanlan.zhihu.com/p/607364156</a></p>
<p>  Flops, 复杂度, 访问次数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626079753">https://zhuanlan.zhihu.com/p/626079753</a></p>
<p>  速度优化原理： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/611236756/answer/3322413586">https://www.zhihu.com/question/611236756/answer/3322413586</a></p>
<p>  v2 两点优化项十分清晰： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645627275">https://zhuanlan.zhihu.com/p/645627275</a></p>
<p>  v2巩固： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/682441154">https://zhuanlan.zhihu.com/p/682441154</a></p>
<p>  Triton引子 &amp; FA循序渐进 &amp; 极具含金量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665170554">https://zhuanlan.zhihu.com/p/665170554</a></p>
<ul>
<li><p>MQA GQA： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q</a></p>
</li>
<li><p>Flash decoding：</p>
</li>
</ul>
<p>  感觉没说： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661478232">https://zhuanlan.zhihu.com/p/661478232</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66426444">https://zhuanlan.zhihu.com/p/66426444</a></p>
<h4 id="4-TensorRT-LLM"><a href="#4-TensorRT-LLM" class="headerlink" title="4.TensorRT-LLM"></a>4.TensorRT-LLM</h4><p>部署调优： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/699333691">https://zhuanlan.zhihu.com/p/699333691</a></p>
<p>evaluate 本地下载： <a target="_blank" rel="noopener" href="https://blog.csdn.net/misaki_min/article/details/132650725">https://blog.csdn.net/misaki_min/article/details/132650725</a></p>
<p>perf json： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/issues/5746">https://github.com/triton-inference-server/server/issues/5746</a></p>
<p>Attention kernels： <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/issues/457">https://github.com/NVIDIA/TensorRT-LLM/issues/457</a></p>
<p>max_batch_size 讲究： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tensorrtllm_backend/issues/72">https://github.com/triton-inference-server/tensorrtllm_backend/issues/72</a></p>
<p>perf_analyzer cli： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md">https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md</a></p>
<p>perf_analyzer -m ensemble -i grpc –shape “bad_words：1” –shape “max_tokens：1” –shape “stop_words：1” –shape “text_input：1” –streaming</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Cannot send stop request without specifying a request_id.</span><br><span class="line">in ensemble &#x27;ensemble&#x27;, Streaming is only supported if model is deployed using decoupled mode.</span><br><span class="line">ModelInfer RPC doesn&#x27;t support models with decoupled transaction policy.</span><br><span class="line">https://github.com/triton-inference-server/server/issues/4994  + --streaming</span><br><span class="line">Failed to init manager inputs： input bad_words contains dynamic shape, provide shapes to send along with the request  + --shape</span><br><span class="line">Cannot process new request： Streaming mode is only supported with beam width of 1.</span><br></pre></td></tr></table></figure>

<p>ulimit memlock&#x3D;-1： <a target="_blank" rel="noopener" href="https://gorden5566.com/post/1089.html">https://gorden5566.com/post/1089.html</a></p>
<p>LD_DEBUG&#x3D;libs 查看程序搜索库的路径：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/skyie53101517/article/details/45461835">https://blog.csdn.net/skyie53101517/article/details/45461835</a></p>
<p>3个.so： <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/issues/388">https://github.com/NVIDIA/TensorRT-LLM/issues/388</a></p>
<p>perf_analyzer cli： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md">https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find / -name pybind11Config.cmake</span><br><span class="line"></span><br><span class="line">cmake -DSM=80 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON -Dpybind11_DIR=/usr/local/lib/python3.8/dist-packages/pybind11/share/cmake/pybind11/pybind11Config.cmake ..</span><br><span class="line"></span><br><span class="line">mpirun -n 2 --allow-run-as-root  python api_server.py --model /data2/dingweihao/llama-2-13b-pretrain-sft-2048-checkpoint-546-20230912/ft/2-gpu/ --tokenizer /data2/dingweihao/llama-2-13b-pretrain-sft-2048-checkpoint-546-20230912/ --lib ../build/lib/ --tensor_para_size 2 --port 7000 --host 0.0.0.0</span><br></pre></td></tr></table></figure>

<p>-Dpybind11_DIR： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38163468/article/details/121600290">https://blog.csdn.net/qq_38163468/article/details/121600290</a></p>
<p>remove padding的目的是为了减少padding部分的计算量，FT 中实现的remove padding仅仅减少了layernorm部分以及ffn部分还有self-attention中最后那个全连接部分的padding的计算量</p>
<h4 id="5-Coroutine-Thread-Process"><a href="#5-Coroutine-Thread-Process" class="headerlink" title="5.Coroutine &amp; Thread &amp; Process"></a>5.Coroutine &amp; Thread &amp; Process</h4><p>并发：由一个处理器快速交替执行多个任务，只是看起来像在“同时执行多个任务”</p>
<p>并行：由多个处理器分别运行多个任务，各任务间严格同时执行</p>
<p>ring all-reduce数学性质推导： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/504957661">https://zhuanlan.zhihu.com/p/504957661</a></p>
<p>MPI通信接口代价函数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653968730">https://zhuanlan.zhihu.com/p/653968730</a></p>
<p>multiprocess &amp; rpc &amp; zeromq：</p>
<p>nccl 仿照 demo debug：<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573/6">https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573/6</a></p>
<p>NCCL increased memory：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nccl/issues/964">https://github.com/NVIDIA/nccl/issues/964</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nccl/issues/864">https://github.com/NVIDIA/nccl/issues/864</a></p>
<p>mp.managers：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424011522">https://zhuanlan.zhihu.com/p/424011522</a></p>
<p>pipe queue：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24883194">https://zhuanlan.zhihu.com/p/24883194</a></p>
<p>pool：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24883077">https://zhuanlan.zhihu.com/p/24883077</a></p>
<p>多进程 + async：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/629916103">https://zhuanlan.zhihu.com/p/629916103</a></p>
<p>asyncio：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># From https://zhuanlan.zhihu.com/p/681935789</span><br><span class="line">import asyncio</span><br><span class="line">import asyncio.tasks</span><br><span class="line"></span><br><span class="line">def inspect_event_loop()：</span><br><span class="line">    for event_loop, running_task in asyncio.tasks._current_tasks.items()：</span><br><span class="line">        print(&quot;Running Task：\n&quot;)</span><br><span class="line">        running_task.print_stack()</span><br><span class="line"></span><br><span class="line">        print(&quot;All Tasks：\n&quot;)</span><br><span class="line">        for task in asyncio.tasks.all_tasks(event_loop)：</span><br><span class="line">            task.print_stack()</span><br></pre></td></tr></table></figure>

<p>动态添加协程： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59621713">https://zhuanlan.zhihu.com/p/59621713</a></p>
<p>asyncio lock： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66050624">https://zhuanlan.zhihu.com/p/66050624</a></p>
<p>event loop： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69210021">https://zhuanlan.zhihu.com/p/69210021</a></p>
<p>值得详细捋一遍 asyncio： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59671241">https://zhuanlan.zhihu.com/p/59671241</a> wait gather讲得很清晰</p>
<p>asyncio 示例讲解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56084772">https://zhuanlan.zhihu.com/p/56084772</a></p>
<p>wait vs gather： <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6872bf356af7">https://www.jianshu.com/p/6872bf356af7</a></p>
<p>coroutine &amp; task： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45521388">https://zhuanlan.zhihu.com/p/45521388</a></p>
<p>最简 event loop： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/83627584">https://zhuanlan.zhihu.com/p/83627584</a></p>
<p>event： <a target="_blank" rel="noopener" href="https://blog.csdn.net/mixintu/article/details/102458809">https://blog.csdn.net/mixintu/article/details/102458809</a></p>
<p>get &#x2F; set_event_loop： <a target="_blank" rel="noopener" href="https://blog.csdn.net/whatday/article/details/106885916">https://blog.csdn.net/whatday/article/details/106885916</a></p>
<p>loop.run_xxx 家族都是阻塞的，例如 run_until_loop 会等到给定的 coroutine 完成才结束</p>
<p>初始情况下，get_event_loop 只会在主线程帮您创建新的 event loop，并且在主线程中多次调用始终返回该 event loop；而在其他线程</p>
<p>中调用 get 则会报错，除非您在这些线程里面手动调用过 set</p>
<p>协程可以包装在 asyncio.Task 对象中独立执行，而不是直接在协程中执行</p>
<p>访问事件循环的原因：</p>
<p>监控任务的进度; 发布任务并从中获取结果; 解雇并忘记一次性任务。</p>
<p>task.done()可以用于检测任务是否完成 .cancelled()</p>
<p>.result()获取任务返回的结果 提前先检查任务是否已完成或是被取消 不然会出现 InvalidStateError</p>
<p>.exception() 检索未处理的异常 .cancel()取消计划任务</p>
<p>add_done_callback()向任务里添加回调 remove_done_callback()</p>
<p>task &#x3D; asyncio.create_task(task_coroutine(), name&#x3D;’MyTask’)<br>set_name() get_name()</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/606363838">https://zhuanlan.zhihu.com/p/606363838</a></p>
<p>asyncio.current_task() 获取当前任务，这将为当前正在运行的任务返回一个任务对象，这可能是：</p>
<ol>
<li>传递给 asyncio.run() 的主协程</li>
<li>通过 asyncio.create_task() 在 asyncio 程序中创建和调度的任务</li>
</ol>
<p>所有协程都可以作为异步事件循环中的任务进行访问</p>
<p>asyncio.all_tasks() 获取一组已计划和正在运行的任务</p>
<p>异步生成器中依次获取值时，可以使用<code>async for</code>来实现</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607709631">https://zhuanlan.zhihu.com/p/607709631</a></p>
<p>asyncio.gather()： 允许将一组可等待对象视为单个可等待对象</p>
<p>一次执行这些任务协程并等待它们全部完成后再继续，例如具有不同数据的相同任务或协程</p>
<ul>
<li>通过 await 表达式执行并等待组中的所有可等待对象完成。</li>
<li>从所有分组的等待对象中获取结果，稍后通过 result() 方法检索。</li>
<li>要通过 cancel() 方法取消的一组等待对象。</li>
<li>通过 done() 方法检查组中的所有可等待对象是否已完成。</li>
<li>仅当组中的所有任务完成时才执行回调函数。</li>
</ul>
<p>一旦创建了 Future 对象，它就会在事件循环中自动调度</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/608952664">https://zhuanlan.zhihu.com/p/608952664</a></p>
<p>asyncio.wait() 函数可用于等待一组异步任务完成</p>
<p>asyncio 任务是包装协程的 asyncio.Task 类的一个实例</p>
<p>它允许独立调度和执行协程，Task 实例提供任务句柄以查询状态和获取结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)</span><br><span class="line">FIRST_COMPLETED 当第一个任务完成并在完成集中返回时，其余任务不会被取消并继续并发执行</span><br><span class="line">FIRST_EXCEPTION 来等待第一个任务因异常而失败</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/609252710">https://zhuanlan.zhihu.com/p/609252710</a></p>
<p>asyncio.wait_for() 函数允许调用者等待 asyncio 任务或协程超时完成。如果没有指定超时，wait_for() 函数将等待直到任务完成。如果在任务完成之前指定了超时并超时，那么任务将被取消</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">await asyncio.wait_for(coro, timeout=10)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/609956166">https://zhuanlan.zhihu.com/p/609956166</a></p>
<p>asyncio.shield() 保护另一个任务或协程不被取消，它以一个可等待对象作为参数并返回一个 asyncio.Future 对象，然后直接等待 Future 对象或将其传递给另一个任务或协程</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/610881194">https://zhuanlan.zhihu.com/p/610881194</a></p>
<p>异步迭代器 &amp; 生成器：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/611864797">https://zhuanlan.zhihu.com/p/611864797</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612439743">https://zhuanlan.zhihu.com/p/612439743</a></p>
<p>asyncio 基本：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619977951">https://zhuanlan.zhihu.com/p/619977951</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619978329">https://zhuanlan.zhihu.com/p/619978329</a></p>
<p>asyncio.create_task() 方法安排许多协程在 asyncio 程序中独立运行</p>
<p>可以通过首先通过 asyncio.all_tasks() 函数获取一组所有正在运行的任务，从该集合中删除自身，然后通过 asyncio.wait() 函数等待剩余的任务来实现</p>
<p>通过直接等待 asyncio.Task 对象来等待任务完成</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/622324216">https://zhuanlan.zhihu.com/p/622324216</a></p>
<h4 id="6-Triton"><a href="#6-Triton" class="headerlink" title="6.Triton"></a>6.Triton</h4><p>Makefile： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350297509">https://zhuanlan.zhihu.com/p/350297509</a></p>
<p>MLIR文章汇总： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/141256429">https://zhuanlan.zhihu.com/p/141256429</a></p>
<p>软链接： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21386275/article/details/79881543">https://blog.csdn.net/qq_21386275/article/details/79881543</a></p>
<p>c++filt -n： <a target="_blank" rel="noopener" href="https://blog.csdn.net/K346K346/article/details/88225726">https://blog.csdn.net/K346K346/article/details/88225726</a></p>
<p>LLVM： <a target="_blank" rel="noopener" href="https://github.com/llvm/llvm-project/issues/63988">https://github.com/llvm/llvm-project/issues/63988</a></p>
<p>&#x2F;usr&#x2F;bin&#x2F;ld： cannot find -lxx： <a target="_blank" rel="noopener" href="https://blog.csdn.net/kuzma_zhang/article/details/131829943/">https://blog.csdn.net/kuzma_zhang/article/details/131829943/</a></p>
<p>ninja install 可以 但是加上 sudo 前缀后提示无该命令： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lfri/p/16277069.html">https://www.cnblogs.com/lfri/p/16277069.html</a></p>
<p>ninja -C build check-llvm</p>
<p>CMake hidden by files： conda deactivate .so文件被隐藏</p>
<p>卸载 sudo make install： <a target="_blank" rel="noopener" href="https://blog.csdn.net/charry_win/article/details/126628169">https://blog.csdn.net/charry_win/article/details/126628169</a></p>
<p>入门： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/622685131/answer/3217107882">https://www.zhihu.com/question/622685131/answer/3217107882</a></p>
<h4 id="7-CUDA"><a href="#7-CUDA" class="headerlink" title="7.CUDA"></a>7.CUDA</h4><p>fp16 cuda：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25147897/article/details/79053748">https://blog.csdn.net/qq_25147897/article/details/79053748</a></p>
<p><a target="_blank" rel="noopener" href="https://hackmd.io/@yrHb-fKBRoyrKDEKdPSDWg/ryvMkGHR">https://hackmd.io/@yrHb-fKBRoyrKDEKdPSDWg/ryvMkGHR</a></p>
<p>位运算高级操作： <a target="_blank" rel="noopener" href="https://blog.csdn.net/GOLOJO/article/details/139420924">https://blog.csdn.net/GOLOJO/article/details/139420924</a></p>
<p>二进制：</p>
<p>推广至fp16： <a target="_blank" rel="noopener" href="https://blog.csdn.net/jiaoyangwm/article/details/129296459">https://blog.csdn.net/jiaoyangwm/article/details/129296459</a></p>
<p>int8： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/peterleee/p/13752162.html">https://www.cnblogs.com/peterleee/p/13752162.html</a></p>
<p>指数位是偏移值,对float就是”实际值”加上127得到得值.即”01111111”得实际值实际上是0,所以整个</p>
<p>3f800000的值为 ： +1 * 2^0 * 1.0 &#x3D; 1.0，0xff提取低8位</p>
<p>self.stream &#x3D; torch.cuda.Stream().cuda_stream const</p>
<p>c10::cuda::CUDAStream+torch.cuda.Stream()编译能过但是运行类型不匹配 cudaStream_t</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/is-torch-cuda-stream-cuda-stream-equivalent-to-cudastream-t/130696">https://discuss.pytorch.org/t/is-torch-cuda-stream-cuda-stream-equivalent-to-cudastream-t/130696</a></p>
<p>cudaError_t cudaMalloc(void** devPtr, size_t size)</p>
<p>第一个参数传递的是存储在cpu内存中的指针变量的地址，cudaMalloc在执行完成后，向这个地址中写入了一个地址值（此地址值是GPU显存里的）。</p>
<p>静态变量应该使用 cudaMemcpyToSymbol(devData, &amp;value, sizeof(float)) 而非 cudaMemcpy(&amp;value, devData, sizeof(float))，不能用动态copy的方式给静态变量赋值</p>
<p>硬要用cudaMemcpy，必须得</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float *dptr;</span><br><span class="line">// 主机端对device变量进行取地址是非法的，想要得到devData的地址可以使用</span><br><span class="line">cudaGetSymbolAddress((void**)&amp;dptr, devData);</span><br><span class="line">cudaMemcpy(dptr, &amp;value, sizeof(float), cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>

<p>固定内存的释放和分配成本 cudaMallocHost 比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。尽量使用流来使内存传输和计算之间同时进行。</p>
<p>零拷贝内存： cudaHostAlloc(flag&#x3D;cudaHostAllocMapped)</p>
<p>零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaHostGetDevicePointer(void** pDevice, void* pHost, unsigned flags)</span><br></pre></td></tr></table></figure>

<p>有了统一虚拟寻址UVA，cudaHostGetDevicePointer就没用了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">float *a_host,*b_host,*res_d;</span><br><span class="line">CHECK(cudaHostAlloc((float**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">CHECK(cudaHostAlloc((float**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">CHECK(cudaMalloc((float**)&amp;res_d,nByte));</span><br><span class="line">res_from_gpu_h=(float*)malloc(nByte);</span><br><span class="line"></span><br><span class="line">initialData(a_host,nElem);</span><br><span class="line">initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line">dim3 block(1024);</span><br><span class="line">dim3 grid(nElem/block.x);</span><br><span class="line">sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_host,b_host,res_d);</span><br></pre></td></tr></table></figure>

<p>统一内存寻址设置内存池：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMallocManaged(void** devPtr, size_t size, unsigned int flags=0)</span><br></pre></td></tr></table></figure>

<p>不可以在host函数中直接访问静态全局内存变量（<em>_device</em>_），但可以用cudaMemcpyTo&#x2F;FromSymbol来完成静态全局内存&#x2F;常量内存与host内存之间的数据传输</p>
<p>-Xptxas -dlcm&#x3D;cg 禁用L1：32Bytes -Xptxas -dlcm&#x3D;ca 启用：128Bytes</p>
<p>利用率的例子：每个线程访问一个fp32，一个warp就等同于需要访问128字节，对于禁用L1来说，因为粒度是32，所以如果warp里的所有线程都只访问同一个地址上的数，那其实使用率就只有4&#x2F;32（对于128Bytes的缓存行，需要4次内存事务，但是粒度上是32，有用的只有4）</p>
<p>可以使用只读缓存__ldg从全局内存中读取数据，或是在间接引用的指针上使用修饰符</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ void copyKernel(float * in,float* out)</span><br><span class="line">&#123;</span><br><span class="line">    int idx=blockDim*blockIdx.x+threadIdx.x;</span><br><span class="line">    out[idx]=__ldg(&amp;in[idx]);</span><br><span class="line">&#125;</span><br><span class="line">AOS： 就是数组 struct A a[N];</span><br><span class="line">SOA： </span><br><span class="line">struct A&#123;</span><br><span class="line">    int a[N];</span><br><span class="line">    int b[N];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>volatile声明的变量始终在全局内存中，以防编译器优化存入缓存，如果此时恰好被其他线程改写就会造成内存缓存不一致的错误</p>
<p>动态 kernel里只能写成一维的，因为运行时才能确定：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">// 方形</span><br><span class="line">__global__ void setRowReadColDynIpad(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    extern __shared__ int tile[];</span><br><span class="line">    unsigned int row_idx=threadIdx.y*(blockDim.x+1)+threadIdx.x;</span><br><span class="line">    tile[row_idx]=row_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    unsigned int col_idx=threadIdx.x*(blockDim.x+1)+threadIdx.y;</span><br><span class="line">    out[row_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br><span class="line">setRowReadColDynIpad&lt;&lt;&lt;grid,block,(BDIMX+1)*BDIMY*sizeof(int)&gt;&gt;&gt;(out);</span><br><span class="line"></span><br><span class="line">// 以下两种写法是等价的，逐元素填充与按行填充</span><br><span class="line">__global__ void setRowReadCol(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    __shared__ int tile[BDIMY][BDIMX];</span><br><span class="line">    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void setRowReadColDyn(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    extern __shared__ int tile[];</span><br><span class="line">    unsigned int row_idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    unsigned int col_idx=threadIdx.x*blockDim.y+threadIdx.y;</span><br><span class="line">    tile[row_idx]=row_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[row_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 矩形</span><br><span class="line">__global__ void setRowReadColRect(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    __shared__ int tile[BDIMY_RECT][BDIMX_RECT];</span><br><span class="line">    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; // 全局一维索引映射至二维坐标</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx; // 按行写入smem</span><br><span class="line">    __syncthreads();</span><br><span class="line">    unsigned int irow=idx/blockDim.y;</span><br><span class="line">    unsigned int icol=idx%blockDim.y;</span><br><span class="line">    // 因为如果是正常按行的话 iy=idx/blockDim.x</span><br><span class="line">    out[idx]=tile[icol][irow]; // 重新编排索引，按列读取smem，但不会影响最终结果，按列读的结果要和原来的一维地址相符</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// smem完成矩阵转置： 全局内存按行读取，按行写入smem + smem读取一列写入全局内存的一行</span><br><span class="line">// 二维矩阵存储在内存的时候是一维的，一般是把二维矩阵按照逐行的方式放入一维内存中，转置的过程可以理解为把逐行从上到下数据改成逐列从左到右的数据</span><br><span class="line">__global__ void transformSmem(float * in,float* out,int nx,int ny)</span><br><span class="line">&#123;</span><br><span class="line">	__shared__ float tile[BDIMY][BDIMX];</span><br><span class="line">	unsigned int ix,iy,transform_in_idx,transform_out_idx;</span><br><span class="line">// 1.计算当前块中的全局坐标，计算对应的一维线性内存的位置 所谓的待转置的矩阵就是一维线性内存</span><br><span class="line">	ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">	transform_in_idx=iy*nx+ix; // 连接线程与内存 引入全局坐标 + 输入矩阵坐标 一维展开</span><br><span class="line">// 2.bidx表示这个块中线程坐标的线性位置（把块中的二维线程位置按照逐行排布的原则，转换成一维的），然后进行转置改成逐列排布的方式，计算出新的二维坐标，逐行到逐列排布的映射就是转置的映射，这只完成了很多块中的一块，而关键的是我们把这块放回到哪</span><br><span class="line">    // 想象成是大网格里套了很多block</span><br><span class="line">	unsigned int bidx,irow,icol;</span><br><span class="line">	bidx=threadIdx.y*blockDim.x+threadIdx.x; // 类似于iy*nx+ix，计算的是块中坐标，重新计算二维坐标</span><br><span class="line">	irow=bidx/blockDim.y;</span><br><span class="line">	icol=bidx%blockDim.y;</span><br><span class="line">// 3.计算出转置后的二维全局线程的目标坐标，注意这里的转置前的行位置是计算出来的是转置后的列的位置，这就是转置的第二步</span><br><span class="line">	ix=blockIdx.y*blockDim.y+icol;</span><br><span class="line">	iy=blockIdx.x*blockDim.x+irow; // 引入转置后的全局坐标</span><br><span class="line">    // 对比 ix=threadIdx.x+blockDim.x*blockIdx.x 因为转置后打乱了数据排布方式，因此原来的threadIdx就无法使用了  </span><br><span class="line">// 4.计算出转置后的二维坐标对应的全局内存的一维位置 &amp; 读取全局内存，写入共享内存，然后按照转置后的位置写入</span><br><span class="line">	transform_out_idx=iy*ny+ix; // 输出矩阵坐标，也就是转置后的矩阵的坐标</span><br><span class="line">	if(ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">	&#123;</span><br><span class="line">		tile[threadIdx.y][threadIdx.x]=in[transform_in_idx];</span><br><span class="line">		__syncthreads();</span><br><span class="line">		out[transform_out_idx]=tile[icol][irow]; // 原来的按列自上往下 等号左边当作一维展开更好理解</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 填充 + 循环展开</span><br><span class="line">__global__ void transformSmemUnrollPad(float * in,float* out,int nx,int ny)</span><br><span class="line">&#123;</span><br><span class="line">	__shared__ float tile[BDIMY*(BDIMX*2+IPAD)];</span><br><span class="line">//1.</span><br><span class="line">	unsigned int ix,iy,transform_in_idx,transform_out_idx;</span><br><span class="line">	ix=threadIdx.x+blockDim.x*blockIdx.x*2;</span><br><span class="line">    iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">	transform_in_idx=iy*nx+ix;</span><br><span class="line">//2.</span><br><span class="line">	unsigned int bidx,irow,icol;</span><br><span class="line">	bidx=threadIdx.y*blockDim.x+threadIdx.x; // 全局才*2</span><br><span class="line">	irow=bidx/blockDim.y;</span><br><span class="line">	icol=bidx%blockDim.y;</span><br><span class="line">//3.</span><br><span class="line">	unsigned int ix2=blockIdx.y*blockDim.y+icol;</span><br><span class="line">	unsigned int iy2=blockIdx.x*blockDim.x*2+irow;</span><br><span class="line">    </span><br><span class="line">//4.计算出转置后的二维坐标对应的全局内存的一维位置，注意这里不是一次计算一个块，而是计算两个块，换个理解方法，我们把原来的块x方向扩大一倍，然后再对这个大块分成两个小块（A,B），每个小块中的对应位置差BDIMX，然后对其中A，B中数据按行写入共享内存</span><br><span class="line">	transform_out_idx=iy2*ny+ix2;</span><br><span class="line">	if(ix+blockDim.x&lt;nx &amp;&amp; iy&lt;ny) // 边界处理</span><br><span class="line">	&#123;</span><br><span class="line">		unsigned int row_idx=threadIdx.y*(blockDim.x*2+IPAD)+threadIdx.x; // 可以对照上方，是块中坐标</span><br><span class="line">		tile[row_idx]=in[transform_in_idx]; // kernel按block执行，所以每次smem填充一个块的大小，因此等号左边使用到的就是块内坐标而非全局坐标</span><br><span class="line">		tile[row_idx+BDIMX]=in[transform_in_idx+BDIMX];</span><br><span class="line">//5.将4中读取到的共享内存中的数据，按照转置后的位置写入全局内存</span><br><span class="line">		__syncthreads();</span><br><span class="line">		unsigned int col_idx=icol*(blockDim.x*2+IPAD)+irow;</span><br><span class="line">        out[transform_out_idx]=tile[col_idx];</span><br><span class="line">		out[transform_out_idx+ny*BDIMX]=tile[col_idx+BDIMX];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>流能封装cudamalloc，cudamemcpy中的一些异步操作，并保持操作顺序，允许操作在流中排队，保证其在前面所有操作启动之后启动，有了流就能查询排队状态</p>
<p>CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销</p>
<p>异步数据传输时，主机端的内存必须是固定内存(pinned)或不可分页内存(non-pageable)，因为操作系统有权在一个程序运行期间改变程序使用的可分页内存的物理地址，这在异步传输过程中会引发不可知错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMallocHost(void **ptr, size_t size);</span><br><span class="line">cudaError_t cudaHostAlloc(void **pHost, size_t size, size_t flags);</span><br><span class="line">cudaError_t cudaFreeHost(void* ptr);</span><br></pre></td></tr></table></figure>

<p>非空流异步，但可能会被空流阻塞，空&#x2F;默认流同步，cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成，空流不需要显式声明，隐式+阻塞，跟所有阻塞流同步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">__shfl_up_sync(mask,v,d,w)： 序号t的线程返回t-d的线程中变量v的值，t&lt;d的返回原来的v</span><br><span class="line">__shfl_down_sync(mask,v,d,w)： 序号t的线程返回t+d的线程中变量v的值，t+d&gt;=w的返回原来的v</span><br><span class="line">int __shfl_snc(int var,int srcLane,int width=warpSize)</span><br><span class="line">假设width=16,srcLane=3，那么0-15接收0+3位置处的值，16-31接收16+3</span><br><span class="line">// 跨warps使用数组索引交换数值 </span><br><span class="line">// https://face2ai.com/CUDA-F-5-6-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4/ </span><br><span class="line">__inline__ __device__</span><br><span class="line">void swap(int *value,int laneIdx,int mask,int firstIdx,int secondIdx)</span><br><span class="line">&#123;</span><br><span class="line">    bool pred=((laneIdx%(2))==0);</span><br><span class="line">    if(pred)</span><br><span class="line">    &#123;</span><br><span class="line">        int tmp=value[firstIdx];</span><br><span class="line">        value[firstIdx]=value[secondIdx];</span><br><span class="line">        value[secondIdx]=tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    value[secondIdx]=__shfl_xor(value[secondIdx],mask,BDIM);</span><br><span class="line">    if(pred)</span><br><span class="line">    &#123;</span><br><span class="line">        int tmp=value[firstIdx];</span><br><span class="line">        value[firstIdx]=value[secondIdx];</span><br><span class="line">        value[secondIdx]=tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void test_shfl_swap(int *in,int* out,int const mask,int firstIdx,int secondIdx)</span><br><span class="line">&#123;</span><br><span class="line">    //1.</span><br><span class="line">    int idx=threadIdx.x*SEGM;</span><br><span class="line">    int value[SEGM];</span><br><span class="line">    for(int i=0;i&lt;SEGM;i++)</span><br><span class="line">        value[i]=in[idx+i];</span><br><span class="line">    //2.</span><br><span class="line">    swap(value,threadIdx.x,mask,firstIdx,secondIdx);</span><br><span class="line">    //3.</span><br><span class="line">    for(int i=0;i&lt;SEGM;i++)</span><br><span class="line">        out[idx+i]=value[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="8-Quantization"><a href="#8-Quantization" class="headerlink" title="8.Quantization"></a>8.Quantization</h4><p>FloatFunctional会在prepare_qat之后activation_post_process会挂上FakeQuantize的hook，</p>
<p>然后就会是 <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/fake_quantize.py#L138-L156">https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/fake_quantize.py#L138-L156</a></p>
<p>observe去更新min_max min_max_observer里<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/observer.py#L432-L443">https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/observer.py#L432-L443</a></p>
<p>用到_calculate_qparams<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/36449ea93134574c2a22b87baad3de0bf8d64d42/torch/ao/quantization/observer.py#L253-L322">https://github.com/pytorch/pytorch/blob/36449ea93134574c2a22b87baad3de0bf8d64d42/torch/ao/quantization/observer.py#L253-L322</a></p>
<p>算出scale和zero_point <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/observer.py#L310-L365">https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/observer.py#L310-L365</a></p>
<p>Pytorch量化流程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/299108528">https://zhuanlan.zhihu.com/p/299108528</a></p>
<p>美团技术实践： <a target="_blank" rel="noopener" href="https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html">https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html</a></p>
<p>OBS OBQ 推导： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/656316235">https://zhuanlan.zhihu.com/p/656316235</a></p>
<p>GPTQ 进一步深挖： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646210009">https://zhuanlan.zhihu.com/p/646210009</a></p>
<p>LU分解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386954541">https://zhuanlan.zhihu.com/p/386954541</a></p>
<p>Cholesky分解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/387603571?utm_id=0">https://zhuanlan.zhihu.com/p/387603571?utm_id=0</a></p>
<p>FP8： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619431625">https://zhuanlan.zhihu.com/p/619431625</a></p>
<p>GPTQ AWQ 综述： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677899362">https://zhuanlan.zhihu.com/p/677899362</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/LLM/" rel="tag"># LLM</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/22/Python/" rel="prev" title="Python">
      <i class="fa fa-chevron-left"></i> Python
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/22/Cpp/" rel="next" title="Cpp">
      Cpp <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-LLM"><span class="nav-number">1.</span> <span class="nav-text">1.LLM</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF"><span class="nav-number">1.2.</span> <span class="nav-text">技术</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-CUDA"><span class="nav-number">2.</span> <span class="nav-text">2.CUDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Speculative-Decoding"><span class="nav-number">3.</span> <span class="nav-text">3.Speculative Decoding</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-Disaggregated-Inference"><span class="nav-number">4.</span> <span class="nav-text">4.Disaggregated Inference</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-vLLM"><span class="nav-number">5.</span> <span class="nav-text">2.vLLM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-Attention-%E4%BC%98%E5%8C%96"><span class="nav-number">6.</span> <span class="nav-text">3.Attention 优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-TensorRT-LLM"><span class="nav-number">7.</span> <span class="nav-text">4.TensorRT-LLM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-Coroutine-Thread-Process"><span class="nav-number">8.</span> <span class="nav-text">5.Coroutine &amp; Thread &amp; Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-Triton"><span class="nav-number">9.</span> <span class="nav-text">6.Triton</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-CUDA"><span class="nav-number">10.</span> <span class="nav-text">7.CUDA</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-Quantization"><span class="nav-number">11.</span> <span class="nav-text">8.Quantization</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
