<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/CV/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/CV/" class="post-title-link" itemprop="url">CV</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 20:10:40 / Modified: 20:10:49" itemprop="dateCreated datePublished" datetime="2025-02-22T20:10:40+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Transformer问题汇总：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266695736">https://zhuanlan.zhihu.com/p/266695736</a></p>
<p>CV：<a target="_blank" rel="noopener" href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing">https://github.com/WZMIAOMIAO/deep-learning-for-image-processing</a></p>
<h5 id="图像分类："><a href="#图像分类：" class="headerlink" title="图像分类："></a>图像分类：</h5><h5 id="kaggle-的-木薯叶分类-不同疾病下叶子的特征（ViT夺冠）"><a href="#kaggle-的-木薯叶分类-不同疾病下叶子的特征（ViT夺冠）" class="headerlink" title="kaggle 的 木薯叶分类 不同疾病下叶子的特征（ViT夺冠）"></a>kaggle 的 木薯叶分类 不同疾病下叶子的特征（ViT夺冠）</h5><p>1.model: 分类网络 efficientb0 b1 倍率代表卷积核个数 输入分辨率</p>
<p><strong>resnet</strong> 残差模块 梯度消失梯度爆炸 退化问题</p>
<p>残差结构 shortcut：$1\times 1$卷积降维,$3\times3$卷积, $1\times1$升维</p>
<p><strong>resnext</strong> 组卷积(对输入的fm，卷积核的channel进行分组，降低参数)</p>
<p><strong>mobilenet</strong> 利用 DW+PW 将分组卷积应用到极致，即网络的分组数与网络的channel数量相等，使网络的计算量减到最低，channel之间的交互由 point-wise</p>
<p> conv,即使用1x1的卷积进行channel之间的融合来进行补充。</p>
<p><strong>V2</strong> 倒残差结构 $Relu6(x)&#x3D;min(max(0,x),6)$ ReLU丢失低微特征信息</p>
<p><strong>V3</strong> SE模块（squeeze &amp; excitation）</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42617455/article/details/108165206">SE模块详解_Mounsey的博客-CSDN博客_se模块</a></p>
<p>S : fm上执行GAP（替代MLP 每个channel输出一个 all像素点的平均值）,fm被压缩为$1\times1\times C$ 的向量</p>
<p>E：两个FC（1&#x2F;4），两个激活函数，bottleneck结构；训练fm中每个channel的权重，</p>
<p> 加权后的fm作为下一层网络的input</p>
<p> hard-s: $x\frac{ReLU6(x+3)}{6}$ NAS搜参</p>
<p><strong>shufflenet</strong> 针对组卷积缺乏通道间的信息传递</p>
<p><strong>V2</strong> 4准则</p>
<p><strong>efficientnet</strong> 权衡分辨率 宽高 SE 改用Swish Sigmoid&#x3D;$x\sigma(x)$</p>
<p>训练尺寸大时速度慢 DWC逐元素慢 同等放大是次优的</p>
<p><strong>V2</strong> Fused-MBConv ：融合了DW卷积，SE模块，dropout</p>
<p><strong>Swin Transformer</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/121119988">https://blog.csdn.net/qq_37541097/article/details/121119988</a></p>
<p>Patch Merging，W-MSA，SW-MSA，相对位置偏移</p>
<p><strong>ConvNeXt</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/122556545">https://blog.csdn.net/qq_37541097/article/details/122556545</a></p>
<p>2.图像增强：</p>
<p><strong>albumentations</strong> ：基于OpenCV的快速训练数据增强库 可以使用 one of 组合 随机选择</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107399127/">albumentations 数据增强工具的使用 - 知乎 (zhihu.com)</a></p>
<p>其他增强：mixup cutmix cutout fmix snapmix</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_36618660/article/details/101633504">数据增强之mixup算法详解_sinat_36618660的博客-CSDN博客_mixup</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38715903/article/details/103999227">【论文阅读笔记】CutMix：数据增强_花噜噜酱的博客-CSDN博客_cutmix</a></p>
<p>对裁剪区域的边界框采样，作为对AB两张图裁剪区域的标定：</p>
<p>采样公式：$r_x\sim U(0,W),r_w&#x3D;W\sqrt{1-\lambda}$ ，裁剪区域面积作为权重</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42990464/article/details/105933992">数据增强之FMix_海里的羊的博客-CSDN博客</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/89ce7fdb9e12">图像的傅里叶变换 - 简书 (jianshu.com)</a></p>
<p><strong>Fmix</strong>: 利用掩膜来定义图像哪部分需要被考虑，对傅里叶空间采样的低频图像进行阈值处理从而去除噪声边界点，<br>因此能从图像中剪切出任意形状的部分，将其粘贴到相关图像上</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/kuweicai/article/details/111877458">数据增强（Data Augmentation）系列： SnapMix 原理及应用_生命在于折腾！-CSDN博客</a></p>
<p>生成 label 的时候，直接在原图中以被 cut 掉的面积作为权重不合理 计算每个 pixel 对标签的贡献度</p>
<p>首先计算输入图像的 CAM（Class Activation Mapping），归一化得到 SPM进行标签融合</p>
<p>$CAM(I_i)&#x3D;\phi(\sum_{l&#x3D;0}^dw_{y_i}^lF_l(I_i))$ 输入图片 上采样(分类权重$\times$fm)</p>
<p>3.优化器：adam + sgd 组合 &#x2F; 5折交叉验证 sgd</p>
<p>学习率：带重启的余弦退火 公式: $\eta_t&#x3D;\eta_{min}+\frac{1}{2}(1+\cos(\frac{T_{cur}}{T_i}\pi))$ 前几个epoch大学习率</p>
<p>ReduceLROnPlateau在发现loss不再降低或者acc不再提高之后，降低学习率</p>
<p>4.<strong>Focal loss</strong>：降低易分样本的权重，提高难分样本的权重 也能解决目标检测中样本不均衡</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113716961">Focal loss及多分类任务实现 - 知乎 (zhihu.com)</a></p>
<p>$FL(p_t)&#x3D;-\alpha(1-p_t)^{\gamma}\log(p_t)$ 概率较大的样本 趋近于0 $\alpha$提高误分类样本的权重</p>
<p>**label smoothing **：提高模型泛化能力；降低迭代次数 onehot的微调</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/302843504">Label Smoothing分析 - 知乎 (zhihu.com)</a></p>
<p>5.Grad-CAM：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/123089851">https://blog.csdn.net/qq_37541097/article/details/123089851</a></p>
<h5 id="目标检测："><a href="#目标检测：" class="headerlink" title="目标检测："></a>目标检测：</h5><p><strong>1.RCNN</strong>(CNN攻克目标检测的开山之作)</p>
<p>selective search算法生成1-2k个候选区域，</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27467369">目标检测（1）-Selective Search - 知乎 (zhihu.com)</a></p>
<p>将图像分割成很多很多的小块 合并规则如下：</p>
<p>颜色、纹理相近；合并后总面积最小；合并后的与其所属gt边界框boundng box公共面积大的</p>
<p>依次对区域用CNN做特征提取经过各个类别的SVM，来判别是否属于该类</p>
<p><strong>NMS非极大值抑制</strong>：detection输出个数未知$\rightarrow$提高召回率</p>
<p>一看就懂：<a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37605642/article/details/98358864">https://blog.csdn.net/m0_37605642/article/details/98358864</a></p>
<p>召回率：指模型找到所有某类目标的能力（所有标注的真实边界框有多少被预测出来了）</p>
<p>$Recall&#x3D;\frac{TP}{TP+FN}$</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/78504109">非极大值抑制Non-Maximum Suppression（NMS）一文搞定理论+多平台实现 - 知乎 (zhihu.com)</a></p>
<p>寻找得分最高的目标，计算与其他目标的iou，删除所有iou大于给定阈值的目标</p>
<p>回归器修正候选框位置：最小二乘法 线性回归 得到bounding box</p>
<p>不足：RoI获取慢；特征提取慢</p>
<p><strong>2.Fast RCNN</strong>（解决特征提取慢）</p>
<p>图像输入网络得到fm，将ss生成的候选框投影到fm上 得到相应的特征矩阵</p>
<p>通过ROI pooling 矩阵缩放到7$\times$7，展平后送入fc预测</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73138740">ROI Pooling和ROI Align - 知乎 (zhihu.com)</a></p>
<p>优点：一次性计算全图特征，不限制输入尺寸</p>
<p>正负样本：1张图 采64个region 16个采自IoU&gt;0.5,&lt;0.1标记为负样本</p>
<p>边界框回归器 输出N+1个类别 4个回归参数$d_x,d_y,d_w,d_h$</p>
<p>分类损失+回归损失 $smooth_{L_1}(x)&#x3D;0.5x^2, |x|-0.5$</p>
<p><strong>3.Faster RCNN</strong></p>
<p>图像输入CNN网络得到fm</p>
<p><strong>RPN</strong>生成候选框region proposals，softmax判断属于正负样本，再利用bounding box regression修正</p>
<p>anchors获得精确的proposals</p>
<p>ROI 收集输入的fm和proposals 提取proposal fm ，送入fc分类</p>
<p>利用 proposal fm 计算 proposal类别，再次bbr得到最终定位</p>
<p>Fast-RCNN</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59186710">Faster RCNN之RPN理解 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49897496">学一百遍都不算多的 Faster-RCNN - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/31426458">一文读懂Faster RCNN - 知乎 (zhihu.com)</a>(<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/49897496">https://zhuanlan.zhihu.com/p/49897496</a>)</p>
<p>RPN来 proposal ROI 原论文3尺度3比例 每个位置在原图对应9个anchor（候选窗口）</p>
<p>在一个滑动窗口上生成不同大小和长宽比例的anchor box，取定IoU的阈值来标定这些ab的正负。与gt有</p>
<p>最大IoU的或者&gt;0.7的为正。</p>
<p>于是传入RPN网络的样本数据被整理为anchor box 坐标 和 每个anchor box是否有物体（二分类标签）</p>
<p>RPN网络将每个样本映射为一个概率值和四个坐标值，概率值表示anchor box有物体的概率，坐标值用</p>
<p>于回归定义物体的位置。最后将二分类和坐标回归的损失统一起来，作为RPN网络的目标训练。</p>
<p>由RPN得到Region Proposal在根据概率值筛选后经过类似的标记过程，被传入Fast RCNN子网络，进行</p>
<p>多分类和坐标回归，同样用多任务损失将二者的损失联合训练。</p>
<p><strong>4.YOLO</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/litt1e/article/details/88907542">一文看懂YOLO v3_litt1e的博客-CSDN博客_yolov3</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/74540100">【论文解读】Yolo三部曲解读——Yolov2 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/76802514">【论文解读】Yolo三部曲解读——Yolov3 - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/94799295">IoU、GIoU、DIoU、CIoU损失函数的那点事儿 - 知乎 (zhihu.com)</a></p>
<p>Yolov5: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183838757">https://zhuanlan.zhihu.com/p/183838757</a></p>
<p>改进实践：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/509295685">https://zhuanlan.zhihu.com/p/509295685</a></p>
<p><strong>5.FCOS</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/124844726?spm=1001.2014.3001.5502">https://blog.csdn.net/qq_37541097/article/details/124844726?spm=1001.2014.3001.5502</a></p>
<p><strong>6.注释</strong></p>
<p>anchor box其实就是从训练集的所有ground truth box中统计(使用k-means)出来的在训练集中最经常出现的几个box形状和尺寸</p>
<p>anchor指人为初始给的先验框，一般在二阶段检测器的RPN阶段或者在一阶段检测器中设置</p>
<p>proposal指的是二阶段方法中RPN的输出框，也就是对anchor第一次做回归得到的结果</p>
<p>ROI指RPN阶段输出的proposal经过排序取topk，然后做nms取一定数量的框，用于第二阶段的再次精</p>
<p>修；在RCNN ，Fast RCNN方法中指通过选择性搜索生成的框</p>
<p>bounding box指proposal经过再次精修后的预测框</p>
<p><strong>7.其他补充：</strong></p>
<p>边界框概率分布：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/151398233">https://zhuanlan.zhihu.com/p/151398233</a></p>
<p>IOU收罗：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/151914931">https://zhuanlan.zhihu.com/p/151914931</a></p>
<p>Neck选择：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342011052">https://zhuanlan.zhihu.com/p/342011052</a></p>
<p>NMS汇总: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/157900024">https://zhuanlan.zhihu.com/p/157900024</a></p>
<p>目标检测backbone，neck，head：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93451942">https://zhuanlan.zhihu.com/p/93451942</a></p>
<p>MAP特别清楚：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/3dfb43aaa14c">https://www.jianshu.com/p/3dfb43aaa14c</a></p>
<p>目标检测中的mAP:</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/53405779/answer/429585383">https://www.zhihu.com/question/53405779/answer/429585383</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/53405779/answer/506000532">https://www.zhihu.com/question/53405779/answer/506000532</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/53405779/answer/2481182203">https://www.zhihu.com/question/53405779/answer/2481182203</a></p>
<p>FN：False Negative,被判定为负样本，但事实上是正样本<br>FP：False Positive,被判定为正样本，但事实上是负样本<br>TN：True Negative,被判定为负样本，事实上也是负样本<br>TP：True Positive,被判定为正样本，事实上也正样本</p>
<h5 id="语义分割："><a href="#语义分割：" class="headerlink" title="语义分割："></a>语义分割：</h5><p>转置卷积：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/120709865">https://blog.csdn.net/qq_37541097/article/details/120709865</a></p>
<p>膨胀卷积：<a target="_blank" rel="noopener" href="https://blog.csdn.net/tracelessle/article/details/106114975">https://blog.csdn.net/tracelessle/article/details/106114975</a></p>
<p>DeepLabV2：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/121752679">https://blog.csdn.net/qq_37541097/article/details/121752679</a> (ASPP)</p>
<p>DeepLabV3：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/121797301">https://blog.csdn.net/qq_37541097/article/details/121797301</a></p>
<h5 id="实例分割："><a href="#实例分割：" class="headerlink" title="实例分割："></a>实例分割：</h5><p>分离对象的前景与背景</p>
<p><strong>1.Mask-RCNN：</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37541097/article/details/123754766">https://blog.csdn.net/qq_37541097/article/details/123754766</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/407831250">https://zhuanlan.zhihu.com/p/407831250</a></p>
<p>RoIAlign替代RoIPool</p>
<p>mask：像素是否是目标的一部分</p>
<p>FCN从每一个ROI中预测出一个m*m大小的mask，这使得mask分支中的每个层能够明确的保持m×m空间布局;预测K个输出意在允许每个类都生成独立的掩膜，避免类间竞争，这样做解耦了掩膜和种类预测。</p>
<p>对于每一个ROI，mask分支有Km<em>m维度的输出，其对K个大小为m</em>m的mask进行编码，每一个mask有K个类别，即K个分辨率为m*m的二值的掩膜</p>
<p>FCN卷积化：是指图像语义分割的输出是个分割结果图，是二维数据</p>
<p>FPN: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/397293649">https://zhuanlan.zhihu.com/p/397293649</a></p>
<p>综述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/412675982">https://zhuanlan.zhihu.com/p/412675982</a></p>
<p>Resrep： <strong>卷积的等价性</strong></p>
<p>1*1理解成FC，所以它就是输入通道的线性重组（就是加权）</p>
<p>首先去理解为什么图中代码是shape一致的</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/laizi_laizi/article/details/110201274">https://blog.csdn.net/laizi_laizi/article/details/110201274</a></p>
<p>input x [9,4,10,11]</p>
<p>注意到kernel&#x3D;3，pad&#x3D;1；kernel&#x3D;1，pad&#x3D;0这是为了保证特征图大小也就是10，11不发生改变</p>
<p>而本身ConvM.weight.shape [3,4,3,3] ConvP [3,5,1,1] ConvA [5,4,3,3]</p>
<p>根据卷积 因为input和weight的第二个维度都是out_channel，所以要一致</p>
<p>所以input是ConA.w.p(1,0,2,3) 也就是[4,5,3,3] 让5对齐</p>
<p>卷积出来的结果就是 [4,3,3,3] permute之后恰好就是M的大小</p>
<p>又因为P来自于B，我们B(A)&#x3D;A 又验证了P(A)等价于M，也就是说剪了P就是剪了M</p>
<p>B是compactor,M本身权重的分配就是来自于A（不变）与P（剪完B之后获得） 所以这两者也是一致的</p>
<p>Rep本身是为了剪A，那构造B&#x3D;I,得到B(A)，只对B做裁剪，不影响原来A的结构，于是得到P(A)，</p>
<p>然后这个结果等价于生成一个M</p>
<h3 id="Deploy"><a href="#Deploy" class="headerlink" title="Deploy"></a>Deploy</h3><h4 id="1-ONNX"><a href="#1-ONNX" class="headerlink" title="1.ONNX"></a>1.ONNX</h4><p>onnxsurgeon: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/664242857">https://zhuanlan.zhihu.com/p/664242857</a></p>
<p>custom op onnx export ：<a target="_blank" rel="noopener" href="https://github.com/onnx/tutorials/blob/master/PyTorchCustomOperator/README.md">https://github.com/onnx/tutorials/blob/master/PyTorchCustomOperator/README.md</a></p>
<p>打印 node：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011622208/article/details/122260965">https://blog.csdn.net/u011622208/article/details/122260965</a></p>
<p>add node attribute: <a target="_blank" rel="noopener" href="https://github.com/onnx/onnx/issues/3546">https://github.com/onnx/onnx/issues/3546</a></p>
<p>tutorial: <a target="_blank" rel="noopener" href="https://github.com/onnx/tutorials/blob/master/PyTorchCustomOperator/README.md">https://github.com/onnx/tutorials/blob/master/PyTorchCustomOperator/README.md</a></p>
<h4 id="2-TensorRT"><a href="#2-TensorRT" class="headerlink" title="2.TensorRT"></a>2.TensorRT</h4><p>builder,engine,context简介：<a target="_blank" rel="noopener" href="https://blog.csdn.net/tengxunbc/article/details/116613328">https://blog.csdn.net/tengxunbc/article/details/116613328</a></p>
<p>[defaultAllocator.cpp::deallocate] Cuda Runtime (invalid argument)：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/2052">https://github.com/NVIDIA/TensorRT/issues/2052</a></p>
<p>Error Code 1: Myelin (Compiled against cuBLASLt 10.2.2.0 but running against cuBLASLt 11.4.2.0.)：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/1945">https://github.com/NVIDIA/TensorRT/issues/1945</a></p>
<p>最简python推理：<a target="_blank" rel="noopener" href="https://blog.csdn.net/JianguoChow/article/details/122684310">https://blog.csdn.net/JianguoChow/article/details/122684310</a></p>
<p>简单ResNet50：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/395590559">https://zhuanlan.zhihu.com/p/395590559</a></p>
<p>动态：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/548006090">https://zhuanlan.zhihu.com/p/548006090</a></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/387853124">https://zhuanlan.zhihu.com/p/387853124</a></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/299845547">https://zhuanlan.zhihu.com/p/299845547</a> （相应评论区）</p>
<p>int8量化配置撰写可参考：<a target="_blank" rel="noopener" href="https://github.com/ihongxx/Resnet50_TensorRT">https://github.com/ihongxx/Resnet50_TensorRT</a></p>
<p>YOLOv5 trt：<a target="_blank" rel="noopener" href="https://github.com/PICOPON/yolov5_engine">https://github.com/PICOPON/yolov5_engine</a></p>
<p> <a target="_blank" rel="noopener" href="https://github.com/ihongxx/Yolov5_TensorRt">https://github.com/ihongxx/Yolov5_TensorRt</a></p>
<p>YOLOv7 &amp; others trt：<a target="_blank" rel="noopener" href="https://github.com/shouxieai/tensorRT_Pro">https://github.com/shouxieai/tensorRT_Pro</a></p>
<p>bilibili：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Xw411f7FW?p=4&vd_source=8683b4f3e93da577cf9216975b225ee5">https://www.bilibili.com/video/BV1Xw411f7FW?p=4&amp;vd_source=8683b4f3e93da577cf9216975b225ee5</a></p>
<p>TensorRT Plug-in：<a target="_blank" rel="noopener" href="https://github.com/wang-xinyu/tensorrtx">https://github.com/wang-xinyu/tensorrtx</a></p>
<p>TensorRT 简易：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/547624036">https://zhuanlan.zhihu.com/p/547624036</a></p>
<p>async vs sync：<a target="_blank" rel="noopener" href="https://gist.github.com/CasiaFan/c8a380aabdc08e1fdcf6310c78431106">https://gist.github.com/CasiaFan/c8a380aabdc08e1fdcf6310c78431106</a></p>
<p>dynamic输入trt报错：<a target="_blank" rel="noopener" href="https://blog.csdn.net/XCCCCZ/article/details/123009816">https://blog.csdn.net/XCCCCZ/article/details/123009816</a></p>
<p>execute_async：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT/issues/930">https://github.com/NVIDIA/TensorRT/issues/930</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/Misc/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/Misc/" class="post-title-link" itemprop="url">Misc</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:58:33 / Modified: 20:09:38" itemprop="dateCreated datePublished" datetime="2025-02-22T19:58:33+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>– [0.Errors](# 0.Errors)</p>
<p>– [2.Math](# 2.Math)</p>
<p>– [4.语音](# 4.语音)</p>
<p>– [6.Docker &amp; VSCode](# 6.Docker &amp; VSCode)</p>
<p>– [7.Git](# 7.Git)</p>
<p>Gameboy： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/676908347">https://zhuanlan.zhihu.com/p/676908347</a></p>
<p>程序员的护城河： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/604014261/answer/3179844243">https://www.zhihu.com/question/604014261/answer/3179844243</a></p>
<p>男人老了的标志是什么： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/515229945/answer/2708757895">https://www.zhihu.com/question/515229945/answer/2708757895</a></p>
<p>如何看待自己终有一天会死去： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/23178728/answer/2682474442">https://www.zhihu.com/question/23178728/answer/2682474442</a></p>
<p>我在贵司这三年： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654508101">https://zhuanlan.zhihu.com/p/654508101</a></p>
<p>什么是人生的最顶级享受： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/538449801/answer/2939200080">https://www.zhihu.com/question/538449801/answer/2939200080</a></p>
<p>被人真心爱着（爱过）是什么感觉： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/55774089/answer/3236902813">https://www.zhihu.com/question/55774089/answer/3236902813</a></p>
<p>为什么我特别讨厌情绪价值这个词： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/653121998/answer/3476785222">https://www.zhihu.com/question/653121998/answer/3476785222</a></p>
<p>妙趣横生的水文： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/537342945/answer/2640347949">https://www.zhihu.com/question/537342945/answer/2640347949</a></p>
<p>用词精准： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/630165959/answer/3497967251">https://www.zhihu.com/question/630165959/answer/3497967251</a></p>
<p>理性的偏见： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/31871242/answer/3487653703">https://www.zhihu.com/question/31871242/answer/3487653703</a></p>
<p>30 岁的人生道理：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/419127632/answer/3104096147">https://www.zhihu.com/question/419127632/answer/3104096147</a></p>
<p>谈恋爱：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/658903348/answer/89212476815">https://www.zhihu.com/question/658903348/answer/89212476815</a></p>
<h4 id="1-Errors"><a href="#1-Errors" class="headerlink" title="1.Errors"></a>1.Errors</h4><p>清除僵尸显存：</p>
<p>fuser -v &#x2F;dev&#x2F;nvidia1 | xargs -t -n 1 kill -9</p>
<p>ps -ef | grep ‘python’ | grep -v grep | awk ‘{print $2}’ | xargs kill -9</p>
<p>chrome keyring： <a target="_blank" rel="noopener" href="https://forum.manjaro.org/t/how-do-i-disable-enter-password-for-default-keyring-when-opening-google-chrome/76405">https://forum.manjaro.org/t/how-do-i-disable-enter-password-for-default-keyring-when-opening-google-chrome/76405</a></p>
<p>&#x2F;dev&#x2F;sda1：clean &amp; nvrm can’t find an irq for your nvidia card： <a target="_blank" rel="noopener" href="https://askubuntu.com/questions/214504/splash-screen-flickers-during-every-boot/1386363#1386363">https://askubuntu.com/questions/214504/splash-screen-flickers-during-every-boot/1386363#1386363</a></p>
<p>host key verifications failed：<a target="_blank" rel="noopener" href="https://blog.csdn.net/wd2014610/article/details/85639741">https://blog.csdn.net/wd2014610/article/details/85639741</a></p>
<p>hexo archive 404：<a target="_blank" rel="noopener" href="https://theme-next.js.org/docs/theme-settings/custom-pages">https://theme-next.js.org/docs/theme-settings/custom-pages</a></p>
<p>VPN：<a target="_blank" rel="noopener" href="https://www.duyaoss.com/archives/3/">https://www.duyaoss.com/archives/3/</a></p>
<p> <a target="_blank" rel="noopener" href="https://mxwljsq.xyz/user/shop">https://mxwljsq.xyz/user/shop</a></p>
<h4 id="2-Ubuntu"><a href="#2-Ubuntu" class="headerlink" title="2.Ubuntu"></a>2.Ubuntu</h4><p>dpkg 依赖究极解法：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/291606128/answer/1194596591">https://www.zhihu.com/question/291606128/answer/1194596591</a></p>
<p>文件管理卡死 nautilus： <a target="_blank" rel="noopener" href="https://blog.csdn.net/u010712012/article/details/89892702">https://blog.csdn.net/u010712012/article/details/89892702</a></p>
<p>90s： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44333320/article/details/134704863">https://blog.csdn.net/qq_44333320/article/details/134704863</a></p>
<p>failed to grab modset ownership： <a target="_blank" rel="noopener" href="https://blog.csdn.net/CC__Vbird__YDD/article/details/128986599">https://blog.csdn.net/CC__Vbird__YDD/article/details/128986599</a></p>
<p>恢复 Downloads 文件夹：<a target="_blank" rel="noopener" href="https://askubuntu.com/questions/1321623/accidentally-deleted-the-downloads-folder">https://askubuntu.com/questions/1321623/accidentally-deleted-the-downloads-folder</a></p>
<p>du -ah –max-depth&#x3D;1 ~ | sort -n 可以去掉~</p>
<p>完美卸载Ubuntu：<a target="_blank" rel="noopener" href="https://blog.csdn.net/ZChen1996/article/details/115436436">https://blog.csdn.net/ZChen1996/article/details/115436436</a></p>
<p>debain unable to bind to codec,导致无法开机： <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/2a1cce843983">https://www.jianshu.com/p/2a1cce843983</a></p>
<p>win10文件夹只读属性无法去除怎么修改： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/409460098">https://www.zhihu.com/question/409460098</a></p>
<p>Ubuntu软件和更新无响应： <a target="_blank" rel="noopener" href="https://askubuntu.com/questions/1271611/app-software-updates-not-responding">https://askubuntu.com/questions/1271611/app-software-updates-not-responding</a></p>
<p>apt update no release： <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46412818/article/details/116806127">https://blog.csdn.net/weixin_46412818/article/details/116806127</a></p>
<p>完美解决关机左上角光标闪烁： <a target="_blank" rel="noopener" href="https://blog.csdn.net/X_T_S/article/details/110144658">https://blog.csdn.net/X_T_S/article/details/110144658</a></p>
<p>wayland： <a target="_blank" rel="noopener" href="https://askubuntu.com/questions/1488844/ubuntu-22-04-boots-to-a-black-screen-with-a-visible-cursor">https://askubuntu.com/questions/1488844/ubuntu-22-04-boots-to-a-black-screen-with-a-visible-cursor</a></p>
<p>deprecated key： <a target="_blank" rel="noopener" href="https://forums.insynchq.com/t/apt-key-deprecation-with-workaround/18100">https://forums.insynchq.com/t/apt-key-deprecation-with-workaround/18100</a></p>
<p>override dns：<a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/588658/override-ubuntu-20-04-dns-using-systemd-resolved">https://unix.stackexchange.com/questions/588658/override-ubuntu-20-04-dns-using-systemd-resolved</a></p>
<p>systemd-journald crash： <a target="_blank" rel="noopener" href="https://askubuntu.com/questions/1173738/crash-systemd-journal-failed-to-write-entry-ignoring-read-only-file-system-on">https://askubuntu.com/questions/1173738/crash-systemd-journal-failed-to-write-entry-ignoring-read-only-file-system-on</a></p>
<p>升级内核： <a target="_blank" rel="noopener" href="https://itsfoss.com/upgrade-linux-kernel-ubuntu">https://itsfoss.com/upgrade-linux-kernel-ubuntu</a></p>
<p>wrong fs type,bad option,bad superlock &amp;</p>
<p>dmesg | tail ： see parse_options()： Unrecognized mount option windows_names.<a target="_blank" rel="noopener" href="https://github.com/storaged-project/udisks/issues/932">https://github.com/storaged-project/udisks/issues/932</a></p>
<p>ImportError： &#x2F;usr&#x2F;lib&#x2F;x86_64-linux-gnu&#x2F;libstdc++.so.6： version &#96;GLIBCXX_3.4.xx‘ not found<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_30653631/article/details/107620137">https://blog.csdn.net/qq_30653631/article/details/107620137</a></p>
<p>libstdc++.so.6：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38342510/article/details/140452160">https://blog.csdn.net/qq_38342510/article/details/140452160</a></p>
<h4 id="3-Math"><a href="#3-Math" class="headerlink" title="3.Math"></a>3.Math</h4><p>凸优化：<a target="_blank" rel="noopener" href="https://www.zhihu.com/column/convex">https://www.zhihu.com/column/convex</a></p>
<p> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/270053227/answer/3138297987">https://www.zhihu.com/question/270053227/answer/3138297987</a></p>
<p> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/49689245/answer/3148945726">https://www.zhihu.com/question/49689245/answer/3148945726</a></p>
<p> <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/68418633/answer/3178197426">https://www.zhihu.com/question/68418633/answer/3178197426</a></p>
<p>一个大学生的日常笔记：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28617379">https://zhuanlan.zhihu.com/p/28617379</a></p>
<p>贝尔曼方程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86525700">https://zhuanlan.zhihu.com/p/86525700</a></p>
<p>矩阵求导系列：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/263777564">https://zhuanlan.zhihu.com/p/263777564</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273729929">https://zhuanlan.zhihu.com/p/273729929</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/288541909">https://zhuanlan.zhihu.com/p/288541909</a></p>
<h4 id="4-Docker-VSCode"><a href="#4-Docker-VSCode" class="headerlink" title="4.Docker &amp; VSCode"></a>4.Docker &amp; VSCode</h4><p>vscode docker 插件权限：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/680737356">https://zhuanlan.zhihu.com/p/680737356</a></p>
<p>docker pull：<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/2485043">https://cloud.tencent.com/developer/article/2485043</a></p>
<p>修复 25.04 plucky 无 release 文件，无法更新源，尤其常发生在 manual 显卡驱动后：</p>
<p>&#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F; 删除过时的 docker 相关</p>
<p>完美解决 Error response from daemon: could not select device driver “” with capabilities: [[gpu]]</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html</a> 装 nvidia-container-toolkit</p>
<p>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin</p>
<p>sudo apt-get install -y nvidia-container-toolkit</p>
<p>根据教程 vim &#x2F;etc&#x2F;os-release 修改为 24.10 jammy &amp; systemctl daemon-reload &amp; systemctl restart docker 即可</p>
<p>openvino c++：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21055820472">https://zhuanlan.zhihu.com/p/21055820472</a></p>
<p>Error response from daemon： could not select device driver： <a target="_blank" rel="noopener" href="https://blog.csdn.net/li4692625/article/details/123015840">https://blog.csdn.net/li4692625/article/details/123015840</a></p>
<p>修改ssh默认端口： <a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1253389">https://developer.aliyun.com/article/1253389</a></p>
<p>vscode 修复Remote-SSH XHR failed： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_45654306/article/details/132047411">https://blog.csdn.net/qq_45654306/article/details/132047411</a> strip 参数很关键</p>
<p>docker clean： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386025157">https://zhuanlan.zhihu.com/p/386025157</a></p>
<p>snap remove docker： <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/75604510/errno-13-permission-denied-%C2%B4-usr-lib-python3-10-pycache-future">https://stackoverflow.com/questions/75604510/errno-13-permission-denied-%C2%B4-usr-lib-python3-10-pycache-future</a></p>
<p>ubuntu install docker engine： <a target="_blank" rel="noopener" href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a></p>
<p>各container明细： <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-22-12.html">https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-22-12.html</a></p>
<p>there-was-an-error-while-opening-file-handle： <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/68052984/there-was-an-error-while-opening-file-handle">https://stackoverflow.com/questions/68052984/there-was-an-error-while-opening-file-handle</a></p>
<p>Error response from daemon： could not select device driver “” with capabilities： [[gpu]].： <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/123760614">https://blog.csdn.net/weixin_44966641/article/details/123760614</a></p>
<p>unsupported distribution： <a target="_blank" rel="noopener" href="https://blog.csdn.net/dou3516/article/details/130228298">https://blog.csdn.net/dou3516/article/details/130228298</a></p>
<h4 id="5-Git"><a href="#5-Git" class="headerlink" title="5.Git"></a>5.Git</h4><p>git ssh key：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42310154/article/details/118340458">https://blog.csdn.net/weixin_42310154/article/details/118340458</a></p>
<p>撤销 git add . ： git reset HEAD</p>
<p>git clone –depth&#x3D;1：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/ppblackboy/article/details/135914344">https://blog.csdn.net/ppblackboy/article/details/135914344</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_30252465/article/details/125762378">https://blog.csdn.net/qq_30252465/article/details/125762378</a></p>
<p>git file mode：git config –add core.filemode false</p>
<p>git常用命令： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/105966867">https://zhuanlan.zhihu.com/p/105966867</a></p>
<p>1.config： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12679145.html">https://www.cnblogs.com/AhuntSun-blog/p/12679145.html</a></p>
<p>① 暂存区还原至工作区的三种方法：</p>
<p>git rm –cached 文件必须得经过一次提交</p>
<p>git restore –stage</p>
<p>git reset HEAD</p>
<p>② 撤销工作区文件操作（修改、删除等）</p>
<p>git checkout</p>
<p>git restore</p>
<p>③ 日志</p>
<p>git log –graph –pretty&#x3D;oneline –abbrev-commit</p>
<p>git log –all –decorate –oneline –graph 一次性查看所有分支及其提交记录</p>
<p>git blame 查看修改日志</p>
<p>git reflog 查看 git 相关操作日志</p>
<p>2.rm diff reset restore rebase ：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12685415.htmlrm">https://www.cnblogs.com/AhuntSun-blog/p/12685415.htmlrm</a></p>
<p>① git rm</p>
<p>git rm 删除版本库中的文件，如果删除工作区会报找不到文件的错误；删除暂存区则会说这些文件曾被改动</p>
<p>git rm 完成了两步操作，删除版本库中文件+将删除操作纳入暂存区，下次提交删除完成</p>
<p>② git diff</p>
<p>diff -u a b 显示详细信息 —a 表示 a 为原文件 +++b 表示 b 为目标文件</p>
<p>git diff &lt;暂存区&gt; &lt;工作区&gt; 的同一文件</p>
<p>git diff commit_id 比较版本库与工作区的同一文件</p>
<p>git diff HEAD 比较最新提交和工作区的同一文件</p>
<p>git diff –cached commit_id 比较版本库和暂存区中的同一文件</p>
<p>git diff –cached 比较最新提交和暂存区中的同一文件</p>
<p>③ 撤销修改：</p>
<p>暂存区恢复到工作区： 见1</p>
<p>撤销工作区： git restore&#x2F;checkout</p>
<p>git commit –amend -m “注释” 修改上一次的提交信息</p>
<p>git rebase -i commit_id 显示 commit_id 之后的提交信息</p>
<p>pick 可修改为 reword 直接修改提交注释；edit 停下 rebase 进程编辑提交，随后通过 git rebase –continue 继续进行 rebase</p>
<p>git rebase -i HEAD~n 显示最近 n 次提交记录</p>
<p>3.branch merge： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12694197.html">https://www.cnblogs.com/AhuntSun-blog/p/12694197.html</a></p>
<p>git branch -av 显示所有本地分支与本地远程分支以及对应分支上的最新提交信息</p>
<p>git checkout - 切换回上一次分支</p>
<p>git branch -m 分支重命名</p>
<p>git branch -d 不能删除当前所在的分支，当需要删除的分支有master没有的内容且删前没有合并时会报错，则需要使用 -D 强行删除</p>
<p>HEAD 指针指向当前分支；master 指针指向提交</p>
<p>4.reset revert checkout： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12700155.html">https://www.cnblogs.com/AhuntSun-blog/p/12700155.html</a></p>
<p>① git reset HEAD^ 回退一次提交 HEAD^^ 两次 ~n n次</p>
<p>–mixed 默认，文件回退到工作区，只保留工作区丢弃暂存区文件</p>
<p>–sort 回退到暂存区，保留工作区和暂存区</p>
<p>–hard 回退到修改前，丢弃工作区和暂存区</p>
<p>② git revert –no-edit -n (-no-commit) 修改已被撤销，但重做的操作还未提交，可以随意修改重做过程中不满意的地方</p>
<p>针对合并冲突使用 git mergetool</p>
<p>③ git checkout 创建游离提交</p>
<p>如果没有保存就从游离的提交上切换到其他分支，这一修改就会被 checkout 命令覆盖，所以在切换前需要进行一次提交；</p>
<p>此时还会出现提示，如果没有任何分支指向刚才游离 commit 链中进行的提交，那么该提交就会被忽略，所以需要创建一个分支保存指向这条游离 commit 链： git branch</p>
<p>综上，只有三种方式可以自由地前后回退： git reset –hard git revert git checkout</p>
<p>④ git stash</p>
<p>位于工作区和暂存区中的修改都会被 checkout 覆盖</p>
<p>恢复通过 git stash 保存到 stash 中的修改可以通过 git stash list 查看该分支上被 stash 保存的修改</p>
<p>git stash pop 恢复 stash 中存储的最新一次修改并从 stash 中删除</p>
<p>git stash apply 取出 stash 中最新修改与分支合并，但 stash 中存储的该修改并不会被删除</p>
<p>git stash apply stash@{n} 从 stash list 中恢复特定的修改，并且不删除 stash 中的该修改</p>
<p>用 git stash save 继续保存修改</p>
<p>5.git tag： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12727188.html">https://www.cnblogs.com/AhuntSun-blog/p/12727188.html</a></p>
<p>git log –all –decorate –oneline –graph</p>
<p>git tag -a -m</p>
<p>git tag list</p>
<p>git show</p>
<p>git tag -l</p>
<p>git push origin –tag</p>
<p>git push origin ： &#x2F; git push origin –delete</p>
<p>git tag -d</p>
<p>git checkout</p>
<p>6.cherry-pick &amp; rebase： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12732946.html">https://www.cnblogs.com/AhuntSun-blog/p/12732946.html</a></p>
<p>cherry-pick： 先复制，再拼接 | 无论内容是否冲突，合并过程都会出现冲突</p>
<p>git checkout + branch -d dev + checkout -b dev</p>
<p>rebase -i： pick &#x2F; reword &#x2F; edit &#x2F; squash</p>
<p>7.submodule &amp; subtree： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/AhuntSun-blog/p/12736934.html">https://www.cnblogs.com/AhuntSun-blog/p/12736934.html</a></p>
<p>git submodule add</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/Cpp/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/Cpp/" class="post-title-link" itemprop="url">Cpp</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:58:27 / Modified: 20:10:08" itemprop="dateCreated datePublished" datetime="2025-02-22T19:58:27+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>数组退化为指针其实就是丢失了长度信息，3 个特例：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/56442598/answer/202694271">https://www.zhihu.com/question/56442598/answer/202694271</a></p>
<p>真实的数组指针：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33911187">https://zhuanlan.zhihu.com/p/33911187</a></p>
<p>虽然可以保留数组信息，但是函数的声明必须把数组长度表示出来，这意味着必须事先知道长度，而且不能改变，这就限制了函数的能力</p>
<p>所以C的处理方式是退化数组为指针，然后加上数组长度，C++加入了 <strong>std::array&lt;type, length&gt;</strong> 的容器</p>
<p>POD：plain old data，简单数据，没有动态分配部分，没有间接引用、没有构造过程、可以直接复制内存的直接数据</p>
<p>清晰的左右值，左右引用：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/2ed2689afa49">https://www.jianshu.com/p/2ed2689afa49</a></p>
<p>左值右值：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/402251966">https://zhuanlan.zhihu.com/p/402251966</a></p>
<p>调用一个返回引用的函数得到左值，其他返回类型得到右值</p>
<p>this是一个常量指针，不允许改变其保存的地址（顶层const）</p>
<p>重载运算符本质上是函数，也有其返回类型与参数列表;如果一个运算符是成员函数，其左侧运算对象就绑定到隐式的this参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class Foo</span><br><span class="line">&#123;</span><br><span class="line">public:</span><br><span class="line">	Fool&amp; operator=(const Foo&amp;) // 拷贝赋值运算符接受一个与其所在类相同类型的参数:</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>子类不能访问父类的private，但能通过父类的protected、private方法间接访问父类的private</p>
<p>子类通过public继承不会改变父类的数据属性；protected继承private是private，其余都是protected；private继承全是private</p>
<p>父类的原属性并没改变，降级的是子类在父类的个别成员属性</p>
<p>std::move &amp;&amp; std::forward：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/384316039">https://zhuanlan.zhihu.com/p/384316039</a></p>
<p>std::forward ，可以保持原始参数的类型，将实参从右值引用的左值，变成了本身就是右值引用</p>
<p>restrict: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/349726808">https://zhuanlan.zhihu.com/p/349726808</a></p>
<p>C 指针层级理解:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660265595">https://zhuanlan.zhihu.com/p/660265595</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660539465">https://zhuanlan.zhihu.com/p/660539465</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660678544">https://zhuanlan.zhihu.com/p/660678544</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660828422">https://zhuanlan.zhihu.com/p/660828422</a></p>
<p>题:</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661249637">https://zhuanlan.zhihu.com/p/661249637</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661378386">https://zhuanlan.zhihu.com/p/661378386</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661767815">https://zhuanlan.zhihu.com/p/661767815</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662313049">https://zhuanlan.zhihu.com/p/662313049</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662640581">https://zhuanlan.zhihu.com/p/662640581</a></p>
<p>const 一语中的: <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/443195492/answer/1723886545">https://www.zhihu.com/question/443195492/answer/1723886545</a></p>
<p>extern C: <a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_46606290/article/details/119973574">https://blog.csdn.net/m0_46606290/article/details/119973574</a></p>
<p>cpu sgemm: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/684812336+">https://zhuanlan.zhihu.com/p/684812336+</a></p>
<p>线程池：</p>
<p>简易线程池：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356904887">https://zhuanlan.zhihu.com/p/356904887</a></p>
<p>for(;;) vs while(1)：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44395686/article/details/103409425">https://blog.csdn.net/weixin_44395686/article/details/103409425</a></p>
<p>notify_one&#x2F;all：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_43743711/article/details/115803461">https://blog.csdn.net/weixin_43743711/article/details/115803461</a></p>
<p>lock：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91062516">https://zhuanlan.zhihu.com/p/91062516</a></p>
<p>静态库编译：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/595527528">https://zhuanlan.zhihu.com/p/595527528</a></p>
<h4 id="Vim"><a href="#Vim" class="headerlink" title="Vim"></a>Vim</h4><p>w右跳到下一个单词开头 W还能无视一些符号(破折号) b 向左 e跳转到下一个单词的末尾</p>
<p>:w test.txt :wqa qa退出所有窗口 ZZ 退出 ZQ不保存退出</p>
<p>0 $ 行首行尾 ^非空格行首 3$ 下3行的行尾 3- 上3行 + 下 跳到行首</p>
<p>v(visualize) bve选中单词 u撤销</p>
<p>x删除 d在normal模式下还会继续等待输入 D &#x3D; d$ 直接删至行尾</p>
<p>cw直接删掉并立即进入插入模式 cw&#x3D;dwi c3w cW</p>
<p>g跳转到第一行 G跳转到最后一行 66G(行尾)66gg(行首)</p>
<p>&#x2F;kwx向下查找 n跳转到下一个 N往上 ？向上找 n上N下 单词# 向上找 *向下找</p>
<p>选中当前单词bve viw c修改 ciw b 修改a&#x3D;x为b&#x3D;x</p>
<p>f b 查找第一个b ；跳转下一个 cf&#x3D; cfb 当前到&#x3D;(b)全部重命名 包含&#x3D; ct&#x3D; 不包含&#x3D;</p>
<p>I# 在^处变为注释 I当前行开头插入 A末尾 i向前插入 a向后插入 insert append</p>
<p>录制宏：q u xj q 录入u寄存器 5@u反复调用 j向下</p>
<p>:norm 0x 0行首 即删除行首注释 :norm I重新注释</p>
<p>ctrl+u &#x2F; d 上下翻页 ctrl+y &#x2F; e 上下单行</p>
<p>:s&#x2F;debian&#x2F;ubuntu&#x2F;g多次文本替换 全局替换：ggVG全部选中 或将标记符换成%</p>
<p>o向下新起一行插入 O向上 5o world自动创建5行world</p>
<p>D&#x3D;d$ 删除该行后续内容 C&#x3D;c$额外进入insert</p>
<p>vi&lt; va&lt; 选中&lt;&gt;内&#x2F;包含&lt;&gt;的内容 “同理</p>
<p>ciw修改当前word caw会连同当前空格一并修改</p>
<p>%可以在两头的{}互换 &lt;&lt;减小缩进 &gt;&gt;增大 &#x3D;&#x3D;自动调整缩进</p>
<p>vi {选中花括号内 a含{} {上一段落 }下 ()在多个函数之间跳转</p>
<p>ggvG :s&#x2F;\s*$&#x2F;&#x2F; 去除末尾不该有的空格</p>
<p>v选择 V选择整行 y复制 yyp 向后复制1行 yyP向前</p>
<p>dd删除一整行 ddp互换两行 xp交换两个字符的位置</p>
<p>5s修改当前5个字符 r仅修改当前字符</p>
<p>:sh g++ tset.cpp &amp;&amp; .&#x2F;a.out exit &#x2F; ctrl+d :nnoremap <F8> : sh <CR></p>
<p>%可以直接替换当前的文件名 :nnoremap <F5> :wa<CR>::!g++ test.cpp -o a.out &amp;&amp; .&#x2F;a.out<CR></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">run: hello</span><br><span class="line">	./hello</span><br><span class="line">hello: hello.cpp</span><br><span class="line">	g++ hello.cpp -o hello</span><br></pre></td></tr></table></figure>

<p>:make :cw直接进入调错窗口</p>
<p>ctrl+i ctrl+o 历史页面跳转 gf向内跳转</p>
<p>:ls查看缓冲区 :b1跳转到1号缓冲</p>
<h4 id="CMake"><a href="#CMake" class="headerlink" title="CMake"></a>CMake</h4><p>本地cmake: <a target="_blank" rel="noopener" href="https://blog.csdn.net/mj412828668/article/details/135766455">https://blog.csdn.net/mj412828668/article/details/135766455</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bookstack.cn/read/CMake-Cookbook/README.md">https://www.bookstack.cn/read/CMake-Cookbook/README.md</a></p>
<p>在开发过程碰到需要在上级目录中构建，而源代码又分别写在下级目录的情况，同时又要根据不同的情况选择性地添加不同的源代码进行编译，所以考虑将需要编译的源代码放到一个 cmake 列表中。但是 set() 对应生成的变量都是局部变量（即不同的目录下不共用），于是使用 set_property() 命令。</p>
<p>undefined symbol：<a target="_blank" rel="noopener" href="https://blog.csdn.net/buknow/article/details/96130049">https://blog.csdn.net/buknow/article/details/96130049</a></p>
<p>INTERFACE: 在interface后面引入的库不会被链接到你的target中，只会导出符号</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/znsoft/article/details/119035578">https://blog.csdn.net/znsoft/article/details/119035578</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 寻找 python 解释器</span><br><span class="line">find_package(PythonInterp REQUIRED)</span><br><span class="line"># Python 头文件和库的模块，称为FindPythonLibs.cmake</span><br><span class="line">find_package(PythonLibs $&#123;PYTHON_VERSION_MAJOR&#125;.$&#123;PYTHON_VERSION_MINOR&#125; EXACT REQUIRED)</span><br><span class="line"># 执行 python 命令</span><br><span class="line">execute_process(</span><br><span class="line">  COMMAND</span><br><span class="line">      $&#123;PYTHON_EXECUTABLE&#125; &quot;-c&quot; &quot;print(&#x27;Hello, world!&#x27;)&quot;</span><br><span class="line">  RESULT_VARIABLE _status</span><br><span class="line">  OUTPUT_VARIABLE _helftflo_world</span><br><span class="line">  ERROR_QUIET</span><br><span class="line">  OUTPUT_STRIP_TRAILING_WHITESPACE</span><br><span class="line">  )</span><br><span class="line">  </span><br><span class="line"># PYTHON_EXECUTABLE：Python解释器到可执行文件的路径</span><br><span class="line"></span><br><span class="line"># 软件包没有安装在标准位置时，CMake无法正确定位它们,可使用CLI的-D参数告诉CMake查看特定的位置</span><br><span class="line">$ cmake -DPYTHON_EXECUTABLE=/custom/location/python ..</span><br></pre></td></tr></table></figure>

<p>其他常用操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line">$ cmake -D CMAKE_CXX_FLAGS=&quot;-fno-exceptions -fno-rtti&quot; .. # 编译项目时，禁用异常和运行时类型标识(RTTI)</span><br><span class="line"></span><br><span class="line">include(CMakeDependentOption)</span><br><span class="line"># 相当于 if M_S_L==off: U_L=on</span><br><span class="line">cmake_dependent_option(</span><br><span class="line">    MAKE_STATIC_LIBRARY &quot;Compile sources into a static library&quot; OFF</span><br><span class="line">    &quot;USE_LIBRARY&quot; ON</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"># 打印编译器标志</span><br><span class="line">message(&quot;C++ compiler flags: $&#123;CMAKE_CXX_FLAGS&#125;&quot;)</span><br><span class="line"></span><br><span class="line"># 设置编译选项 可以是链接库也可以是可执行文件</span><br><span class="line">target_compile_options(geometry</span><br><span class="line">  PRIVATE</span><br><span class="line">    $&#123;flags&#125;</span><br><span class="line">  ) </span><br><span class="line"># INTERFACE，只应用于指定目标，并传递给与目标相关的目标</span><br><span class="line"></span><br><span class="line"># 设定语言标准</span><br><span class="line">set_target_properties(animals</span><br><span class="line">  PROPERTIES</span><br><span class="line">    CXX_STANDARD 14</span><br><span class="line">    CXX_EXTENSIONS OFF  # 只启用ISO C++标准的编译器标志，而不使用特定编译器的扩展</span><br><span class="line">    CXX_STANDARD_REQUIRED ON  # 如果off 就先从 c++20 17往下找</span><br><span class="line">    POSITION_INDEPENDENT_CODE 1</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># 控制流</span><br><span class="line">list(</span><br><span class="line">  APPEND sources_with_lower_optimization</span><br><span class="line">    geometry_circle.cpp</span><br><span class="line">    geometry_rhombus.cpp</span><br><span class="line">  )</span><br><span class="line">foreach(_source IN LISTS sources_with_lower_optimization)</span><br><span class="line">  set_source_files_properties($&#123;_source&#125; PROPERTIES COMPILE_FLAGS -O2)</span><br><span class="line">  message(STATUS &quot;Appending -O2 flag for $&#123;_source&#125;&quot;)</span><br><span class="line">  get_source_file_property(_flags $&#123;_source&#125; COMPILE_FLAGS)</span><br><span class="line">  message(STATUS &quot;Source $&#123;_source&#125; has the following extra COMPILE_FLAGS: $&#123;_flags&#125;&quot;)</span><br><span class="line">endforeach()</span><br><span class="line"></span><br><span class="line">if(CMAKE_SIZEOF_VOID_P EQUAL 8)</span><br><span class="line">  target_compile_definitions(arch-dependent PUBLIC &quot;IS_64_BIT_ARCH&quot;)</span><br><span class="line">elseif(CMAKE_HOST_SYSTEM_PROCESSOR MATCHES &quot;x86_64&quot;)</span><br><span class="line">  message(STATUS &quot;x86_64 architecture detected&quot;)</span><br><span class="line">  </span><br><span class="line"># 寻找CheckCXXCompilerFlag.cmake标准模块文件</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line"># 检查 -march=native编译器标志是否工作</span><br><span class="line">check_cxx_compiler_flag(&quot;-march=native&quot; _march_native_works)</span><br><span class="line"></span><br><span class="line"># 打印模块</span><br><span class="line">include(CMakePrintHelpers)</span><br><span class="line">cmake_print_variables(_status _hello_world)</span><br><span class="line"></span><br><span class="line"># pkg-config检测外部库</span><br><span class="line">find_package(PkgConfig REQUIRED QUIET)  # 传递QUIET参数，只有在找不到 pkg-config时，CMake才会报错</span><br><span class="line">pkg_search_module(</span><br><span class="line">  ZeroMQ</span><br><span class="line">  REQUIRED</span><br><span class="line">      libzeromq libzmq lib0mq</span><br><span class="line">  IMPORTED_TARGET</span><br><span class="line">  )</span><br><span class="line">if(TARGET PkgConfig::ZeroMQ)</span><br><span class="line">    message(STATUS &quot;Found ZeroMQ&quot;)</span><br><span class="line">endif()</span><br><span class="line"></span><br><span class="line"># 单元测试</span><br><span class="line">find_package(PythonInterp REQUIRED)</span><br><span class="line">find_program(BASH_EXECUTABLE NAMES bash REQUIRED)</span><br><span class="line"></span><br><span class="line">enable_testing()  # 测试这个目录和所有子文件夹</span><br><span class="line">add_test(  # 设置测试名称和运行指令</span><br><span class="line">  NAME cpp_test</span><br><span class="line">  COMMAND $&lt;TARGET_FILE:cpp_test&gt;  # 生成器表达式，是在生成构建系统生成时的表达式</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"># 动态分析检测内存泄露</span><br><span class="line">find_program(MEMORYCHECK_COMMAND NAMES valgrind)</span><br><span class="line">set(MEMORYCHECK_COMMAND_OPTIONS &quot;--trace-children=yes --leak-check=full&quot;)</span><br><span class="line">$ ctest -T memcheck --parallel 4</span><br><span class="line"></span><br><span class="line"># 文件操作</span><br><span class="line">add_custom_target(unpack-eigen  # 构建没有输出的命令</span><br><span class="line">  ALL  # 目标始终被执行</span><br><span class="line">  COMMAND</span><br><span class="line">      $&#123;CMAKE_COMMAND&#125; -E tar xzf $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/eigen-eigen-5a0156e40feb.tar.gz</span><br><span class="line">  COMMAND</span><br><span class="line">      $&#123;CMAKE_COMMAND&#125; -E rename eigen-eigen-5a0156e40feb eigen-3.3.4</span><br><span class="line">  WORKING_DIRECTORY</span><br><span class="line">      $&#123;CMAKE_CURRENT_BINARY_DIR&#125;</span><br><span class="line">  COMMENT</span><br><span class="line">      &quot;Unpacking Eigen3 in $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/eigen-3.3.4&quot;</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"># 探究编译器是否支持某种特性</span><br><span class="line">try_compile(</span><br><span class="line">  omp_taskloop_test_1</span><br><span class="line">      $&#123;CMAKE_CURRENT_BINARY_DIR&#125;/omp_try_compile  # 用于保存编译成功与否的状态</span><br><span class="line">  SOURCES</span><br><span class="line">      $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/taskloop.cpp</span><br><span class="line">  LINK_LIBRARIES</span><br><span class="line">      OpenMP::OpenMP_CXX</span><br><span class="line">  )</span><br><span class="line">message(STATUS &quot;Result of try_compile: $&#123;omp_taskloop_test_1&#125;&quot;)</span><br><span class="line"></span><br><span class="line">include(CheckCXXSourceCompiles)  # 使用check_cxx_source_compiles函数，需要包含CheckCXXSourceCompiles.cmake模块文件</span><br><span class="line">file(READ $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/taskloop.cpp _snippet)  # 复制源文件的内容，file(READ ...)命令读取内容到一个变量中，试图编译和连接这个变量</span><br><span class="line">set(CMAKE_REQUIRED_LIBRARIES OpenMP::OpenMP_CXX)  # 对于下一步正确调用编译器是必需的,使用导入的OpenMP::OpenMP_CXX目标，它还将设置正确的编译器标志和包含目录:</span><br><span class="line">check_cxx_source_compiles(&quot;$&#123;_snippet&#125;&quot; omp_taskloop_test_2)</span><br><span class="line">unset(CMAKE_REQUIRED_LIBRARIES)</span><br><span class="line"></span><br><span class="line"># 探究编译器标志</span><br><span class="line">list(APPEND CXX_BASIC_FLAGS &quot;-g3&quot; &quot;-O1&quot;)  # 声明列表CXX_BASIC_FLAGS，其中包含构建项目时始终使用的编译器标志-g3和-O1:</span><br><span class="line">include(CheckCXXCompilerFlag)</span><br><span class="line">check_cxx_compiler_flag($&#123;ASAN_FLAGS&#125; asan_works)  # 调用check_cxx_compiler_flag来确保编译器理解ASAN_FLAGS变量中的标志</span><br><span class="line">unset(CMAKE_REQUIRED_FLAGS)</span><br><span class="line">if(asan_works)  # 如果编译器理解这些选项，将变量转化为一个列表，分号替换空格</span><br><span class="line">    string(REPLACE &quot; &quot; &quot;;&quot; _asan_flags $&#123;ASAN_FLAGS&#125;)</span><br><span class="line">    </span><br><span class="line"># 将cmake子目录添加到CMake模块搜索的路径列表中,告诉cmake去哪里查找宏</span><br><span class="line">list(APPEND CMAKE_MODULE_PATH &quot;$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/cmake&quot;)</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/LLM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/LLM/" class="post-title-link" itemprop="url">LLM</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:58:23 / Modified: 20:08:59" itemprop="dateCreated datePublished" datetime="2025-02-22T19:58:23+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>– [1.LLM](# 1.LLM)</p>
<p>– [2.vLLM](# 2.vLLM)</p>
<p>– [3.Attention 优化](# 3.Attention 优化)</p>
<p>– [4.TensorRT-LLM](# 4.TensorRT-LLM)</p>
<p>– [5.Coroutine &amp; Thread &amp; Process](# 5.Coroutine &amp; Thread &amp; Process)</p>
<p>– [3.Speculative Decoding](# 3.Speculative Decoding)</p>
<p>– [4.Disaggregated Inference](# 4.Disaggregated Inference)</p>
<p>– [6.Triton](# 6.Triton)</p>
<p>– [7.CUDA](# 7.CUDA)</p>
<p>– [8.Quantization](# 8.Quantization)</p>
<p>– [9.DiT](# 9.DiT)</p>
<p>Deepseek：</p>
<p>什么水平：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/10666202502/answer/89907687711">https://www.zhihu.com/question/10666202502/answer/89907687711</a></p>
<p>直观参数量：<a target="_blank" rel="noopener" href="https://yangwenbo.com/articles/deepseek-v3-parameter-size.html">https://yangwenbo.com/articles/deepseek-v3-parameter-size.html</a></p>
<p>EP TP 通信量：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/13997146226">https://zhuanlan.zhihu.com/p/13997146226</a></p>
<p>通信分析 + 训练时 grouped gemm 优化建议</p>
<p>ring_reduce &#x3D; reduce_scatter + all_gather：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/617133971">https://zhuanlan.zhihu.com/p/617133971</a></p>
<p>所以 TP 的同步 all reduce 需要先从每张卡上求和得到自己卡上的数据，再同步其余卡上的求和结果</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/16704076805">https://zhuanlan.zhihu.com/p/16704076805</a></p>
<p>SGL walk through：</p>
<p>![截图 2025-02-22 20-02-56](&#x2F;home&#x2F;kwx&#x2F;blog&#x2F;source&#x2F;_posts&#x2F;assets&#x2F;截图 2025-02-22 20-02-56.png)</p>
<p>P:D &#x3D; 1:10：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/8531371805/answer/70643107756">https://www.zhihu.com/question/8531371805/answer/70643107756</a><br><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/8531371805/answer/80649923584">https://www.zhihu.com/question/8531371805/answer/80649923584</a></p>
<p> 显存会先于算力达到瓶颈点，而且是显存带宽，而非显存容量</p>
<p>什么是 Grouped GEMM：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/484319691">https://zhuanlan.zhihu.com/p/484319691</a></p>
<p>部署提炼：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/23640350617">https://zhuanlan.zhihu.com/p/23640350617</a></p>
<p>FP8：</p>
<p><img src="/home/kwx/blog/source/_posts/assets/v2-00480748742764c41b4d2ce2463a0ac6_r.jpg" alt="v2-00480748742764c41b4d2ce2463a0ac6_r"></p>
<p>Triton：</p>
<p>如果调用 JIT 编译的函数入参具有 .data_ptr() 方法和 .dtype 属性，则会隐式转换为指针。</p>
<p>exp2：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/6874512980/answer/85865475090">https://www.zhihu.com/question/6874512980/answer/85865475090</a> 参看其他回答可能还能防止精度下溢</p>
<p>指针块：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/639423790">https://zhuanlan.zhihu.com/p/639423790</a></p>
<h4 id="1-LLM"><a href="#1-LLM" class="headerlink" title="1.LLM"></a>1.LLM</h4><h5 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h5><p>special tokens：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/17052593700">https://zhuanlan.zhihu.com/p/17052593700</a></p>
<p>TPS QPS 并发数 吞吐量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/337708438">https://zhuanlan.zhihu.com/p/337708438</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">并发数 = QPS*平均响应时间</span><br></pre></td></tr></table></figure>

<p>采样：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/631847634">https://zhuanlan.zhihu.com/p/631847634</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653926703">https://zhuanlan.zhihu.com/p/653926703</a></p>
<p>分词：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/652520262">https://zhuanlan.zhihu.com/p/652520262</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Scaling law： 假设transformer参数量为N，数据集tokens个数为D，那么模型的计算量C≈6N*D </span><br><span class="line">C确定后，模型性能就基本确定，它的决策变量只有N和D，跟模型的具体结构诸如层数、 深度、 attention头个数（宽度）基本无关，相关性非常小，性能在2%的区间内</span><br></pre></td></tr></table></figure>

<p>encoder decoder：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/427311331">https://zhuanlan.zhihu.com/p/427311331</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">encoder： 给一个向量，输出一个同样长度的向量</span><br><span class="line">decoder： 输出一个向量，长度为 词表 长度，该向量是一个概率分布，表示取得对应词的概率。</span><br><span class="line">自回归解码器 将解码器自己当前步的输出加入下一步的输入，融合所有已经输入的向量来输出下一个向量，越往后的输出考虑了越多输入，在一定程度上自己预测自己</span><br><span class="line">解码器和编码器的另一个区别是：解码器block的第一个自注意力是 masked MHA</span><br><span class="line">训练阶段，其输出序列的所有位置（时间步）的标记都是已知的；但预测阶段，其输出序列的标记是逐个生成的。因此，在任何解码器时间步中，只有生成的标记才能用于解码器的自注意力计算中</span><br></pre></td></tr></table></figure>

<p>Decoder only 完美回答：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/588325646/answer/3357252612">https://www.zhihu.com/question/588325646/answer/3357252612</a></p>
<p>Deepnorm： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/480783670">https://zhuanlan.zhihu.com/p/480783670</a></p>
<p>参数微调： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/635710004">https://zhuanlan.zhihu.com/p/635710004</a></p>
<p>位置编码性质：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662489503">https://zhuanlan.zhihu.com/p/662489503</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">正余弦交替的位置编码只与偏移量k有关，这意味着两个正弦位置嵌入的点乘可以反应两个tokens的距离，且该距离对称</span><br></pre></td></tr></table></figure>

<p>Left padding： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646852375">https://zhuanlan.zhihu.com/p/646852375</a></p>
<p>bos_token： <a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main_classes/tokenizer">https://huggingface.co/docs/transformers/main_classes/tokenizer</a></p>
<p>cpu &amp; tensor core 算力估算： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661062002">https://zhuanlan.zhihu.com/p/661062002</a></p>
<p>Gpipe： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/613196255">https://zhuanlan.zhihu.com/p/613196255</a> 流水线并行 压缩空闲气泡 切分更细碎的batch</p>
<p>forward的时候不存中间结果 bp 的时候重新计算</p>
<p>ZeRO： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/618865052">https://zhuanlan.zhihu.com/p/618865052</a> 数据并行 ZeRO 优化器 梯度 模型参数 分散各卡 减少通讯</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Prefix LM：前缀语言模型是一种生成模型，它在生成每个词时都可以考虑之前的上下文信息。在生成时，前缀语言模型会根据给定的前缀（即部分文本序列）预测下一个可能的词。这种模型可以用于文本生成、机器翻译等任务。</span><br><span class="line">Causal LM：因果语言模型是一种自回归模型，它只能根据之前的文本生成后续的文本，而不能根据后续的文本生成之前的文本。在训练时，因果语言模型的目标是预测下一个词的概率，给定之前的所有词作为上下文。这种模型可以用于文本生成、语言建模等任务。</span><br></pre></td></tr></table></figure>

<p>Layernorm + Linear： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692679244">https://zhuanlan.zhihu.com/p/692679244</a></p>
<p>BN vs LN： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/395811291/answer/3103148309">https://www.zhihu.com/question/395811291/answer/3103148309</a></p>
<p>BN 是 所有 token 的每个特征 ；LN 是每个 token 的所有特征</p>
<p>CUDA Core or Tensor Core： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/636533414/answer/3345355574">https://www.zhihu.com/question/636533414/answer/3345355574</a></p>
<p>MFU &amp; HFU： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671537309">https://zhuanlan.zhihu.com/p/671537309</a></p>
<p>NCCL 环境变量：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/598730156/answer/3187578933">https://www.zhihu.com/question/598730156/answer/3187578933</a></p>
<p>NVLink 提升：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/654832546/answer/71647384740">https://www.zhihu.com/question/654832546/answer/71647384740</a></p>
<p>4090 快于 A100：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/615946801/answer/90445392759">https://www.zhihu.com/question/615946801/answer/90445392759</a></p>
<p>避免隐式 CUDA 同步：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/67209417/answer/3459890650">https://www.zhihu.com/question/67209417/answer/3459890650</a></p>
<p>A800 训练不收敛： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701623664">https://zhuanlan.zhihu.com/p/701623664</a></p>
<p>昇腾精度 debug：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/15940824171">https://zhuanlan.zhihu.com/p/15940824171</a></p>
<p>千卡超时：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/712505336">https://zhuanlan.zhihu.com/p/712505336</a></p>
<p>continuous batching： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688551989">https://zhuanlan.zhihu.com/p/688551989</a></p>
<p>bandwidthTest：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/703039225">https://zhuanlan.zhihu.com/p/703039225</a></p>
<p>Pinned &amp; Pageable memory：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/662795350/answer/3576429708">https://www.zhihu.com/question/662795350/answer/3576429708</a></p>
<p>面经：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658609960">https://zhuanlan.zhihu.com/p/658609960</a> （Nvidia）</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/634557901">https://zhuanlan.zhihu.com/p/634557901</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663917237">https://zhuanlan.zhihu.com/p/663917237</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/716021841">https://zhuanlan.zhihu.com/p/716021841</a></p>
<p>sft 细节：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/6497090767">https://zhuanlan.zhihu.com/p/6497090767</a></p>
<p>O1 复现：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/8102196012">https://zhuanlan.zhihu.com/p/8102196012</a></p>
<h5 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h5><ul>
<li><p>split k： <a target="_blank" rel="noopener" href="https://blog.csdn.net/u013701860/article/details/128674224">https://blog.csdn.net/u013701860/article/details/128674224</a></p>
</li>
<li><p>Cache miss &amp; 分块矩阵一致性： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/342923482">https://zhuanlan.zhihu.com/p/342923482</a></p>
</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为a[0][0]本身就不在缓存中，所以肯定有一次 cache miss 啊</span><br></pre></td></tr></table></figure>

<p>row&#x2F;col parallel &amp; tp&#x2F;dp 通讯量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/622212228">https://zhuanlan.zhihu.com/p/622212228</a></p>
<p>int4 分块 &amp; fp8 模拟 ： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653735572">https://zhuanlan.zhihu.com/p/653735572</a></p>
<p>LLM 部署代价评估： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658868628">https://zhuanlan.zhihu.com/p/658868628</a></p>
<p>Transformer 时空复杂度： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/606514058/answer/3078324182">https://www.zhihu.com/question/606514058/answer/3078324182</a></p>
<p>Transformer 参数量计算量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/624740065">https://zhuanlan.zhihu.com/p/624740065</a></p>
<p>手推 FLOPs 和 50% 利用率下的推理时间：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648988727">https://zhuanlan.zhihu.com/p/648988727</a></p>
<p>RoPE： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/642884818">https://zhuanlan.zhihu.com/p/642884818</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">假设Ra表示角度为a的旋转矩阵，那么R具有如下性质：</span><br><span class="line">1. Ra^T = R(-a)</span><br><span class="line">2. Ra Rb = R(a+b)</span><br><span class="line"></span><br><span class="line">回到旋转位置编码，我们可以去证明 &lt;RaX, RbY&gt; = &lt;X, R(b-a)Y&gt; ，证明如下：</span><br><span class="line">&lt;RaX, RbY&gt;</span><br><span class="line">= (RaX)^T RbY</span><br><span class="line">= X^T Ra^T RbY</span><br><span class="line">= X^T R(b-a) Y</span><br><span class="line">= &lt;X, R(b-a)Y&gt;</span><br></pre></td></tr></table></figure>

<p>KV Cache 清晰图解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662498827">https://zhuanlan.zhihu.com/p/662498827</a></p>
<p>KV Cache 优化 MQA GQA CLA： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/697311739">https://zhuanlan.zhihu.com/p/697311739</a></p>
<p>为什么没 Q Cache： Q 只在当前 token 的计算中有用到 而 KV 在后续所有 token 的推计算中都得用</p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/653658936/answer/3545520807">https://www.zhihu.com/question/653658936/answer/3545520807</a></p>
<p>KV Cache 稀疏化： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701580870">https://zhuanlan.zhihu.com/p/701580870</a></p>
<p>KV Cache 压缩评测： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/709684237">https://zhuanlan.zhihu.com/p/709684237</a></p>
<p>fa变长使用： <a target="_blank" rel="noopener" href="https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/">https://66ring.github.io/2024/05/31/universe/ml/flash_attn_varlen_batcing_api_usage/</a></p>
<p>量化：</p>
<p>GPTQ 推导：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692338716">https://zhuanlan.zhihu.com/p/692338716</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690834228">https://zhuanlan.zhihu.com/p/690834228</a></p>
<p>AutoGPTQ bug： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/680047456">https://zhuanlan.zhihu.com/p/680047456</a></p>
<p>AWQ一看就会： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/685867596">https://zhuanlan.zhihu.com/p/685867596</a></p>
<h4 id="2-CUDA"><a href="#2-CUDA" class="headerlink" title="2.CUDA"></a>2.CUDA</h4><p>cuobjdump nvdisasm cu++filt nvprune：<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html">https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html</a></p>
<p>CUDA 全局坐标运算： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/675603584">https://zhuanlan.zhihu.com/p/675603584</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x,y,z三维坐标系，先确定z，x*y是平面面积大小</span><br></pre></td></tr></table></figure>

<p>多维数组和一维数组的映射关系指的是当出现 threadIdx.y&#x2F;z 的时候映射回线程块内的线程索引 tid： $tid &#x3D; threadIdx.z \times blockDim.x \times blockDim.y +threadIdx.y \times blockDim.x + threadIdx.x $</p>
<p>双重指针含义： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/648578880">https://zhuanlan.zhihu.com/p/648578880</a></p>
<p>不注水的显卡指标： <a target="_blank" rel="noopener" href="https://moonshot.feishu.cn/sheets/OaBLsKXazhMXjLt9dw5cC4Gvnzc?sheet=1357cc">https://moonshot.feishu.cn/sheets/OaBLsKXazhMXjLt9dw5cC4Gvnzc?sheet=1357cc</a></p>
<p>ptx sass code arch： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673833765">https://zhuanlan.zhihu.com/p/673833765</a></p>
<p>优化思路：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/356661099/answer/2449633440">https://www.zhihu.com/question/356661099/answer/2449633440</a></p>
<p>while (0)： <a target="_blank" rel="noopener" href="https://blog.csdn.net/dldw8816/article/details/86519575">https://blog.csdn.net/dldw8816/article/details/86519575</a></p>
<p>单线程 print 发生了什么： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/658620196/answer/3527314467">https://www.zhihu.com/question/658620196/answer/3527314467</a></p>
<p>浮点移位对齐： <a target="_blank" rel="noopener" href="https://www.bilibili.com/read/cv130411/">https://www.bilibili.com/read/cv130411/</a></p>
<p>__ldg： 只能用于读取全局内存中的单个数据，不能用于读取数组或结构体中的数据</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_42536162/article/details/129892382">https://blog.csdn.net/qq_42536162/article/details/129892382</a></p>
<p>位运算取余： <a target="_blank" rel="noopener" href="https://perkins4j2.github.io/posts/19120/">https://perkins4j2.github.io/posts/19120/</a></p>
<p><em>_restrict</em>_： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_28090573/article/details/89497788">https://blog.csdn.net/qq_28090573/article/details/89497788</a></p>
<p>cuda 编译： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690717002">https://zhuanlan.zhihu.com/p/690717002</a></p>
<p>float4 作用： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/574968879/answer/3005751704">https://www.zhihu.com/question/574968879/answer/3005751704</a></p>
<p>grid block size： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688610975">https://zhuanlan.zhihu.com/p/688610975</a></p>
<p><strong>Instruction：</strong></p>
<p>ld.cg：直接从 L2 缓存中读取数据，不会访问 L1 缓存，L1 cache hit rate &#x3D; 0</p>
<p><strong>cuda-mode：</strong></p>
<p>class 1 profile 概述：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706469164">https://zhuanlan.zhihu.com/p/706469164</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">load_inline</span><br><span class="line">@triton.jit(interprete=True)</span><br></pre></td></tr></table></figure>

<p><strong>Profile：</strong></p>
<p>简单访存效率对比示例： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660559591">https://zhuanlan.zhihu.com/p/660559591</a></p>
<p>warp 内产生bank conflict 造成的负面影响，要大于单 thread 内 cache miss 造成的影响</p>
<p>warp stall： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646523089">https://zhuanlan.zhihu.com/p/646523089</a></p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/464172074">https://zhuanlan.zhihu.com/p/464172074</a></p>
<p>warp state statistics：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671741797">https://zhuanlan.zhihu.com/p/671741797</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Stall Long Scoreboard：warp等待数据在L1TEX中的记分牌依赖（local，global，surface，tex）而停顿的平均周期数。为了减少等待L1TEX数据访问的周期数，可以考虑做的优化有：1）验证核函数的内存访问模式是否在目标GPU架构上最优；2）尝试增大cache命中率来提高数据局部性；3）改变L1 cache配置，考虑将频繁使用的数据移入SMEM。</span><br><span class="line">注：L1TEX即L1 Cache，可以认为它包括L1D，L1T等。</span><br><span class="line">注：Global memory是49-bit的虚拟地址，对GPU中的所有线程可见，它缓存在SM L1和GPU L2中。</span><br><span class="line">Stall Short Scoreboard：warp等待记分牌依赖MIO(memory input/output)操作(不会访问L1TEX)而等停顿的周期数。主要原因是启用SMEM并且发生了bank争用，次要原因包括频繁地执行特殊算数指令或动态分支指令。</span><br><span class="line">Stall Wait：warp等待固定延迟的执行依赖而停顿的周期数。可以考虑做的优化有：增大活跃的warp数。</span><br><span class="line">注：推测这种情况下访问的数据是在寄存器或SMEM(没有发生bank争用)中，所以可以认为是固定延迟。</span><br><span class="line">注：warp发生执行依赖只与它的前一条指令有关。</span><br><span class="line">Stall Branch Resolving：warp等待计算分支目标地址和更新warp PC而停顿的周期数。过多的分支指令可能导致这种停顿。</span><br><span class="line">Selected：warp调度器从warp池选择一条warp指令发射所需的周期数，固定为1个周期。</span><br><span class="line">Stall No Instruction：warp等待取指或等待ICache miss而停顿的周期数。</span><br><span class="line">注：wave：在给定GPU上并发运行的CTA总数称为wave。wave的大小与GPU可用SM数量以及核函数的占用率成比例关系。</span><br></pre></td></tr></table></figure>

<p>nsight compute 解读：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707107808">https://zhuanlan.zhihu.com/p/707107808</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/709873278">https://zhuanlan.zhihu.com/p/709873278</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">SOL：</span><br><span class="line">1.看内存吞吐量和计算吞吐量，表明计算密集or访存密集？</span><br><span class="line">2.L1/L2吞吐量优化空间</span><br><span class="line">3.DRAM吞吐量和总体内存吞吐量一致的话，表明主要内存操作是直接与DRAM交互</span><br><span class="line"></span><br><span class="line">Memory Workload Analysis：</span><br><span class="line">1.memory throughput：访问 DRAM 的字节数 </span><br><span class="line">2.mem busy：缓存和 DRAM 内部活动的吞吐量，峰值持续速率的百分比</span><br><span class="line"></span><br><span class="line">Source Counters：</span><br><span class="line">1.分支指令</span><br><span class="line">2.未合并的全局访问</span><br><span class="line">3.warp stall sampling：warp 停滞的主要原因和位置</span><br><span class="line"></span><br><span class="line">Warp State Statistics：</span><br><span class="line">某种 stall 状态下消耗的周期数越多，影响性能的可能性就越大</span><br><span class="line"></span><br><span class="line">Compute Workload Analysis</span><br><span class="line"></span><br><span class="line">Launch Statistics：观察有无 tail effect</span><br><span class="line"></span><br><span class="line">Scheduler Statitics：对 warp stall 现象的解释</span><br><span class="line">1.每个调度器每个周期都可以发出一条指令</span><br><span class="line">2.每个调度器最多分配12个warps，内核分配了active warps per scheduler，每个周期里只有 eligible 是可以发出指令的</span><br><span class="line">2.提高可以发出指令的warps数量，减少活跃warps阻塞时间，参看Warp State Statistics &amp; Source Counters</span><br><span class="line"></span><br><span class="line">Occupancy：</span><br><span class="line">1.Achieved active warps per SM / Theoretical active warps per SM</span><br><span class="line">2.block size / smem size / reg per thread 对性能的影响</span><br></pre></td></tr></table></figure>

<p>ncu 各参数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/673770855">https://zhuanlan.zhihu.com/p/673770855</a></p>
<p>nsys nvtx：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1909450324">https://zhuanlan.zhihu.com/p/1909450324</a></p>
<p>nsys cuda kernel timeline：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/691307737">https://zhuanlan.zhihu.com/p/691307737</a></p>
<p>nsys ncu 使用流程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/721868054">https://zhuanlan.zhihu.com/p/721868054</a></p>
<p>nsys profile –gpu-metrics-device&#x3D;0 -t cuda,nvtx –cuda-memory-usage&#x3D;true .&#x2F;sgemm</p>
<p>ncu -f –set full –target-processes all -o profile .&#x2F;sgemm –kernel-name Sgemmv3</p>
<p><strong>Reduce 优化：</strong></p>
<p>Reduce 优化： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654027980">https://zhuanlan.zhihu.com/p/654027980</a></p>
<p>深入浅出优化 Reduce： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/426978026">https://zhuanlan.zhihu.com/p/426978026</a></p>
<p>BBuf 细致解读： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/688610091">https://zhuanlan.zhihu.com/p/688610091</a></p>
<p>blockReduceSum： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/584936904">https://zhuanlan.zhihu.com/p/584936904</a></p>
<p>warpReduce：All Thread + shared[WarpID]： LaneID&#x3D;0 只需要每个线程束里的第一个线程来存取</p>
<p>&amp; warpReduce： WarpID&#x3D;0 只需要32个线程就够了</p>
<p><strong>SGEMM 优化：</strong></p>
<p>Gemm 优化精讲：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/584236348">https://zhuanlan.zhihu.com/p/584236348</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">访存覆盖：block tiling，局部性原理</span><br><span class="line">1.避免内存访问瓶颈的内存访问带宽1：dtype Tflops / 计算访存比</span><br><span class="line">2.避免内存访问瓶颈的加权内存访问带宽2，假设 L2 cache 命中率 20%：</span><br><span class="line">0.8 * DRAM bandwidth + 0.2 * L2 bandwidth</span><br><span class="line">3.更大的分块需要用到更多的寄存器，一个SM允许使用的寄存器数量和线程数数量是固定的，一个SM能够同时承载的线程数就会减少 e.g. 65536 / 1536 = 42</span><br><span class="line">4.occupancy指的是每个SM能够同时调度的线程数量除以一个SM最大可调度的线程数量</span><br><span class="line">一个warp stall时，就会切换到另一个warp执行指令，occupancy越低可供切换的线程就越少</span><br><span class="line">但较高的occupancy不意味着高性能，每个线程本身就能通过更多的寄存器来达到延迟覆盖的目的，也就不需要SM来做这</span><br><span class="line">件事了，反倒是一些线程内的延迟甚至无法被SM切换所覆盖而得不偿失</span><br><span class="line"></span><br><span class="line">延迟覆盖：thread 级优化</span><br><span class="line">1.向量内积 or 外积的选择，外积搭配寄存器的细节，double buffer需要外积</span><br><span class="line">2.(load from g -&gt; store2s) g2s -&gt; sync -&gt; s2r -&gt; reg ffma -&gt; sync</span><br><span class="line"></span><br><span class="line">访存覆盖：warp 级优化 </span><br><span class="line">1.4*8 or 8*4 合并访问，wrap的m和n越接近，计算访存比越大，减少memory transaction</span><br><span class="line">2.float4</span><br><span class="line">3.一个warp搬运连续地址</span><br><span class="line"></span><br><span class="line">提升 L2 cache 命中率：block 级优化</span><br><span class="line">1.wav &amp; L2 cache hit rate 的计算，注意wave_gpu与wav_m,wav_n不可约</span><br><span class="line">2.wav_m, wav_n计算还不太明白</span><br><span class="line">假设一个wave是一横条，那么wave_m=block_m=128，wave_n=block_n*wave_gpu=128*46*2。46是sm个数，2是一个sm能跑的block数</span><br><span class="line">A的一行B的一列，横条wav，所以A就是wavm,blockm,B就相当于是最大wav乘以blockn</span><br><span class="line">3.横条变竖条后的 L2 cache miss 也还不明白</span><br><span class="line">需要明白到底哪种更好，图例到底指的是哪种</span><br><span class="line"></span><br><span class="line">内积不适合预取</span><br></pre></td></tr></table></figure>

<p>Gemm 优化指标：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687176254">https://zhuanlan.zhihu.com/p/687176254</a></p>
<p>一共 $m\times n$ 次向量乘法操作，每个向量乘进行 $k$ 次 FMA 操作，每次 FMA 需要一对乘加两个 op，对于 FP16，</p>
<p>$Arithmetic\space Intensity &#x3D; \frac{num\space of\space FLOPs}{byte\space accesses}&#x3D;\frac{2\cdot M\cdot N\cdot K}{2\cdot (M\cdot N+M\cdot K+N\cdot K)} $，易得 GEMV 时 AI &lt; 1，memory bound</p>
<p>vs cublas：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/518857175">https://zhuanlan.zhihu.com/p/518857175</a></p>
<p>配套代码： <a target="_blank" rel="noopener" href="https://github.com/nicolaswilde/cuda-sgemm">https://github.com/nicolaswilde/cuda-sgemm</a></p>
<p>register 单拍即可访问，smem 大约 20-30 拍，gmem 300 多拍</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以 A100 为例，网上的测评给出的访问 shared memory 的 latency 大概是 22.4 ns，GPU 时钟频率是 1.5GHz，算下来单次访问就是 22.4 / (1 / 1.5GHz) = 33.6 cycle</span><br></pre></td></tr></table></figure>

<p>每个时钟周期，SM 对应的若干个 block 都会尝试选择一个可以调度的 warp 去发射执行</p>
<p>naive 需要 M*N 个线程和 K 次乘累加，两次 load gmem 才能完成一次乘法运算</p>
<p>一共需要 M * N * 2K 次对 gmem 的访问，2 指的是 gmem 中的两个数，计算访存比低，数据复用性差</p>
<p>对 smem 分块后仅需 $$M &#x2F; BM * N &#x2F; BN * K &#x2F; BK * (BM * BK + BN * BK) &#x3D; (\frac{1}{BM}+\frac{1}{BN})*MNK $$</p>
<p>每个 block 一次从 gmem 中读取 1024A 和1024 B，每个线程 1 次读 4A 4B</p>
<p>gmem 搬运到 smem &amp; 计算各需要一次 syncthreads，以防没算完执行到下一次循环修改了 smem 中对应数据</p>
<p>对 bank conflict 的分析参考： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/683465339">https://zhuanlan.zhihu.com/p/683465339</a></p>
<p>一个 smem 分为 32 bank，可以容纳 32 个int32 &#x2F; fp32，尽可能让同一个线程取到同一列 bank</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">smem到register也可以做double buffer，但看SASS，编译器已经自动优化了</span><br><span class="line">处理器核发射完访存请求，就会继续执行后面的指令，直到碰到scoreboard机制处理hazard，这是编译器设置在指令编码里发下去的，SASS中需要手动设置</span><br><span class="line">gmem和smem的访问都是可以复用的，不需要每次都去gmem或smem里面取数。对于sgemm这种用cuda core计算的，从寄存器堆里面取两个32-bit数可以做1次计算，一个sm的subcore一拍可以取128个数（32threads*4banks）但是只能算16次计算，所以限制性能的是cuda core的数目，只要cuda程序写得好访存带宽占用是比较低的</span><br><span class="line">因为在gemm这个负载下，一个SM里只有一个thread block，而且这个thread block里的所有线程在计算所需的数据在shared memory中准备好之后需要barrier的，所以warp scheduler没啥用</span><br><span class="line">smem是32个bank，每个bank数据宽度是32 bits，所以在没有bank conflict的情况下一拍只能出128B的数</span><br><span class="line">gpu SM processing block是顺序的处理器核，不会像CPU核那样可以乱序执行。gpu里碰到数据相关是一定会阻塞的，例如现在有三条指令：load r0 addr，add r1 r0 r0，load r2 addr，第一条load发出去之后，第二条add跟第一条load有写后读的真相关，所以第二条add不会发射，那么第三条load虽然没有相关可以发射，但是在顺序流水线里第三条load也不会跨过第二条add指令提前发射，需要等到第一个load的数据读回来，第二条add才能发射执行，然后第三条load才能发射执行。现在我们给这三条指令换个顺序，变成load r0 addr，load r2 addr，add r1 r0 r0，这样第一条load发出去之后，第二条load可以继续发射执行，然后才阻塞在第三条add指令，这也就是双缓冲里下一次访存和本次计算不同的相对位置会有不同的效果。</span><br></pre></td></tr></table></figure>

<p>向量化访存下的 bank conflict： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/690052715">https://zhuanlan.zhihu.com/p/690052715</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">对于 quarter/half/Warp 内所有活跃的第 i 号线程，第 i xor 1 号线程不活跃或者访存地址和其一致；(i.e. T0==T1, T2==T3, T4==T5, T6==T7, T8 == T9, ......, T30 == T31, etc.)</span><br><span class="line">对于 Warp 内所有活跃的第 i 号线程，第 i xor 2 号线程不活跃或者访存地址和其一致；(i.e. T0==T2, T1==T3, T4==T6, T5==T7 etc.)</span><br></pre></td></tr></table></figure>

<p>解决 bank conflict： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/722286440">https://zhuanlan.zhihu.com/p/722286440</a></p>
<p>gmem load to smem： smem[tid.y][tid.x] &#x3D; … x↑ y 行 x 列</p>
<p>smem write to gmem： out &#x3D; smem[tid.x][tid.y] x↑ x 行 y 列</p>
<p>padding 按照 32 bank 排列</p>
<p>0,…,31,</p>
<p>32,0,…,30,</p>
<p>31,32,0…</p>
<p>行列之和取余</p>
<p>(x + y) % 32 x列时32线程随着x↑y不变各自错开，x行时随着x↑也各自错开，是个一一映射</p>
<p>0,1,…,31,</p>
<p>1,…,31,0,</p>
<p>2,…,31,0,1</p>
<p>深入浅出 Gemm：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/435908830">https://zhuanlan.zhihu.com/p/435908830</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">如果一条指令的的源寄存器有2个以上来自同一bank，就会产生冲突。指令会重发射，浪费一个cycle</span><br><span class="line">内积外积计算各有优劣。内积计算不需要使用很多的寄存器，直接一个for-k循环，但需要多次访问shared mem，虽然shared mem在片上，但是访存开销也是几十个cycle, (rm + rn) * bk；外积在计算rm*rn这个小矩阵过程当中，对应的是A的列和B的行，这些元素其实只从smem访问一次，但是用到了很多寄存器, rm * rn * bk</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/442930482">https://zhuanlan.zhihu.com/p/442930482</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">将数据从shared mem放到寄存器也是相对耗时的，这个双缓冲是让本轮计算和下一轮shared mem-&gt;register的过程错开，让计算指令和访存指令发射的时候尽量避免stall，这样的话，计算的流水和访存的流水可以打得更满</span><br><span class="line">加载到寄存器，指的就是赋值给kernel中的局部变量</span><br><span class="line">从global mem搬运到shared mem中间需要经过register，安培架构（async copy）前，只有从global到寄存器的指令，然后再用一条指令把寄存器的数放到share mem。</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/481600052">https://zhuanlan.zhihu.com/p/481600052</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">首先计算warp_id=tid/32，随后计算lane_id=tid%32，通过warp_id和lane_id来算出，对应128个元素当中的哪一个。先算(warp_id%4)×16，假设是warp2，就是上图左侧的第2个（从0算）warp。前面有2个warp，跳过了2*16=32个元素。然后再看看当前lane_id。0-15在左半边，16-31在右半边。所以lane_id/16，先看是左半边还是右半边。右半边的话，先跳过8个元素。最后再看lane_id的奇偶数，如果奇数的话，就再跳一个四个元素</span><br><span class="line">remapping reuse</span><br><span class="line">比如之前的一句有R0.reuse，.reuse把R0的值keep在reuse cache中，那后一句的R0虽然没加reuse，但是reuse cache里还是存着R0的值，相当于cache命中了，然后最后这句没有reuse，R0的值不再keep而是invalidate了。.reuse的含义并不是立即使用了reuse cache，而是把reg的值pass到了reuse cache（如果不在cache中）并且keep住</span><br><span class="line">volta/turing因为register file改成了2 bank 2port，更难出现bank conflict了，所以再增加reuse cache的利用率性能提升并不明显，更多是降功耗。如果把所有的reuse flag全去掉，大概影响10%的性能，这时候原因主要是寄存器带宽的压力比较大（fma把寄存器带宽用满了，但同时LDS又在写reg），只要每条fma有一个src operand是从reuse cache读的，寄存器带宽也不是瓶颈了，自然你再增加reuse也不会增加什么性能</span><br><span class="line">寄存器映射的关键在于C，不能让src2和src0 1同bank产生conflict，但是只映射解决不了src0和src1的冲突，就得引入reuse，当然也需要通过reuse解决寄存器带宽的压力；volta只要不是3src同奇偶就不会conflict，纯沿用maxwell的映射本身就已经没conflict，加reuse纯起到解决寄存器带宽压力和降功耗的作用，然后只要每条fma有一个reuse就已经没有寄存器带宽的问题，在此基础上再提升reuse的利用率也不能提高性能了</span><br><span class="line">turing给reg file的sram做成了2port，2RW，同一bank也一次能同时读两次，只是turing 2port地址线分奇偶，所以只要fma的3src不是同奇偶，就不会有conflict</span><br></pre></td></tr></table></figure>

<p>double buffer： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/696844342">https://zhuanlan.zhihu.com/p/696844342</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">访存和计算对应着不同的硬件单元, 可以并行执行。代码执行顺序执行取决于编译后硬件指令发射顺序。 指令的发射过程虽然是顺序的, 但发射速度很快, 而指令发出后需要一段时间才能执行完成, 这也就对应着某个指令需要相应的时钟周期才能完成, 访存的延迟也就是访存指令相比于计算指令有更长的时钟周期。</span><br><span class="line">在线程块层面, 由于少了一次同步, GPU可以提前发射后面线程分片的计算指令, 从而掩盖从 GMEM 加载到 SMEM 的访存延迟。</span><br><span class="line">在线程层面, 加载和计算的是两个线程分片, 指令上没有依赖关系, 计算指令可以无需等待数据加载完成就可以发射, 从而掩盖从 SMEM 加载到寄存器的访存延迟。</span><br></pre></td></tr></table></figure>

<p>初探 register bank conflict &amp; reuse：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/410278370">https://zhuanlan.zhihu.com/p/410278370</a></p>
<p><strong>CUDA Graph</strong></p>
<ul>
<li>cuda graph &amp; multi-stream： <a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1391695">https://developer.aliyun.com/article/1391695</a></li>
</ul>
<p>kernel launch 开销拆分： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/3339296238/answer/24864699000">https://www.zhihu.com/question/3339296238/answer/24864699000</a></p>
<p>kernel launch 的时间也就是把每个操作提交给GPU花费的时间，一般为微秒级，但如果大量launch，总耗时就不可忽略</p>
<p>每次内核执行的空隙代表GPU处于闲置状态，可以用同步语句来模拟 等待内核完全在 GPU 上启动并完成</p>
<p>cuda context： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/694214348">https://zhuanlan.zhihu.com/p/694214348</a></p>
<p>cuda graph：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/700224642">https://zhuanlan.zhihu.com/p/700224642</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/705785807">https://zhuanlan.zhihu.com/p/705785807</a></p>
<p>只是对固定的计算任务提交到CPU的速度做优化，不会改变其他逻辑</p>
<p>理想状态下CPU总是能及时把计算任务提交到GPU，GPU利用率可以维持比较高的水平。但对于小算子，CPU提交的速度甚至不如GPU执行的速度快，这时GPU侧不可避免会出现空洞</p>
<p>单次提交的优化是有上限的，执行计算图包含了大量的单次提交，重复运行计算图，执行的单次提交也都是重复的。cuda graph 后续每次图的执行只有一次提交的开销</p>
<p>cuda event &amp; cuda stream： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/699754357">https://zhuanlan.zhihu.com/p/699754357</a></p>
<p>简易使用示例：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/715863693">https://zhuanlan.zhihu.com/p/715863693</a></p>
<p><strong>Structure releated</strong></p>
<p>片上调度： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/713114525">https://zhuanlan.zhihu.com/p/713114525</a></p>
<p>gpu 包含多个sm，一个sm对应一个及以上block；sm包含多个sp，一个sp对应一个及以上线程</p>
<p>一个sm运行多个block时，多个block共用一块smem，每个block分到的就少了</p>
<p>一个warp陷入等待时，就切换到另一组warp继续计算，这样一个warp的内存延迟就被另一个warp的计算延迟隐藏起来</p>
<p>所以对于使用寄存器较少访存为主的核函数（vector add） 使用大blockdim 反之可尝试分配较小的线程数</p>
<p>每个SM被划分为4个partition，对应4个warp scheduler，一个时钟周期可调度128个线程</p>
<p>同一时刻同一个partition的多个warp串行执行，同一时刻不同partition间可以并行</p>
<p>sp闲置： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/445590537/answer/3549844270">https://www.zhihu.com/question/445590537/answer/3549844270</a></p>
<p><strong>Tensor Core</strong></p>
<p>mma 计算过程： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614429902">https://zhuanlan.zhihu.com/p/614429902</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">一个warp内所有thread所取的数据全部完成计算，算作一个硬件指令，虽然这些thread可能会分组串行执行耗费多个周期，也叫作一条硬件指令。</span><br><span class="line">需要 fetch register的就是一条硬件指令；复用寄存器的操作，但是需要两个周期计算（寄存器交换）叫做step</span><br><span class="line">一条硬件指令可能会把thread分成4组，每个set由8个thread联合提供数据计算,每个set会分成若干个step执行，随着数制变化而变化</span><br><span class="line"></span><br><span class="line">32个thread一次所取回的数据能够完成M16N8K16的矩阵乘，但是tensorCore的大小是8x4x8，所以需要一次取回的数据，需要分成8个step完成</span><br><span class="line">Ampere完成一个16x16x16需要两次hmma，一共28次寄存器访问（读+写），一次hmma=2*((3.5+0.5)+(2.5+0.5))=14次</span><br><span class="line">B的上半部分已经在第一轮被r4读过了</span><br></pre></td></tr></table></figure>

<p>cublas： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666391239">https://zhuanlan.zhihu.com/p/666391239</a></p>
<h4 id="3-Speculative-Decoding"><a href="#3-Speculative-Decoding" class="headerlink" title="3.Speculative Decoding"></a>3.Speculative Decoding</h4><p>两种实现： <a target="_blank" rel="noopener" href="https://github.com/feifeibear/LLMSpeculativeSampling">https://github.com/feifeibear/LLMSpeculativeSampling</a></p>
<p>投机采样简介： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/651359908">https://zhuanlan.zhihu.com/p/651359908</a></p>
<p>投机采样核心问答： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/656374338">https://zhuanlan.zhihu.com/p/656374338</a></p>
<p>投机采样并行验证： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653935025">https://zhuanlan.zhihu.com/p/653935025</a></p>
<p>Medusa head简介： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658544279">https://zhuanlan.zhihu.com/p/658544279</a></p>
<p>Medusa 代码浅析： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/655809033">https://zhuanlan.zhihu.com/p/655809033</a></p>
<p>特征层：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/704755926">https://zhuanlan.zhihu.com/p/704755926</a></p>
<p>优化树结构： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707596864">https://zhuanlan.zhihu.com/p/707596864</a></p>
<p>论文合订： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/684217993">https://zhuanlan.zhihu.com/p/684217993</a></p>
<h4 id="4-Disaggregated-Inference"><a href="#4-Disaggregated-Inference" class="headerlink" title="4.Disaggregated Inference"></a>4.Disaggregated Inference</h4><p>Splitwise： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/701772045">https://zhuanlan.zhihu.com/p/701772045</a></p>
<p>Distserve： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706761664">https://zhuanlan.zhihu.com/p/706761664</a></p>
<p>Chunked Prefill：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/710165390">https://zhuanlan.zhihu.com/p/710165390</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/718715866">https://zhuanlan.zhihu.com/p/718715866</a></p>
<p>Distserve &amp; Tetrilnfer &amp; Mooncake： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706469785">https://zhuanlan.zhihu.com/p/706469785</a></p>
<p>Mooncake 碎碎念：<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/710706218">https://zhuanlan.zhihu.com/p/710706218</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/705754254">https://zhuanlan.zhihu.com/p/705754254</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706204757">https://zhuanlan.zhihu.com/p/706204757</a></p>
<p>更多思考： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/707199343">https://zhuanlan.zhihu.com/p/707199343</a></p>
<p>Mooncake 技术笔记： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/706097807">https://zhuanlan.zhihu.com/p/706097807</a></p>
<p>请求调度： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/713712371">https://zhuanlan.zhihu.com/p/713712371</a></p>
<h4 id="2-vLLM"><a href="#2-vLLM" class="headerlink" title="2.vLLM"></a>2.vLLM</h4><ul>
<li>Paged_AttentionV2： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/pull/1348">https://github.com/vllm-project/vllm/pull/1348</a></li>
<li>+34%： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/issues/421">https://github.com/vllm-project/vllm/issues/421</a></li>
</ul>
<p>完全弄懂 prefix caching： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/693556044">https://zhuanlan.zhihu.com/p/693556044</a></p>
<p>Automatic Prefix Caching： <a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/pull/2762">https://github.com/vllm-project/vllm/pull/2762</a></p>
<p>Warp-level Primitives： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/572820783">https://zhuanlan.zhihu.com/p/572820783</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">T __shfl_sync(unsigned mask, T var, int srcLane, int width=warpSize);</span><br><span class="line">T __shfl_xor_sync(unsigned mask, T var, int laneMask, int width=warpSize);</span><br></pre></td></tr></table></figure>

<p>Transformer-4： Paged Attention kernel</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663632255">https://zhuanlan.zhihu.com/p/663632255</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663719053">https://zhuanlan.zhihu.com/p/663719053</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">一个线程组处理16个字节，一个线程8个字节相当于两个fp16</span><br><span class="line">K_vec 和 Q_vec 都表示这样的 vector：类型是fp16，大小为4</span><br><span class="line"></span><br><span class="line">加载q向量： q 是传入的关于整体 q 所在的显存起始地址，q_ptr 是当前 head 下当前 seq 的 q 的地址</span><br><span class="line">q_stride = num_head*head_size，表示要越过前面所有 seq 所有 head 的 q，当前处理的 head 的 idx 是 head_idx，那么在此 head 之前有关于当前 seq 的 head_idx 个 HEAD_SIZE 数量，也要越过，才是当前 seq 在当前 head 下的 q 的显存地址 q_ptr</span><br><span class="line"></span><br><span class="line">vec_idx * VEC_SIZE 求出包含的 fp16 个数，vec_idx 实际为 0 ~ 63</span><br><span class="line">const int offset1 = (vec_idx * VEC_SIZE) / x;  (0,4,8,12,...,252) / 8 = (0,0,1,1,...,31,31)</span><br><span class="line">const int offset2 = (vec_idx * VEC_SIZE) % x;  (0,0,0,0,4,4,4,4,...)</span><br></pre></td></tr></table></figure>

<p>Transformer-10： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/671344566">https://zhuanlan.zhihu.com/p/671344566</a> SchedulerOutputs</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">num_batched_tokens = len(new_seq_lens) * max(new_seq_lens)</span><br><span class="line">当前运行的 seq 总数 x 最大长度不能超过 profile 预设</span><br><span class="line">有新加入 scheduled 的 seq 就做 prompt run 并提前 return</span><br><span class="line">在 not self.swapped 的情况下，从 waiting 中拿 seq group 构造的 SchedulerOutputs 是promopt 阶段的调度；反之是在 running 和可以 swap_in 的 list 中拿数据，是generator阶段的调度</span><br><span class="line"></span><br><span class="line">generation 阶段的 每个 seq 都只持有一个 token slot，所以此后的 num_batched_tokens == running 中的 seq 数目</span><br><span class="line">两种抢占策略</span><br></pre></td></tr></table></figure>

<p>Paged Attention：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/662900859">https://zhuanlan.zhihu.com/p/662900859</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/660192497">https://zhuanlan.zhihu.com/p/660192497</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/664276902">https://zhuanlan.zhihu.com/p/664276902</a></p>
<p>Paged Attention 源码解析：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663632255">https://zhuanlan.zhihu.com/p/663632255</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663719053">https://zhuanlan.zhihu.com/p/663719053</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">当前 head 下的 seq 的 q 向量，load到共享内存 q_vecs 中，即 256 个 fp16 的数据，使用第一个 warp load 进来</span><br><span class="line">for (int i = thread_group_idx; i &lt; NUM_VECS_PER_THREAD; i += NUM_THREAD_GROUPS) &#123;</span><br><span class="line">    const int vec_idx = thread_group_offset + i * THREAD_GROUP_SIZE;</span><br><span class="line">    q_vecs[thread_group_offset][i] = *reinterpret_cast&lt;const Q_vec*&gt;(q_ptr + vec_idx * VEC_SIZE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">关于NUM_TOKENS_PER_THREAD_GROUP的循环：</span><br><span class="line">for (int i = 0; i &lt; NUM_TOKENS_PER_THREAD_GROUP; i++) &#123;  // 按照线程组处理的 token 数量循环，每1个线程组也就是2个线程处理1个token</span><br><span class="line">    const int physical_block_offset = (thread_group_idx + i * WARP_SIZE) % BLOCK_SIZE;</span><br><span class="line">    const int token_idx = block_idx * BLOCK_SIZE + physical_block_offset;  </span><br><span class="line">    ... </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">根据 offset 获得当前线程负责的 token 的 k 向量  j = 0, 1</span><br><span class="line">const int offset1 = (vec_idx * VEC_SIZE) / x;  256个元素，两组128，offset1=0~15,offset2=1,3,5,7/0,2,4,6</span><br><span class="line">const int offset2 = (vec_idx * VEC_SIZE) % x;  </span><br><span class="line">类似于展平成一行后重新寻址 一共256个数放到容量为32的寄存器中 每8个数放一起 一个线程放几个 一个线程组一次传输16byte 也就是一个线程4个数</span><br><span class="line"></span><br><span class="line">kv_head_stride 本质上是一个 block 中所有的 token 乘以 head_size，只算1个头，所占的 fp16 的数量</span><br><span class="line">    </span><br><span class="line">thread_group_offset为0的线程，收集的是当前 group（1个group负责1个token的1个head的head_size个数据）中最大的 qk值</span><br><span class="line"></span><br><span class="line">处理和传输带宽要区分开</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665609491">https://zhuanlan.zhihu.com/p/665609491</a></p>
<ul>
<li>源码分析： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/641999400">https://zhuanlan.zhihu.com/p/641999400</a></li>
</ul>
<p>vLLM 推理流程梳理：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/649974825">https://zhuanlan.zhihu.com/p/649974825</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/649977422">https://zhuanlan.zhihu.com/p/649977422</a></p>
<p>Paged Attention 核心： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/655561941">https://zhuanlan.zhihu.com/p/655561941</a></p>
<p>一致性 penalty： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/658780653">https://zhuanlan.zhihu.com/p/658780653</a></p>
<ul>
<li><p>vLLM top down 概览： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645251151">https://zhuanlan.zhihu.com/p/645251151</a></p>
</li>
<li><p>fastapi 部署： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/6256971">https://zhuanlan.zhihu.com/p/6256971</a></p>
</li>
</ul>
<p>  fastapi async single threaded： <a target="_blank" rel="noopener" href="https://github.com/tiangolo/fastapi/issues/4265">https://github.com/tiangolo/fastapi/issues/4265</a></p>
<h4 id="3-Attention-优化"><a href="#3-Attention-优化" class="headerlink" title="3.Attention 优化"></a>3.Attention 优化</h4><ul>
<li>FlashAttention：</li>
</ul>
<p>  从 online softmax 开始推导：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/9339626150">https://zhuanlan.zhihu.com/p/9339626150</a></p>
<p>  配图解读，十分清晰：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/693843979">https://zhuanlan.zhihu.com/p/693843979</a></p>
<p>  只看图： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/663932651">https://zhuanlan.zhihu.com/p/663932651</a></p>
<p>  值得和2最后细看： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607364156">https://zhuanlan.zhihu.com/p/607364156</a></p>
<p>  Flops, 复杂度, 访问次数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/626079753">https://zhuanlan.zhihu.com/p/626079753</a></p>
<p>  速度优化原理： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/611236756/answer/3322413586">https://www.zhihu.com/question/611236756/answer/3322413586</a></p>
<p>  v2 两点优化项十分清晰： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/645627275">https://zhuanlan.zhihu.com/p/645627275</a></p>
<p>  v2巩固： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/682441154">https://zhuanlan.zhihu.com/p/682441154</a></p>
<p>  Triton引子 &amp; FA循序渐进 &amp; 极具含金量： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665170554">https://zhuanlan.zhihu.com/p/665170554</a></p>
<ul>
<li><p>MQA GQA： <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q">https://mp.weixin.qq.com/s/_4OxoRLxhOcjGf0Q4Tvp2Q</a></p>
</li>
<li><p>Flash decoding：</p>
</li>
</ul>
<p>  感觉没说： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/661478232">https://zhuanlan.zhihu.com/p/661478232</a></p>
<p>  <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66426444">https://zhuanlan.zhihu.com/p/66426444</a></p>
<h4 id="4-TensorRT-LLM"><a href="#4-TensorRT-LLM" class="headerlink" title="4.TensorRT-LLM"></a>4.TensorRT-LLM</h4><p>部署调优： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/699333691">https://zhuanlan.zhihu.com/p/699333691</a></p>
<p>evaluate 本地下载： <a target="_blank" rel="noopener" href="https://blog.csdn.net/misaki_min/article/details/132650725">https://blog.csdn.net/misaki_min/article/details/132650725</a></p>
<p>perf json： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/server/issues/5746">https://github.com/triton-inference-server/server/issues/5746</a></p>
<p>Attention kernels： <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/issues/457">https://github.com/NVIDIA/TensorRT-LLM/issues/457</a></p>
<p>max_batch_size 讲究： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/tensorrtllm_backend/issues/72">https://github.com/triton-inference-server/tensorrtllm_backend/issues/72</a></p>
<p>perf_analyzer cli： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md">https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md</a></p>
<p>perf_analyzer -m ensemble -i grpc –shape “bad_words：1” –shape “max_tokens：1” –shape “stop_words：1” –shape “text_input：1” –streaming</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Cannot send stop request without specifying a request_id.</span><br><span class="line">in ensemble &#x27;ensemble&#x27;, Streaming is only supported if model is deployed using decoupled mode.</span><br><span class="line">ModelInfer RPC doesn&#x27;t support models with decoupled transaction policy.</span><br><span class="line">https://github.com/triton-inference-server/server/issues/4994  + --streaming</span><br><span class="line">Failed to init manager inputs： input bad_words contains dynamic shape, provide shapes to send along with the request  + --shape</span><br><span class="line">Cannot process new request： Streaming mode is only supported with beam width of 1.</span><br></pre></td></tr></table></figure>

<p>ulimit memlock&#x3D;-1： <a target="_blank" rel="noopener" href="https://gorden5566.com/post/1089.html">https://gorden5566.com/post/1089.html</a></p>
<p>LD_DEBUG&#x3D;libs 查看程序搜索库的路径：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/skyie53101517/article/details/45461835">https://blog.csdn.net/skyie53101517/article/details/45461835</a></p>
<p>3个.so： <a target="_blank" rel="noopener" href="https://github.com/NVIDIA/TensorRT-LLM/issues/388">https://github.com/NVIDIA/TensorRT-LLM/issues/388</a></p>
<p>perf_analyzer cli： <a target="_blank" rel="noopener" href="https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md">https://github.com/triton-inference-server/client/blob/main/src/c%2B%2B/perf_analyzer/docs/cli.md</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find / -name pybind11Config.cmake</span><br><span class="line"></span><br><span class="line">cmake -DSM=80 -DCMAKE_BUILD_TYPE=Release -DBUILD_PYT=ON -DBUILD_MULTI_GPU=ON -Dpybind11_DIR=/usr/local/lib/python3.8/dist-packages/pybind11/share/cmake/pybind11/pybind11Config.cmake ..</span><br><span class="line"></span><br><span class="line">mpirun -n 2 --allow-run-as-root  python api_server.py --model /data2/dingweihao/llama-2-13b-pretrain-sft-2048-checkpoint-546-20230912/ft/2-gpu/ --tokenizer /data2/dingweihao/llama-2-13b-pretrain-sft-2048-checkpoint-546-20230912/ --lib ../build/lib/ --tensor_para_size 2 --port 7000 --host 0.0.0.0</span><br></pre></td></tr></table></figure>

<p>-Dpybind11_DIR： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38163468/article/details/121600290">https://blog.csdn.net/qq_38163468/article/details/121600290</a></p>
<p>remove padding的目的是为了减少padding部分的计算量，FT 中实现的remove padding仅仅减少了layernorm部分以及ffn部分还有self-attention中最后那个全连接部分的padding的计算量</p>
<h4 id="5-Coroutine-Thread-Process"><a href="#5-Coroutine-Thread-Process" class="headerlink" title="5.Coroutine &amp; Thread &amp; Process"></a>5.Coroutine &amp; Thread &amp; Process</h4><p>并发：由一个处理器快速交替执行多个任务，只是看起来像在“同时执行多个任务”</p>
<p>并行：由多个处理器分别运行多个任务，各任务间严格同时执行</p>
<p>ring all-reduce数学性质推导： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/504957661">https://zhuanlan.zhihu.com/p/504957661</a></p>
<p>MPI通信接口代价函数： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/653968730">https://zhuanlan.zhihu.com/p/653968730</a></p>
<p>multiprocess &amp; rpc &amp; zeromq：</p>
<p>nccl 仿照 demo debug：<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573/6">https://discuss.pytorch.org/t/cuda-allocation-lifetime-for-inputs-to-distributed-all-reduce/191573/6</a></p>
<p>NCCL increased memory：</p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nccl/issues/964">https://github.com/NVIDIA/nccl/issues/964</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/NVIDIA/nccl/issues/864">https://github.com/NVIDIA/nccl/issues/864</a></p>
<p>mp.managers：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424011522">https://zhuanlan.zhihu.com/p/424011522</a></p>
<p>pipe queue：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24883194">https://zhuanlan.zhihu.com/p/24883194</a></p>
<p>pool：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/24883077">https://zhuanlan.zhihu.com/p/24883077</a></p>
<p>多进程 + async：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/629916103">https://zhuanlan.zhihu.com/p/629916103</a></p>
<p>asyncio：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># From https://zhuanlan.zhihu.com/p/681935789</span><br><span class="line">import asyncio</span><br><span class="line">import asyncio.tasks</span><br><span class="line"></span><br><span class="line">def inspect_event_loop()：</span><br><span class="line">    for event_loop, running_task in asyncio.tasks._current_tasks.items()：</span><br><span class="line">        print(&quot;Running Task：\n&quot;)</span><br><span class="line">        running_task.print_stack()</span><br><span class="line"></span><br><span class="line">        print(&quot;All Tasks：\n&quot;)</span><br><span class="line">        for task in asyncio.tasks.all_tasks(event_loop)：</span><br><span class="line">            task.print_stack()</span><br></pre></td></tr></table></figure>

<p>动态添加协程： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59621713">https://zhuanlan.zhihu.com/p/59621713</a></p>
<p>asyncio lock： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/66050624">https://zhuanlan.zhihu.com/p/66050624</a></p>
<p>event loop： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69210021">https://zhuanlan.zhihu.com/p/69210021</a></p>
<p>值得详细捋一遍 asyncio： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/59671241">https://zhuanlan.zhihu.com/p/59671241</a> wait gather讲得很清晰</p>
<p>asyncio 示例讲解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/56084772">https://zhuanlan.zhihu.com/p/56084772</a></p>
<p>wait vs gather： <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6872bf356af7">https://www.jianshu.com/p/6872bf356af7</a></p>
<p>coroutine &amp; task： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45521388">https://zhuanlan.zhihu.com/p/45521388</a></p>
<p>最简 event loop： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/83627584">https://zhuanlan.zhihu.com/p/83627584</a></p>
<p>event： <a target="_blank" rel="noopener" href="https://blog.csdn.net/mixintu/article/details/102458809">https://blog.csdn.net/mixintu/article/details/102458809</a></p>
<p>get &#x2F; set_event_loop： <a target="_blank" rel="noopener" href="https://blog.csdn.net/whatday/article/details/106885916">https://blog.csdn.net/whatday/article/details/106885916</a></p>
<p>loop.run_xxx 家族都是阻塞的，例如 run_until_loop 会等到给定的 coroutine 完成才结束</p>
<p>初始情况下，get_event_loop 只会在主线程帮您创建新的 event loop，并且在主线程中多次调用始终返回该 event loop；而在其他线程</p>
<p>中调用 get 则会报错，除非您在这些线程里面手动调用过 set</p>
<p>协程可以包装在 asyncio.Task 对象中独立执行，而不是直接在协程中执行</p>
<p>访问事件循环的原因：</p>
<p>监控任务的进度; 发布任务并从中获取结果; 解雇并忘记一次性任务。</p>
<p>task.done()可以用于检测任务是否完成 .cancelled()</p>
<p>.result()获取任务返回的结果 提前先检查任务是否已完成或是被取消 不然会出现 InvalidStateError</p>
<p>.exception() 检索未处理的异常 .cancel()取消计划任务</p>
<p>add_done_callback()向任务里添加回调 remove_done_callback()</p>
<p>task &#x3D; asyncio.create_task(task_coroutine(), name&#x3D;’MyTask’)<br>set_name() get_name()</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/606363838">https://zhuanlan.zhihu.com/p/606363838</a></p>
<p>asyncio.current_task() 获取当前任务，这将为当前正在运行的任务返回一个任务对象，这可能是：</p>
<ol>
<li>传递给 asyncio.run() 的主协程</li>
<li>通过 asyncio.create_task() 在 asyncio 程序中创建和调度的任务</li>
</ol>
<p>所有协程都可以作为异步事件循环中的任务进行访问</p>
<p>asyncio.all_tasks() 获取一组已计划和正在运行的任务</p>
<p>异步生成器中依次获取值时，可以使用<code>async for</code>来实现</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/607709631">https://zhuanlan.zhihu.com/p/607709631</a></p>
<p>asyncio.gather()： 允许将一组可等待对象视为单个可等待对象</p>
<p>一次执行这些任务协程并等待它们全部完成后再继续，例如具有不同数据的相同任务或协程</p>
<ul>
<li>通过 await 表达式执行并等待组中的所有可等待对象完成。</li>
<li>从所有分组的等待对象中获取结果，稍后通过 result() 方法检索。</li>
<li>要通过 cancel() 方法取消的一组等待对象。</li>
<li>通过 done() 方法检查组中的所有可等待对象是否已完成。</li>
<li>仅当组中的所有任务完成时才执行回调函数。</li>
</ul>
<p>一旦创建了 Future 对象，它就会在事件循环中自动调度</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/608952664">https://zhuanlan.zhihu.com/p/608952664</a></p>
<p>asyncio.wait() 函数可用于等待一组异步任务完成</p>
<p>asyncio 任务是包装协程的 asyncio.Task 类的一个实例</p>
<p>它允许独立调度和执行协程，Task 实例提供任务句柄以查询状态和获取结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">done, pending = await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)</span><br><span class="line">FIRST_COMPLETED 当第一个任务完成并在完成集中返回时，其余任务不会被取消并继续并发执行</span><br><span class="line">FIRST_EXCEPTION 来等待第一个任务因异常而失败</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/609252710">https://zhuanlan.zhihu.com/p/609252710</a></p>
<p>asyncio.wait_for() 函数允许调用者等待 asyncio 任务或协程超时完成。如果没有指定超时，wait_for() 函数将等待直到任务完成。如果在任务完成之前指定了超时并超时，那么任务将被取消</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">await asyncio.wait_for(coro, timeout=10)</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/609956166">https://zhuanlan.zhihu.com/p/609956166</a></p>
<p>asyncio.shield() 保护另一个任务或协程不被取消，它以一个可等待对象作为参数并返回一个 asyncio.Future 对象，然后直接等待 Future 对象或将其传递给另一个任务或协程</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/610881194">https://zhuanlan.zhihu.com/p/610881194</a></p>
<p>异步迭代器 &amp; 生成器：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/611864797">https://zhuanlan.zhihu.com/p/611864797</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/612439743">https://zhuanlan.zhihu.com/p/612439743</a></p>
<p>asyncio 基本：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619977951">https://zhuanlan.zhihu.com/p/619977951</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619978329">https://zhuanlan.zhihu.com/p/619978329</a></p>
<p>asyncio.create_task() 方法安排许多协程在 asyncio 程序中独立运行</p>
<p>可以通过首先通过 asyncio.all_tasks() 函数获取一组所有正在运行的任务，从该集合中删除自身，然后通过 asyncio.wait() 函数等待剩余的任务来实现</p>
<p>通过直接等待 asyncio.Task 对象来等待任务完成</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/622324216">https://zhuanlan.zhihu.com/p/622324216</a></p>
<h4 id="6-Triton"><a href="#6-Triton" class="headerlink" title="6.Triton"></a>6.Triton</h4><p>Makefile： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/350297509">https://zhuanlan.zhihu.com/p/350297509</a></p>
<p>MLIR文章汇总： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/141256429">https://zhuanlan.zhihu.com/p/141256429</a></p>
<p>软链接： <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_21386275/article/details/79881543">https://blog.csdn.net/qq_21386275/article/details/79881543</a></p>
<p>c++filt -n： <a target="_blank" rel="noopener" href="https://blog.csdn.net/K346K346/article/details/88225726">https://blog.csdn.net/K346K346/article/details/88225726</a></p>
<p>LLVM： <a target="_blank" rel="noopener" href="https://github.com/llvm/llvm-project/issues/63988">https://github.com/llvm/llvm-project/issues/63988</a></p>
<p>&#x2F;usr&#x2F;bin&#x2F;ld： cannot find -lxx： <a target="_blank" rel="noopener" href="https://blog.csdn.net/kuzma_zhang/article/details/131829943/">https://blog.csdn.net/kuzma_zhang/article/details/131829943/</a></p>
<p>ninja install 可以 但是加上 sudo 前缀后提示无该命令： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/lfri/p/16277069.html">https://www.cnblogs.com/lfri/p/16277069.html</a></p>
<p>ninja -C build check-llvm</p>
<p>CMake hidden by files： conda deactivate .so文件被隐藏</p>
<p>卸载 sudo make install： <a target="_blank" rel="noopener" href="https://blog.csdn.net/charry_win/article/details/126628169">https://blog.csdn.net/charry_win/article/details/126628169</a></p>
<p>入门： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/622685131/answer/3217107882">https://www.zhihu.com/question/622685131/answer/3217107882</a></p>
<h4 id="7-CUDA"><a href="#7-CUDA" class="headerlink" title="7.CUDA"></a>7.CUDA</h4><p>fp16 cuda：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_25147897/article/details/79053748">https://blog.csdn.net/qq_25147897/article/details/79053748</a></p>
<p><a target="_blank" rel="noopener" href="https://hackmd.io/@yrHb-fKBRoyrKDEKdPSDWg/ryvMkGHR">https://hackmd.io/@yrHb-fKBRoyrKDEKdPSDWg/ryvMkGHR</a></p>
<p>位运算高级操作： <a target="_blank" rel="noopener" href="https://blog.csdn.net/GOLOJO/article/details/139420924">https://blog.csdn.net/GOLOJO/article/details/139420924</a></p>
<p>二进制：</p>
<p>推广至fp16： <a target="_blank" rel="noopener" href="https://blog.csdn.net/jiaoyangwm/article/details/129296459">https://blog.csdn.net/jiaoyangwm/article/details/129296459</a></p>
<p>int8： <a target="_blank" rel="noopener" href="https://www.cnblogs.com/peterleee/p/13752162.html">https://www.cnblogs.com/peterleee/p/13752162.html</a></p>
<p>指数位是偏移值,对float就是”实际值”加上127得到得值.即”01111111”得实际值实际上是0,所以整个</p>
<p>3f800000的值为 ： +1 * 2^0 * 1.0 &#x3D; 1.0，0xff提取低8位</p>
<p>self.stream &#x3D; torch.cuda.Stream().cuda_stream const</p>
<p>c10::cuda::CUDAStream+torch.cuda.Stream()编译能过但是运行类型不匹配 cudaStream_t</p>
<p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/is-torch-cuda-stream-cuda-stream-equivalent-to-cudastream-t/130696">https://discuss.pytorch.org/t/is-torch-cuda-stream-cuda-stream-equivalent-to-cudastream-t/130696</a></p>
<p>cudaError_t cudaMalloc(void** devPtr, size_t size)</p>
<p>第一个参数传递的是存储在cpu内存中的指针变量的地址，cudaMalloc在执行完成后，向这个地址中写入了一个地址值（此地址值是GPU显存里的）。</p>
<p>静态变量应该使用 cudaMemcpyToSymbol(devData, &amp;value, sizeof(float)) 而非 cudaMemcpy(&amp;value, devData, sizeof(float))，不能用动态copy的方式给静态变量赋值</p>
<p>硬要用cudaMemcpy，必须得</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">float *dptr;</span><br><span class="line">// 主机端对device变量进行取地址是非法的，想要得到devData的地址可以使用</span><br><span class="line">cudaGetSymbolAddress((void**)&amp;dptr, devData);</span><br><span class="line">cudaMemcpy(dptr, &amp;value, sizeof(float), cudaMemcpyHostToDevice);</span><br></pre></td></tr></table></figure>

<p>固定内存的释放和分配成本 cudaMallocHost 比可分页内存要高很多，但是传输速度更快，所以对于大规模数据，固定内存效率更高。尽量使用流来使内存传输和计算之间同时进行。</p>
<p>零拷贝内存： cudaHostAlloc(flag&#x3D;cudaHostAllocMapped)</p>
<p>零拷贝内存虽然不需要显式的传递到设备上，但是设备还不能通过pHost直接访问对应的内存地址，设备需要访问主机上的零拷贝内存，需要先获得另一个地址，这个地址帮助设备访问到主机对应的内存</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaHostGetDevicePointer(void** pDevice, void* pHost, unsigned flags)</span><br></pre></td></tr></table></figure>

<p>有了统一虚拟寻址UVA，cudaHostGetDevicePointer就没用了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">float *a_host,*b_host,*res_d;</span><br><span class="line">CHECK(cudaHostAlloc((float**)&amp;a_host,nByte,cudaHostAllocMapped));</span><br><span class="line">CHECK(cudaHostAlloc((float**)&amp;b_host,nByte,cudaHostAllocMapped));</span><br><span class="line">CHECK(cudaMalloc((float**)&amp;res_d,nByte));</span><br><span class="line">res_from_gpu_h=(float*)malloc(nByte);</span><br><span class="line"></span><br><span class="line">initialData(a_host,nElem);</span><br><span class="line">initialData(b_host,nElem);</span><br><span class="line"></span><br><span class="line">dim3 block(1024);</span><br><span class="line">dim3 grid(nElem/block.x);</span><br><span class="line">sumArraysGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(a_host,b_host,res_d);</span><br></pre></td></tr></table></figure>

<p>统一内存寻址设置内存池：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMallocManaged(void** devPtr, size_t size, unsigned int flags=0)</span><br></pre></td></tr></table></figure>

<p>不可以在host函数中直接访问静态全局内存变量（<em>_device</em>_），但可以用cudaMemcpyTo&#x2F;FromSymbol来完成静态全局内存&#x2F;常量内存与host内存之间的数据传输</p>
<p>-Xptxas -dlcm&#x3D;cg 禁用L1：32Bytes -Xptxas -dlcm&#x3D;ca 启用：128Bytes</p>
<p>利用率的例子：每个线程访问一个fp32，一个warp就等同于需要访问128字节，对于禁用L1来说，因为粒度是32，所以如果warp里的所有线程都只访问同一个地址上的数，那其实使用率就只有4&#x2F;32（对于128Bytes的缓存行，需要4次内存事务，但是粒度上是32，有用的只有4）</p>
<p>可以使用只读缓存__ldg从全局内存中读取数据，或是在间接引用的指针上使用修饰符</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__global__ void copyKernel(float * in,float* out)</span><br><span class="line">&#123;</span><br><span class="line">    int idx=blockDim*blockIdx.x+threadIdx.x;</span><br><span class="line">    out[idx]=__ldg(&amp;in[idx]);</span><br><span class="line">&#125;</span><br><span class="line">AOS： 就是数组 struct A a[N];</span><br><span class="line">SOA： </span><br><span class="line">struct A&#123;</span><br><span class="line">    int a[N];</span><br><span class="line">    int b[N];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>volatile声明的变量始终在全局内存中，以防编译器优化存入缓存，如果此时恰好被其他线程改写就会造成内存缓存不一致的错误</p>
<p>动态 kernel里只能写成一维的，因为运行时才能确定：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">// 方形</span><br><span class="line">__global__ void setRowReadColDynIpad(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    extern __shared__ int tile[];</span><br><span class="line">    unsigned int row_idx=threadIdx.y*(blockDim.x+1)+threadIdx.x;</span><br><span class="line">    tile[row_idx]=row_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    unsigned int col_idx=threadIdx.x*(blockDim.x+1)+threadIdx.y;</span><br><span class="line">    out[row_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br><span class="line">setRowReadColDynIpad&lt;&lt;&lt;grid,block,(BDIMX+1)*BDIMY*sizeof(int)&gt;&gt;&gt;(out);</span><br><span class="line"></span><br><span class="line">// 以下两种写法是等价的，逐元素填充与按行填充</span><br><span class="line">__global__ void setRowReadCol(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    __shared__ int tile[BDIMY][BDIMX];</span><br><span class="line">    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[idx]=tile[threadIdx.x][threadIdx.y];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void setRowReadColDyn(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    extern __shared__ int tile[];</span><br><span class="line">    unsigned int row_idx=threadIdx.y*blockDim.x+threadIdx.x;</span><br><span class="line">    unsigned int col_idx=threadIdx.x*blockDim.y+threadIdx.y;</span><br><span class="line">    tile[row_idx]=row_idx;</span><br><span class="line">    __syncthreads();</span><br><span class="line">    out[row_idx]=tile[col_idx];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 矩形</span><br><span class="line">__global__ void setRowReadColRect(int * out)</span><br><span class="line">&#123;</span><br><span class="line">    __shared__ int tile[BDIMY_RECT][BDIMX_RECT];</span><br><span class="line">    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x; // 全局一维索引映射至二维坐标</span><br><span class="line">    tile[threadIdx.y][threadIdx.x]=idx; // 按行写入smem</span><br><span class="line">    __syncthreads();</span><br><span class="line">    unsigned int irow=idx/blockDim.y;</span><br><span class="line">    unsigned int icol=idx%blockDim.y;</span><br><span class="line">    // 因为如果是正常按行的话 iy=idx/blockDim.x</span><br><span class="line">    out[idx]=tile[icol][irow]; // 重新编排索引，按列读取smem，但不会影响最终结果，按列读的结果要和原来的一维地址相符</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// smem完成矩阵转置： 全局内存按行读取，按行写入smem + smem读取一列写入全局内存的一行</span><br><span class="line">// 二维矩阵存储在内存的时候是一维的，一般是把二维矩阵按照逐行的方式放入一维内存中，转置的过程可以理解为把逐行从上到下数据改成逐列从左到右的数据</span><br><span class="line">__global__ void transformSmem(float * in,float* out,int nx,int ny)</span><br><span class="line">&#123;</span><br><span class="line">	__shared__ float tile[BDIMY][BDIMX];</span><br><span class="line">	unsigned int ix,iy,transform_in_idx,transform_out_idx;</span><br><span class="line">// 1.计算当前块中的全局坐标，计算对应的一维线性内存的位置 所谓的待转置的矩阵就是一维线性内存</span><br><span class="line">	ix=threadIdx.x+blockDim.x*blockIdx.x;</span><br><span class="line">    iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">	transform_in_idx=iy*nx+ix; // 连接线程与内存 引入全局坐标 + 输入矩阵坐标 一维展开</span><br><span class="line">// 2.bidx表示这个块中线程坐标的线性位置（把块中的二维线程位置按照逐行排布的原则，转换成一维的），然后进行转置改成逐列排布的方式，计算出新的二维坐标，逐行到逐列排布的映射就是转置的映射，这只完成了很多块中的一块，而关键的是我们把这块放回到哪</span><br><span class="line">    // 想象成是大网格里套了很多block</span><br><span class="line">	unsigned int bidx,irow,icol;</span><br><span class="line">	bidx=threadIdx.y*blockDim.x+threadIdx.x; // 类似于iy*nx+ix，计算的是块中坐标，重新计算二维坐标</span><br><span class="line">	irow=bidx/blockDim.y;</span><br><span class="line">	icol=bidx%blockDim.y;</span><br><span class="line">// 3.计算出转置后的二维全局线程的目标坐标，注意这里的转置前的行位置是计算出来的是转置后的列的位置，这就是转置的第二步</span><br><span class="line">	ix=blockIdx.y*blockDim.y+icol;</span><br><span class="line">	iy=blockIdx.x*blockDim.x+irow; // 引入转置后的全局坐标</span><br><span class="line">    // 对比 ix=threadIdx.x+blockDim.x*blockIdx.x 因为转置后打乱了数据排布方式，因此原来的threadIdx就无法使用了  </span><br><span class="line">// 4.计算出转置后的二维坐标对应的全局内存的一维位置 &amp; 读取全局内存，写入共享内存，然后按照转置后的位置写入</span><br><span class="line">	transform_out_idx=iy*ny+ix; // 输出矩阵坐标，也就是转置后的矩阵的坐标</span><br><span class="line">	if(ix&lt;nx &amp;&amp; iy&lt;ny)</span><br><span class="line">	&#123;</span><br><span class="line">		tile[threadIdx.y][threadIdx.x]=in[transform_in_idx];</span><br><span class="line">		__syncthreads();</span><br><span class="line">		out[transform_out_idx]=tile[icol][irow]; // 原来的按列自上往下 等号左边当作一维展开更好理解</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 填充 + 循环展开</span><br><span class="line">__global__ void transformSmemUnrollPad(float * in,float* out,int nx,int ny)</span><br><span class="line">&#123;</span><br><span class="line">	__shared__ float tile[BDIMY*(BDIMX*2+IPAD)];</span><br><span class="line">//1.</span><br><span class="line">	unsigned int ix,iy,transform_in_idx,transform_out_idx;</span><br><span class="line">	ix=threadIdx.x+blockDim.x*blockIdx.x*2;</span><br><span class="line">    iy=threadIdx.y+blockDim.y*blockIdx.y;</span><br><span class="line">	transform_in_idx=iy*nx+ix;</span><br><span class="line">//2.</span><br><span class="line">	unsigned int bidx,irow,icol;</span><br><span class="line">	bidx=threadIdx.y*blockDim.x+threadIdx.x; // 全局才*2</span><br><span class="line">	irow=bidx/blockDim.y;</span><br><span class="line">	icol=bidx%blockDim.y;</span><br><span class="line">//3.</span><br><span class="line">	unsigned int ix2=blockIdx.y*blockDim.y+icol;</span><br><span class="line">	unsigned int iy2=blockIdx.x*blockDim.x*2+irow;</span><br><span class="line">    </span><br><span class="line">//4.计算出转置后的二维坐标对应的全局内存的一维位置，注意这里不是一次计算一个块，而是计算两个块，换个理解方法，我们把原来的块x方向扩大一倍，然后再对这个大块分成两个小块（A,B），每个小块中的对应位置差BDIMX，然后对其中A，B中数据按行写入共享内存</span><br><span class="line">	transform_out_idx=iy2*ny+ix2;</span><br><span class="line">	if(ix+blockDim.x&lt;nx &amp;&amp; iy&lt;ny) // 边界处理</span><br><span class="line">	&#123;</span><br><span class="line">		unsigned int row_idx=threadIdx.y*(blockDim.x*2+IPAD)+threadIdx.x; // 可以对照上方，是块中坐标</span><br><span class="line">		tile[row_idx]=in[transform_in_idx]; // kernel按block执行，所以每次smem填充一个块的大小，因此等号左边使用到的就是块内坐标而非全局坐标</span><br><span class="line">		tile[row_idx+BDIMX]=in[transform_in_idx+BDIMX];</span><br><span class="line">//5.将4中读取到的共享内存中的数据，按照转置后的位置写入全局内存</span><br><span class="line">		__syncthreads();</span><br><span class="line">		unsigned int col_idx=icol*(blockDim.x*2+IPAD)+irow;</span><br><span class="line">        out[transform_out_idx]=tile[col_idx];</span><br><span class="line">		out[transform_out_idx+ny*BDIMX]=tile[col_idx+BDIMX];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>流能封装cudamalloc，cudamemcpy中的一些异步操作，并保持操作顺序，允许操作在流中排队，保证其在前面所有操作启动之后启动，有了流就能查询排队状态</p>
<p>CUDA流中排队的操作和主机都是异步的，所以排队的过程中并不耽误主机运行其他指令，所以这就隐藏了执行这些操作的开销</p>
<p>异步数据传输时，主机端的内存必须是固定内存(pinned)或不可分页内存(non-pageable)，因为操作系统有权在一个程序运行期间改变程序使用的可分页内存的物理地址，这在异步传输过程中会引发不可知错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cudaError_t cudaMallocHost(void **ptr, size_t size);</span><br><span class="line">cudaError_t cudaHostAlloc(void **pHost, size_t size, size_t flags);</span><br><span class="line">cudaError_t cudaFreeHost(void* ptr);</span><br></pre></td></tr></table></figure>

<p>非空流异步，但可能会被空流阻塞，空&#x2F;默认流同步，cudaStreamCreate创建的是阻塞流，意味着里面有些操作会被阻塞，直到空流中默写操作完成，空流不需要显式声明，隐式+阻塞，跟所有阻塞流同步</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">__shfl_up_sync(mask,v,d,w)： 序号t的线程返回t-d的线程中变量v的值，t&lt;d的返回原来的v</span><br><span class="line">__shfl_down_sync(mask,v,d,w)： 序号t的线程返回t+d的线程中变量v的值，t+d&gt;=w的返回原来的v</span><br><span class="line">int __shfl_snc(int var,int srcLane,int width=warpSize)</span><br><span class="line">假设width=16,srcLane=3，那么0-15接收0+3位置处的值，16-31接收16+3</span><br><span class="line">// 跨warps使用数组索引交换数值 </span><br><span class="line">// https://face2ai.com/CUDA-F-5-6-%E7%BA%BF%E7%A8%8B%E6%9D%9F%E6%B4%97%E7%89%8C%E6%8C%87%E4%BB%A4/ </span><br><span class="line">__inline__ __device__</span><br><span class="line">void swap(int *value,int laneIdx,int mask,int firstIdx,int secondIdx)</span><br><span class="line">&#123;</span><br><span class="line">    bool pred=((laneIdx%(2))==0);</span><br><span class="line">    if(pred)</span><br><span class="line">    &#123;</span><br><span class="line">        int tmp=value[firstIdx];</span><br><span class="line">        value[firstIdx]=value[secondIdx];</span><br><span class="line">        value[secondIdx]=tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    value[secondIdx]=__shfl_xor(value[secondIdx],mask,BDIM);</span><br><span class="line">    if(pred)</span><br><span class="line">    &#123;</span><br><span class="line">        int tmp=value[firstIdx];</span><br><span class="line">        value[firstIdx]=value[secondIdx];</span><br><span class="line">        value[secondIdx]=tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">__global__ void test_shfl_swap(int *in,int* out,int const mask,int firstIdx,int secondIdx)</span><br><span class="line">&#123;</span><br><span class="line">    //1.</span><br><span class="line">    int idx=threadIdx.x*SEGM;</span><br><span class="line">    int value[SEGM];</span><br><span class="line">    for(int i=0;i&lt;SEGM;i++)</span><br><span class="line">        value[i]=in[idx+i];</span><br><span class="line">    //2.</span><br><span class="line">    swap(value,threadIdx.x,mask,firstIdx,secondIdx);</span><br><span class="line">    //3.</span><br><span class="line">    for(int i=0;i&lt;SEGM;i++)</span><br><span class="line">        out[idx+i]=value[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="8-Quantization"><a href="#8-Quantization" class="headerlink" title="8.Quantization"></a>8.Quantization</h4><p>FloatFunctional会在prepare_qat之后activation_post_process会挂上FakeQuantize的hook，</p>
<p>然后就会是 <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/fake_quantize.py#L138-L156">https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/fake_quantize.py#L138-L156</a></p>
<p>observe去更新min_max min_max_observer里<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/observer.py#L432-L443">https://github.com/pytorch/pytorch/blob/v1.10.0/torch/ao/quantization/observer.py#L432-L443</a></p>
<p>用到_calculate_qparams<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/36449ea93134574c2a22b87baad3de0bf8d64d42/torch/ao/quantization/observer.py#L253-L322">https://github.com/pytorch/pytorch/blob/36449ea93134574c2a22b87baad3de0bf8d64d42/torch/ao/quantization/observer.py#L253-L322</a></p>
<p>算出scale和zero_point <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/observer.py#L310-L365">https://github.com/pytorch/pytorch/blob/master/torch/ao/quantization/observer.py#L310-L365</a></p>
<p>Pytorch量化流程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/299108528">https://zhuanlan.zhihu.com/p/299108528</a></p>
<p>美团技术实践： <a target="_blank" rel="noopener" href="https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html">https://tech.meituan.com/2022/09/22/yolov6-quantization-in-meituan.html</a></p>
<p>OBS OBQ 推导： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/656316235">https://zhuanlan.zhihu.com/p/656316235</a></p>
<p>GPTQ 进一步深挖： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/646210009">https://zhuanlan.zhihu.com/p/646210009</a></p>
<p>LU分解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/386954541">https://zhuanlan.zhihu.com/p/386954541</a></p>
<p>Cholesky分解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/387603571?utm_id=0">https://zhuanlan.zhihu.com/p/387603571?utm_id=0</a></p>
<p>FP8： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/619431625">https://zhuanlan.zhihu.com/p/619431625</a></p>
<p>GPTQ AWQ 综述： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/677899362">https://zhuanlan.zhihu.com/p/677899362</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/Python/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/Python/" class="post-title-link" itemprop="url">Python</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:57:50 / Modified: 19:59:16" itemprop="dateCreated datePublished" datetime="2025-02-22T19:57:50+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="1-Pytorch"><a href="#1-Pytorch" class="headerlink" title="1.Pytorch"></a>1.Pytorch</h4><p>2.0 feature：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torch._inductor.config.conv_1x1_as_mm = True # 默认 False</span><br><span class="line">torch._inductor.config.coordinate_descent_tuning = True # 默认 False</span><br><span class="line">torch._inductor.config.epilogue_fusion = False # 默认 True</span><br><span class="line">torch._inductor.config.coordinate_descent_check_all_directions = True # 默认 False</span><br><span class="line">torch._inductor.config.triton.unique_kernel_names = True # 默认 False</span><br><span class="line">torch._inductor.config.fx_graph_cache = True # 默认 True，减少编译时间</span><br><span class="line">torch._inductor.config.triton.cudagraphs=True # 默认 False</span><br><span class="line">torch._inductor.config.shape_padding = True # 默认 True</span><br><span class="line">torch._inductor.config.max_autotune = False # 默认 False</span><br><span class="line">torch._inductor.config.triton.cudagraph_trees = False # 默认 True</span><br><span class="line">torch._inductor.config.reorder_for_compute_comm_overlap = True # 默认 False</span><br><span class="line"></span><br><span class="line">torch._dynamo.config.automatic_dynamic_shapes = True # 默认 True</span><br><span class="line">torch._dynamo.config.cache_size_limit = 100000 # 默认 8</span><br><span class="line">torch._dynamo.config.optimize_ddp = False # 默认 True</span><br><span class="line"></span><br><span class="line">torch._functorch.config.enable_autograd_cache = True # 默认 False，减少编译时间</span><br><span class="line"></span><br><span class="line">torch.ops.aten._convert_weight_to_int4pack(weight_int32, inner_k_tiles)  # fp32 -&gt; uint8(kByte) -&gt; int4</span><br><span class="line">torch.ops.aten._weight_int4pack_mm(x, weight_int4pack, groupsize, scales_and_zeros)</span><br><span class="line"></span><br><span class="line">torch.profiler._utils._init_for_cuda_graphs()</span><br><span class="line">prof = torch.profiler.profile()</span><br></pre></td></tr></table></figure>

<p>调参 trick：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/25097993/answer/2718208647">https://www.zhihu.com/question/25097993/answer/2718208647</a></p>
<p>torch.fx.wrap：<a target="_blank" rel="noopener" href="https://vimsky.com/examples/usage/python-torch.fx.wrap-pt.html">https://vimsky.com/examples/usage/python-torch.fx.wrap-pt.html</a></p>
<p>torch.finfo：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43844342/article/details/116209999">https://blog.csdn.net/qq_43844342/article/details/116209999</a></p>
<p>DW：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/92134485">https://zhuanlan.zhihu.com/p/92134485</a></p>
<p>shape 广播机制：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/86997775">https://zhuanlan.zhihu.com/p/86997775</a></p>
<p>index_select：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/329104226">https://zhuanlan.zhihu.com/p/329104226</a></p>
<p>group卷积：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113928413">https://zhuanlan.zhihu.com/p/113928413</a></p>
<p>张量乘法：<a target="_blank" rel="noopener" href="https://blog.csdn.net/hottie_xiaomiao/article/details/124262405">https://blog.csdn.net/hottie_xiaomiao/article/details/124262405</a></p>
<p>替换模型任意层：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/356273702">https://zhuanlan.zhihu.com/p/356273702</a></p>
<p>scatter：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u010630669/article/details/105425572/">https://blog.csdn.net/u010630669/article/details/105425572/</a> 第一个1意味着每行有1个</p>
<p>hook：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/152314451">https://zhuanlan.zhihu.com/p/152314451</a></p>
<p>flops：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/65305385/answer/1699436281">https://www.zhihu.com/question/65305385/answer/1699436281</a></p>
<p>cudnn.benchmark：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/73711222">https://zhuanlan.zhihu.com/p/73711222</a></p>
<p>gather：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/110289027">https://zhuanlan.zhihu.com/p/110289027</a></p>
<p>np.take_long_axis &#x3D; torch.gather np.count_nonzero</p>
<p>np.argpartition, np.partition：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/546886845">https://zhuanlan.zhihu.com/p/546886845</a></p>
<p>np.min vs torch.min：<a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_37145472/article/details/94753866">https://blog.csdn.net/sinat_37145472/article/details/94753866</a></p>
<p>np -&gt; torch.topk：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/374269641">https://zhuanlan.zhihu.com/p/374269641</a></p>
<p>dynamo 基础接口：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620163218">https://zhuanlan.zhihu.com/p/620163218</a></p>
<p>debug memory leaks：<a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741/23?page=2">https://discuss.pytorch.org/t/how-to-debug-causes-of-gpu-memory-leaks/6741/23?page=2</a></p>
<p>BN1d &amp; 2d：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/664143309">https://zhuanlan.zhihu.com/p/664143309</a></p>
<p>源码编译：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">RUN cd pytorch &amp;&amp; \</span><br><span class="line">    CUDA_HOME=&quot;/usr/local/cuda&quot; \</span><br><span class="line">    CMAKE_PREFIX_PATH=&quot;$(dirname $(which conda))/../&quot; \</span><br><span class="line">    NCCL_INCLUDE_DIR=&quot;/usr/include/&quot; \</span><br><span class="line">    NCCL_LIB_DIR=&quot;/usr/lib/&quot; \</span><br><span class="line">    USE_SYSTEM_NCCL=1 \</span><br><span class="line">    USE_OPENCV=1 \</span><br><span class="line">    pip install --no-cache-dir -v .</span><br></pre></td></tr></table></figure>

<p>ncclUniqueId：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/614746112">https://zhuanlan.zhihu.com/p/614746112</a></p>
<p>Undefined Symbol: pybind11::detail::type_caster&lt;at::Tensor, void&gt;::load(pybind11::handle, bool)： <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/108041">https://github.com/pytorch/pytorch/issues/108041</a></p>
<p>CUDA Error：device not ready：<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/issues/85796Error%EF%BC%9A">https://github.com/pytorch/pytorch/issues/85796Error：</a> Open MPI has detected that this process has attempted to initialize MPI (via MPI_INIT or MPI_INIT_THREAD) more than once. This is erroneous.</p>
<p>Allow previously initialized：<a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/pull/105023/files">https://github.com/pytorch/pytorch/pull/105023/files</a></p>
<p>torch显存机制分析：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/424512257">https://zhuanlan.zhihu.com/p/424512257</a></p>
<p>repeat, repeat_interleave, tile的用法：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/474153365">https://zhuanlan.zhihu.com/p/474153365</a></p>
<p>offset, stride：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/101434655">https://zhuanlan.zhihu.com/p/101434655</a></p>
<p>torch fx 调研和实践：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/654834182">https://zhuanlan.zhihu.com/p/654834182</a></p>
<p>2.0 Graph Capture：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/644590863">https://zhuanlan.zhihu.com/p/644590863</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flag = torch.lt(x.sum(), 0)</span><br><span class="line">return flag * (x + 1) + (1 - flag) * (x - 1)</span><br></pre></td></tr></table></figure>

<p>gather &amp; scatter_：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/187401278">https://zhuanlan.zhihu.com/p/187401278</a></p>
<p>clone copy detach data：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/393041305">https://zhuanlan.zhihu.com/p/393041305</a></p>
<p>clone()可以返回完全相同的tensor,新的tensor开辟新的内存，tensor仍然留在计算图中</p>
<p>detach() 函数返回完全相同的tensor，新的tensor与旧的tensor共享内存，新的tensor脱离计算图(不会有梯度计算，梯度只流向原始的tensor，in-place操作会出错)；data() 是共用同一块内存，不在计算图中，但不安全，可以做in-place 操作</p>
<p>pad：<a target="_blank" rel="noopener" href="https://blog.csdn.net/jorg_zhao/article/details/105295686">https://blog.csdn.net/jorg_zhao/article/details/105295686</a></p>
<p>Function<strong>一般只定义一个操作</strong>，因为其无法保存参数，因此适用于激活函数、pooling等操作；Module是保存了参数，因此适合于定义一层，如线性层，卷积层，也适用于定义一个网络</p>
<p>TorchDynamo初探（字节码）：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/589115427">https://zhuanlan.zhihu.com/p/589115427</a></p>
<p>嵌套 list,tensor,ndarray 互转：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38703529/article/details/120216078">https://blog.csdn.net/qq_38703529/article/details/120216078</a></p>
<p>转置卷积：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/115070523">https://zhuanlan.zhihu.com/p/115070523</a></p>
<p>DDP详细流程：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/178402798">https://zhuanlan.zhihu.com/p/178402798</a></p>
<p>torch.utils.benchmark：<a target="_blank" rel="noopener" href="https://runebook.dev/zh/docs/pytorch/benchmark_utils">https://runebook.dev/zh/docs/pytorch/benchmark_utils</a></p>
<p>bn vs LN：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/113233908">https://zhuanlan.zhihu.com/p/113233908</a></p>
<p>bmm：<a target="_blank" rel="noopener" href="https://blog.csdn.net/foneone/article/details/103876519">https://blog.csdn.net/foneone/article/details/103876519</a></p>
<p>baddbmm：<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html">https://pytorch.org/docs/stable/generated/torch.baddbmm.html</a></p>
<p>tril_：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38406029/article/details/122059507">https://blog.csdn.net/qq_38406029/article/details/122059507</a></p>
<p>apply, _apply：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340453841">https://zhuanlan.zhihu.com/p/340453841</a></p>
<p>einsum：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361209187">https://zhuanlan.zhihu.com/p/361209187</a> 出现在右的自由索引与只出现在左的求和索引</p>
<p> <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/44954540">https://zhuanlan.zhihu.com/p/44954540</a></p>
<p>torch.compile：<a target="_blank" rel="noopener" href="https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab">https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit#heading=h.ivdr7fmrbeab</a></p>
<p>one of the variables needed for gradient computation has been modified by an inplace operation, good luck： 非流式显式喂了全0, 可以用torch.autograd.set_detect_anomaly(True)生成更具体的报错日志定位到代码行配合._version+shape判断具体是哪个变量</p>
<p>分布式训练代码层面：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/483656352">https://zhuanlan.zhihu.com/p/483656352</a></p>
<p>TFLite 只支持 same valid padding：<a target="_blank" rel="noopener" href="https://blog.csdn.net/ch206265/article/details/115298414">https://blog.csdn.net/ch206265/article/details/115298414</a></p>
<p>no_sync：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/250471767">https://zhuanlan.zhihu.com/p/250471767</a></p>
<h4 id="2-Python"><a href="#2-Python" class="headerlink" title="2.Python"></a>2.Python</h4><p>cpu 推理性能抖动：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/18022692140">https://zhuanlan.zhihu.com/p/18022692140</a></p>
<p>dataclass：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/419778289">https://zhuanlan.zhihu.com/p/419778289</a></p>
<p>conda 安装 nvcc：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/21676609751">https://zhuanlan.zhihu.com/p/21676609751</a></p>
<p>鸭子类型 &amp; 猴子补丁：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/577415129">https://zhuanlan.zhihu.com/p/577415129</a></p>
<p> <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1484390">https://cloud.tencent.com/developer/article/1484390</a></p>
<p>Python颜色：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_37844142/article/details/108282190">https://blog.csdn.net/qq_37844142/article/details/108282190</a></p>
<p>copy &amp; deepcopy：<a target="_blank" rel="noopener" href="https://blog.csdn.net/sodalife/article/details/89461030">https://blog.csdn.net/sodalife/article/details/89461030</a></p>
<p>pickle：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/152527979">https://zhuanlan.zhihu.com/p/152527979</a></p>
<p>typehints：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/519335398">https://zhuanlan.zhihu.com/p/519335398</a></p>
<p>类方法 &amp; 静态方法： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28010894">https://zhuanlan.zhihu.com/p/28010894</a></p>
<p>@staticmethod：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/389770470">https://zhuanlan.zhihu.com/p/389770470</a></p>
<p>静态方法放到类外边也不影响，主要是放在类里面给它一个作用域，方便管理</p>
<p>需要和类交互而不需要和实例交互时，就可以选择类方法</p>
<p>classmethod：定义<strong>操作类，而非操作实例</strong>的方法，最常用于定义<strong>备选构造方法</strong>。因此类方法的第一个参数是类本身而非实例</p>
<p>@classmethod：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/leviopku/article/details/100745811">https://blog.csdn.net/leviopku/article/details/100745811</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/20021164/answer/132619179">https://www.zhihu.com/question/20021164/answer/132619179</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/544021480">https://zhuanlan.zhihu.com/p/544021480</a></p>
<p>数据分析：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_35757704/article/details/89280715">https://blog.csdn.net/weixin_35757704/article/details/89280715</a></p>
<p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000015926584">https://segmentfault.com/a/1190000015926584</a></p>
<p>setuptools详解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/460233022">https://zhuanlan.zhihu.com/p/460233022</a></p>
<p>alternative 切换 python 版本：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/15948314">https://zhuanlan.zhihu.com/p/15948314</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_17105473/article/details/120909679">https://blog.csdn.net/qq_17105473/article/details/120909679</a></p>
<p>jupyter-remote：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36603177/article/details/132117549">https://blog.csdn.net/qq_36603177/article/details/132117549</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_40641725/article/details/114636779">https://blog.csdn.net/weixin_40641725/article/details/114636779</a></p>
<p>fire：<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_17550379/article/details/79943740">https://blog.csdn.net/qq_17550379/article/details/79943740</a></p>
<p>Nginx负载均衡： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/134220193">https://zhuanlan.zhihu.com/p/134220193</a></p>
<p>locals() -局部变量的字典 globals() -全局变量的字典</p>
<p>mpi4py基本使用： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/441259590">https://zhuanlan.zhihu.com/p/441259590</a></p>
<p>mpi4py多进程并行： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25332041">https://zhuanlan.zhihu.com/p/25332041</a></p>
<p>ClientOSError：<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/70448419/how-to-retry-async-requests-upon-clientoserror-errno-104-connection-reset-by">https://stackoverflow.com/questions/70448419/how-to-retry-async-requests-upon-clientoserror-errno-104-connection-reset-by</a></p>
<p>nohup：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_42840933/article/details/85780125">https://blog.csdn.net/weixin_42840933/article/details/85780125</a></p>
<p>函数默认参数一定要设定为不可变参数：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34395671">https://zhuanlan.zhihu.com/p/34395671</a></p>
<p>可变对象地址不变 所以改变a b也跟着变 不可变对象改变其中之一 地址发生变化 另一个则不会变</p>
<p>可变对象： list dict set 不可变对象： tuple string int float bool</p>
<p><code>*args, **kwargs</code> 这是一个非常常见的捕获<strong>不定长位置参数</strong>与<strong>不定长关键字参数</strong>的方式</p>
<p>repr vs str：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/542320058">https://zhuanlan.zhihu.com/p/542320058</a></p>
<p><strong>slot</strong>：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/578699693">https://zhuanlan.zhihu.com/p/578699693</a></p>
<p>值传递与引用传递：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/578152117">https://zhuanlan.zhihu.com/p/578152117</a></p>
<p>正则表达式：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91689180">https://zhuanlan.zhihu.com/p/91689180</a></p>
<p>functools.wraps：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/45535784">https://zhuanlan.zhihu.com/p/45535784</a></p>
<p>惰性求值：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/420508064">https://zhuanlan.zhihu.com/p/420508064</a></p>
<p>Unittest：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/466076772">https://zhuanlan.zhihu.com/p/466076772</a></p>
<p>traceback：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yuanfate/article/details/119916008">https://blog.csdn.net/yuanfate/article/details/119916008</a></p>
<p>mro：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/de7d38c84443">https://www.jianshu.com/p/de7d38c84443</a></p>
<p>super和父类没有实质性的关联；super(cls, inst)获得的是cls在inst的MRO列表中下一个类</p>
<p>super：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/636105836">https://zhuanlan.zhihu.com/p/636105836</a></p>
<p>dataclass： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/419778289">https://zhuanlan.zhihu.com/p/419778289</a></p>
<p>setuptools_scm：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/ExMan/p/10678565.html">https://www.cnblogs.com/ExMan/p/10678565.html</a></p>
<p>tensorrt api 变更：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/691157302">https://zhuanlan.zhihu.com/p/691157302</a></p>
<p>np.set_printoptions：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/659123626">https://zhuanlan.zhihu.com/p/659123626</a></p>
<p>纯 python 调用NCCL： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/689643675">https://zhuanlan.zhihu.com/p/689643675</a></p>
<p><code>ldconfig -p | grep nccl</code>查看服务器上的<code>nccl</code>库文件的位置</p>
<p><code>nm -D /lib/x86_64-linux-gnu/libnccl.so.2 </code>查看它有哪些符号可供外部使用</p>
<p>subprocess： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/664895510">https://zhuanlan.zhihu.com/p/664895510</a></p>
<p>判断点在多边形内： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/26934313/answer/3528590082">https://www.zhihu.com/question/26934313/answer/3528590082</a></p>
<p>conda install cuda -c nvidia</p>
<p>polygraphy： <a target="_blank" rel="noopener" href="https://blog.csdn.net/yitiaoxiaolu/article/details/136413877">https://blog.csdn.net/yitiaoxiaolu/article/details/136413877</a></p>
<p>trt-10.0 demo：<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/trt-samples-for-hackathon-cn/blob/master/cookbook/01-SimpleDemo/TensorRT-10.0/main_pytorch.py">https://github.com/NVIDIA/trt-samples-for-hackathon-cn/blob/master/cookbook/01-SimpleDemo/TensorRT-10.0/main_pytorch.py</a></p>
<p>python3 -m site 查看 sys.path.append 解决 no module named error</p>
<p>hash complexity： <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/51338089/complexity-of-the-internal-hash-function-in-python">https://stackoverflow.com/questions/51338089/complexity-of-the-internal-hash-function-in-python</a></p>
<p>pyi： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/665970213">https://zhuanlan.zhihu.com/p/665970213</a></p>
<p>deepcopy memo： <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/1950563/what-is-the-purpose-of-deepcopys-second-parameter-memo">https://stackoverflow.com/questions/1950563/what-is-the-purpose-of-deepcopys-second-parameter-memo</a></p>
<p>frozen&#x3D;True &amp; 元类只读保护： <a target="_blank" rel="noopener" href="https://blog.csdn.net/yuhengshi/article/details/127527765">https://blog.csdn.net/yuhengshi/article/details/127527765</a></p>
<p>wrap async to sync： <a target="_blank" rel="noopener" href="https://gist.github.com/phizaz/20c36c6734878c6ec053245a477572ec">https://gist.github.com/phizaz/20c36c6734878c6ec053245a477572ec</a></p>
<p>杀死进程： ps -ef | grep ‘python’ | awk ‘{print $2$}’ | xargs kill -9</p>
<p>seaborn：</p>
<p>displot 子图： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/451862785">https://zhuanlan.zhihu.com/p/451862785</a></p>
<p>color： <a target="_blank" rel="noopener" href="https://blog.csdn.net/Droke_Zhou/article/details/87479132">https://blog.csdn.net/Droke_Zhou/article/details/87479132</a></p>
<p>set_theme： <a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_35757704/article/details/119325859">https://blog.csdn.net/weixin_35757704/article/details/119325859</a></p>
<p>叠加多图： <a target="_blank" rel="noopener" href="https://geek-docs.com/seaborn/seaborn-questions/15_seaborn_how_can_i_overlay_two_graphs_in_seaborn.html">https://geek-docs.com/seaborn/seaborn-questions/15_seaborn_how_can_i_overlay_two_graphs_in_seaborn.html</a></p>
<p>中文乱码： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/566430362">https://zhuanlan.zhihu.com/p/566430362</a></p>
<p>seaborn 图表大小： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/451862785">https://zhuanlan.zhihu.com/p/451862785</a></p>
<p>pickle 深入理解： <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/695157912">https://zhuanlan.zhihu.com/p/695157912</a></p>
<p>itertools： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/581245124/answer/3363576932">https://www.zhihu.com/question/581245124/answer/3363576932</a></p>
<p>闭包计时器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import functools</span><br><span class="line">def recorder(a, b, n)：</span><br><span class="line">    def outer(func)：</span><br><span class="line">        @functools.wraps(func)</span><br><span class="line">        def record(*args, **kwargs)：</span><br><span class="line">            a.record()</span><br><span class="line">            for i in range(n)：</span><br><span class="line">                result = func(*args, **kwargs)</span><br><span class="line">            # torch.cuda.synchronize()</span><br><span class="line">            b.record()</span><br><span class="line">            torch.cuda.synchronize()</span><br><span class="line">            print(f&quot;&#123;func.__qualname__&#125;&quot;.replace(&quot;.forward&quot;, &quot;&quot;) + f&quot;average cost： &#123;(a.elapsed_time(b) / n)：.6f&#125; s&quot;)</span><br><span class="line">            return result</span><br><span class="line">        return record</span><br><span class="line">    return outer</span><br></pre></td></tr></table></figure>

<h4 id="3-Magic-Python"><a href="#3-Magic-Python" class="headerlink" title="3.Magic Python"></a>3.Magic Python</h4><p>美化 json：python -m json.tool demo.json</p>
<p>python -i 报错立马进入调试模式，如果希望脚本正确完成时自动退出，可以在脚本最后加上一行<code>__import__(&quot;os&quot;)._exit(0)</code></p>
<p>快速计算程序执行时长：python3 -m timeit ‘“-“.join(map(str, range(100)))’</p>
<p>想要使用 <code>-m</code> 的方式执行模块，有两种方式：</p>
<ul>
<li>第一种：以 <code>-m &lt;package&gt;</code> 的方式执行，只要在 package 下写一个 <code>__main__.py</code> 的文件即可。</li>
<li>第二种：以 <code>-m &lt;package.module&gt;</code> 的方式执行，只要在 module 的代码中，定义一个 main 函数，然后在最外层写入下面这段固定的代码</li>
</ul>
<p>创建一个 tar 压缩包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将 demo 文件夹压缩成 demo.tar </span><br><span class="line">$ python3 -m tarfile/zipfile -c demo.tar demo</span><br></pre></td></tr></table></figure>

<p>解压 tar 压缩包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 解压 demo.tar 到 demo_new 文件夹下</span><br><span class="line">$ python3 -m tarfile -e demo.tar demo_new </span><br></pre></td></tr></table></figure>

<p>连接列表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将可迭代对象（在这里指的是列表）串联起来，组成一个更大的可迭代对象，再用 list 将其转化为列表。</span><br><span class="line">&gt;&gt;&gt; from itertools import chain</span><br><span class="line">&gt;&gt;&gt; list(chain(list01, list02, list03))</span><br><span class="line"></span><br><span class="line">from heapq import merge # 还会高效排序</span><br><span class="line">&gt;&gt;&gt; list(merge(list01, list02, list03))</span><br></pre></td></tr></table></figure>

<p>字典合并：a | b</p>
<p>and or</p>
<p>fileinput：<a target="_blank" rel="noopener" href="https://github.com/iswbm/magic-python/blob/master/source/c07/c07_07.md">https://github.com/iswbm/magic-python/blob/master/source/c07/c07_07.md</a></p>
<p>读取文件中任意行：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import linecache</span><br><span class="line">&gt;&gt;&gt; linecache.getline(&#x27;/etc/passwd&#x27;, 4)</span><br><span class="line">found = False</span><br><span class="line">for thing in things:</span><br><span class="line">    if thing == other_thing:</span><br><span class="line">        found = True</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>

<p>更好的写法是使用 <code>any()</code> 函数，能够使这段代码变得更加清晰、简洁</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">found = any(thing == other_thing for thing in things)</span><br></pre></td></tr></table></figure>

<p>同理，当需要判断是否满足某一组集合中所有条件，也可以使用 <code>all()</code> 函数，只要有一个不满足条件，all 函数的结果就会立刻返回 False</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">found = all(thing == other_thing for thing in things)</span><br></pre></td></tr></table></figure>

<p>父类自己独有的，不想被子类继承的属性，可以在前面加两个下划线</p>
<p>默认参数不要是可变对象：<a target="_blank" rel="noopener" href="https://github.com/iswbm/magic-python/blob/master/source/c06/c06_02.md">https://github.com/iswbm/magic-python/blob/master/source/c06/c06_02.md</a> 理解入栈图</p>
<p>嵌套 for 循环写成单行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">list1 = range(1,3)</span><br><span class="line">list2 = range(4,6)</span><br><span class="line">list3 = range(7,9)</span><br><span class="line">for item1 in list1:</span><br><span class="line">    for item2 in list2:</span><br><span class="line">      	for item3 in list3:</span><br><span class="line">        	  print(item1+item2+item3)</span><br></pre></td></tr></table></figure>

<p>使用 itertools 这个库来实现更优雅易读的代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from itertools import product</span><br><span class="line">list1 = range(1,3)</span><br><span class="line">list2 = range(4,6)</span><br><span class="line">list3 = range(7,9)</span><br><span class="line">for item1,item2,item3 in product(list1, list2, list3):</span><br><span class="line">    print(item1+item2+item3)</span><br></pre></td></tr></table></figure>

<p>单行 for 死循环：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in iter(int, 1):pass</span><br></pre></td></tr></table></figure>

<p>iter有两种使用方法。</p>
<ul>
<li>将一个列表转化为一个迭代器。</li>
<li>而第二种方法，他接收一个 callable对象，和一个sentinel 参数。第一个对象会一直运行，直到它返回 sentinel 值才结束。</li>
</ul>
<p>那<code>int</code> 呢？</p>
<p>int 是一个内建方法。通过看注释，可以看出它是有默认值0的。你可以在console 模式下输入 <code>int()</code> 看看是不是返回0。</p>
<p>由于int() 永远返回0，永远返回不了1，所以这个 for 循环会没有终点，一直运行下去。</p>
<p>缓存：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">@functools.lru_cache(maxsize=None, typed=False)</span><br></pre></td></tr></table></figure>

<ul>
<li>maxsize：最多可以缓存多少个此函数的调用结果，如果为None，则无限制，设置为 2 的幂时，性能最佳</li>
<li>typed：若为 True，则不同参数类型的调用将分别缓存。</li>
</ul>
<p>流式读取超大文件：指定每次只读取固定大小的内容，比如下面的代码中，每次只读取 8kb 返回。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def read_from_file(filename, block_size = 1024 * 8):</span><br><span class="line">    with open(filename, &quot;r&quot;) as fp:</span><br><span class="line">        while True:</span><br><span class="line">            chunk = fp.read(block_size)</span><br><span class="line">            if not chunk:</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">            yield chunk</span><br></pre></td></tr></table></figure>

<p>上面的代码，功能上已经没有问题了，但是代码看起来还是有些臃肿。</p>
<p>借助偏函数 和 iter 函数可以优化一下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from functools import partial</span><br><span class="line"></span><br><span class="line">def read_from_file(filename, block_size = 1024 * 8):</span><br><span class="line">    with open(filename, &quot;r&quot;) as fp:</span><br><span class="line">        for chunk in iter(partial(fp.read, block_size), &quot;&quot;):</span><br><span class="line">            yield chunk</span><br></pre></td></tr></table></figure>

<p>海象运算符 代码如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def read_from_file(filename, block_size = 1024 * 8):</span><br><span class="line">    with open(filename, &quot;r&quot;) as fp:</span><br><span class="line">        while chunk := fp.read(block_size):</span><br><span class="line">            yield chunk</span><br><span class="line">&gt;&gt;&gt; from inspect import signature</span><br><span class="line">&gt;&gt;&gt; inspect.getsource(add)</span><br><span class="line">&gt;&gt;&gt; sig = signature(add) # # 获取函数签名</span><br></pre></td></tr></table></figure>

<p>打印内容输出到文件：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">with open(&#x27;test.log&#x27;, mode=&#x27;w&#x27;) as f:</span><br><span class="line">	print(&#x27;hello, python&#x27;, file=f, flush=True)</span><br><span class="line">&gt;&gt;&gt; &quot;a&quot;.ljust(10) </span><br><span class="line">&#x27;a         &#x27;</span><br><span class="line">&gt;&gt;&gt; &quot;a&quot;.rjust(10)</span><br><span class="line">&#x27;         a&#x27;</span><br><span class="line">&gt;&gt;&gt; &quot;a&quot;.center(10)</span><br><span class="line">&#x27;    a     &#x27;</span><br></pre></td></tr></table></figure>

<p>版本比较：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from distutils.version import LooseVersion, StrictVersion</span><br><span class="line">&gt;&gt;&gt; LooseVersion(&quot;2.3.1&quot;) &lt; LooseVersion(&quot;10.1.2&quot;)</span><br><span class="line">True</span><br><span class="line">&gt;&gt;&gt; StrictVersion(&quot;2.3.1&quot;) &lt; StrictVersion(&quot;10.1.2&quot;)</span><br><span class="line">True</span><br></pre></td></tr></table></figure>

<p>在 types 中有一个 MethodType，可以将普通方法与实例进行绑定：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import types</span><br><span class="line"></span><br><span class="line">class People:</span><br><span class="line">    def speak(self):</span><br><span class="line">        print(&quot;hello, world&quot;)</span><br><span class="line"></span><br><span class="line">def speak(self):</span><br><span class="line">    print(&quot;hello, python&quot;)</span><br><span class="line"></span><br><span class="line">p = People()</span><br><span class="line">p.speak = types.MethodType(speak, p)</span><br><span class="line">p.speak()</span><br></pre></td></tr></table></figure>

<p>捕获 &#x2F; 过滤 warning：上下文管理器，加上 <code>record=True</code> 会返回一个列表，列表里存放的是所有捕获到的警告</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&#x27;error&#x27;)    </span><br><span class="line"></span><br><span class="line">def do_warning():</span><br><span class="line">    warnings.warn(&quot;deprecated&quot;, DeprecationWarning)</span><br><span class="line"></span><br><span class="line">with warnings.catch_warnings(record=True) as w:</span><br><span class="line">    do_warning()</span><br><span class="line">    if len(w) &gt;0:</span><br><span class="line">        print(w[0].message)</span><br></pre></td></tr></table></figure>

<p>协程：遇到 IO 自动切换，等待 IO 结束期间去做别的事</p>
<p>事件循环：一个死循环</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">while True：</span><br><span class="line">	可执行的任务列表，已完成的任务列表 = 检查任务列表</span><br><span class="line">	for 就绪任务 in 可执行：</span><br><span class="line">		执行就绪任务</span><br><span class="line">	for 已完成 in 已完成：</span><br><span class="line">		移除任务列表中的已完成</span><br><span class="line">	如果 任务列表 中的任务都已完成，终止循环</span><br><span class="line"></span><br><span class="line">所以，如果在任务列表中遇到不可执行的，就会忽略它，继续放在任务列表里，比如说遭遇 IO 等待</span><br><span class="line"></span><br><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">async def func():</span><br><span class="line">	print(&quot;hello&quot;)</span><br><span class="line"></span><br><span class="line">result = func()</span><br><span class="line">执行协程函数仅创建对象，函数内部代码并未运行,必须交给事件循环来处理</span><br><span class="line"></span><br><span class="line">loop = asyncio.get_event_loop()</span><br><span class="line"># 任务放到任务列表，让事件循环去检测状态是否可执行</span><br><span class="line">loop.run_until_complete(result)</span><br><span class="line">or </span><br><span class="line">asyncio.run(result) # 3.7</span><br><span class="line"></span><br><span class="line">await + 可等待对象(协程对象, Future, Task -&gt; IO 等待)</span><br><span class="line">await 在等待的过程中依然会切到其他任务(但不是这之后的语句)去执行</span><br><span class="line"></span><br><span class="line">asyncio.create_task(协程对象, name=&#x27;&#x27;)创建对象的瞬间，会将任务添加到事件循环</span><br><span class="line">done, pending = await asyncio.wait([task1, task2], timeout=最多等多久)  # 返回值的集合，用得更多</span><br><span class="line"></span><br><span class="line">done, pending = asyncio.run(asyncio.wait([task1, task2]))  # 事件循环还未创建，报错</span><br><span class="line">done, pending = asyncio.run(asyncio.wait([co1, co2]))  # 放协程对象可以，因为内部会在创建完loop后加入包装成Task的协程</span><br><span class="line"></span><br><span class="line">Future是Task的基类,Task对象内部await结果的处理是基于Future对象来的</span><br><span class="line">future = loop.create_future()  # 什么也不干 永远也等不到结果</span><br><span class="line">await loop.create_task(set_after(future))  # set_after里给future赋值，就能等到了</span><br><span class="line">await future</span><br><span class="line"></span><br><span class="line">loop.run_in_executor(None, func)  # 返回一个 Future，None等价于用concurrent.futures创建一个Threadpoolexecutor，默认min(32,os.cpu_count()+4)</span><br><span class="line"></span><br><span class="line">import asyncio</span><br><span class="line">import uvloop  # fastapi使用</span><br><span class="line">asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())</span><br></pre></td></tr></table></figure>

<p>迭代器和生成器：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">## 迭代器</span><br><span class="line">for _ in + 可迭代对象</span><br><span class="line">共同 __iter__</span><br><span class="line"></span><br><span class="line">iter(可迭代对象) 得到 迭代器</span><br><span class="line">共同 __next__ __iter__</span><br><span class="line">超过长度得到 stopiteration </span><br><span class="line"></span><br><span class="line">迭代器的__iter__返回self</span><br><span class="line">使得整个迭代过程只需要迭代器就够了，直接让可迭代对象远离了循环</span><br><span class="line">意思就是不用再去遍历列表等容器类型了</span><br><span class="line"></span><br><span class="line">一个可迭代对象可以拥有任意多个迭代器</span><br><span class="line">a=[0] b=iter(a) c=iter(a) b==c False 不同内存地址</span><br><span class="line"></span><br><span class="line">迭代器协议：</span><br><span class="line">__iter__返回迭代器自身</span><br><span class="line">__next__每次返回一个迭代数据，如果没有数据则抛出StopIteration异常</span><br><span class="line"></span><br><span class="line">## 生成器</span><br><span class="line">yield 函数没有运行而是返回一个生成器对象</span><br><span class="line">生成器函数内的代码需要通过生成器对象来执行 生成器函数的作用和类差不多</span><br><span class="line">import inspect</span><br><span class="line">print(inspect.isfunction(gen))</span><br><span class="line">print(inspect.isgeneratorfunction(gen)) 含yield的函数就叫生成器函数</span><br><span class="line">print(inspect.isgenerator(g))</span><br><span class="line">生成器对象一定是迭代器，通过next调用，每次next都会返回yield后的结果，函数运行结束(遇到return或默认返回None)抛出StopIteration异常</span><br><span class="line"></span><br><span class="line">## 内部机制</span><br><span class="line">生成器函数并不直接运行，而是借助于生成器对象间接运行</span><br><span class="line">创建生成器对象的同时创建了帧对象，并且由生成器对象保持引用</span><br><span class="line">每次使用next调用生成器就是将生成器引用的帧对象入栈</span><br><span class="line">next返回，也就是遇到yield暂停的时候，就是将帧出栈</span><br><span class="line">直到迭代结束，帧最后一次出栈并且被销毁</span><br><span class="line"></span><br><span class="line">同步 普通函数</span><br><span class="line">调用：构建帧对象并入栈</span><br><span class="line">函数执行结束：帧出栈并销毁</span><br><span class="line"></span><br><span class="line">异步 生成器函数</span><br><span class="line">创建生成器：构建帧对象</span><br><span class="line">（多次）通过next触发执行：帧入栈</span><br><span class="line">（多次）遇到yield：帧出栈（保留）# 在帧出栈的时候可以插入其他任务的执行</span><br><span class="line">迭代结束：帧出栈并销毁</span><br><span class="line"></span><br><span class="line">生成器对象是一个迭代执行生成器函数的迭代器，针对一个包含很多代码的函数，分段执行其中的代码</span><br><span class="line">让一个函数可以多次迭代运行其中的代码才是生成器对象最根本的作用</span><br><span class="line"></span><br><span class="line">def a()：</span><br><span class="line">    print(&quot;here&quot;)</span><br><span class="line">    x = (yield) + 1</span><br><span class="line">    print(x)</span><br><span class="line">g = a()</span><br><span class="line">g.send(None) # here 停在yield处没有print的动作</span><br><span class="line">g.send(1) # 2</span><br><span class="line"></span><br><span class="line">结束协程也是靠异常实现的，GeneratorExit，except之后必须接return或者是循环时的break</span><br><span class="line"></span><br><span class="line">事件通常都是通过回调函数来处理的</span><br><span class="line"></span><br><span class="line">## yield from</span><br><span class="line">大大减少了被动协程的编码</span><br><span class="line">最末端遇到阻塞而不得不主动yield的协程叫做主动协程</span><br><span class="line">中间接收到下游传导而不得不跟随着yield的协程叫做被动协程</span><br><span class="line">用yield from统一yield</span><br><span class="line">总结：Task任务驱动器封装 + yield from 完全体改造 + YieldFromable</span><br><span class="line">协程的传染性也就是yield from的必要性；</span><br><span class="line">明白yield是怎么被隐藏的，从而理解协程的调用链末端发生了什么；</span><br><span class="line">理解协程的开端Task存在的必要性</span><br><span class="line">yield为了暂停 task驱动生成器 yield from链接生成器</span><br><span class="line"></span><br><span class="line">看到yield要等同于理解成函数要出栈了，暂时让出函数运行的权力</span><br><span class="line">yield from/await并不会执行一个出栈的过程，而是透传，层层深入直到遇到了yield</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/22/Confused/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/22/Confused/" class="post-title-link" itemprop="url">Confused</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2025-02-22 19:54:51 / Modified: 19:55:48" itemprop="dateCreated datePublished" datetime="2025-02-22T19:54:51+08:00">2025-02-22</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>女生素颜能漂亮到什么程度？ - 一个勺子的回答 - 知乎 <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/305888519/answer/636137849">https://www.zhihu.com/question/305888519/answer/636137849</a></p>
<p>你认为怎样的女孩才算是“可爱： <a target="_blank" rel="noopener" href="https://www.zhihu.com/question/266398947/answer/307244871">https://www.zhihu.com/question/266398947/answer/307244871</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">要论一个女孩可爱的话，首先我觉得她得美好——善良，聪明，宽容，勇敢，独立自主，既能圆滑处事，又有自己锋利的原则。</span><br><span class="line">我不认为那些——整天显得很单纯，很幼稚，说话有点嗲，娇滴滴的，很会撒娇，说自己是小仙女，看起来弱弱的想让人保护却又念着“要坚强哦”实际上内心空乏脆弱——的女孩子就是可爱，反而让我觉得她们做作，虚假。</span><br><span class="line">我觉得一个真正可爱的女孩，她会低调，会不屑于博得人的目光，会深沉，会不急于表现自己，会不刻意表现自己。而她的可爱，只有能与她站在一起的人，才能看到、懂得。</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/343516892/answers/updated">有一个大学同学，每年都会准时发生日快乐，连续6年，但是除了生日快乐也没有再多闲聊。这是什么情况? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/311405000/answers/updated">认识了6年的男孩，每一年都准时12点和我说生日快乐,然后闲聊几句就没了，他到底是什么想法? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/311497395/answers/updated">认识6年,几乎没什么见面, 但他却每年都12点和我说生日快乐, 然后闲聊几句,我和他有没有发展的可能? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/347423988/answers/updated">那个男孩又和我说了生日快乐，我们已经8年没见面了,他去了外国念书，这个算是爱情吗? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/438222464/answers/updated">想问问大家，那些一直不聊天，只在生日时发生日快乐或新年时发新年快乐的朋友是怎么想的呀？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/38430655/answers/updated">你曾喜欢过的男生很久没联系，但他记得你生日还发来祝福，他这是什么心理？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/376273564/answers/updated">一个女生记得你的生日说明什么？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/378186135/answers/updated">有个很长时间不联系的朋友祝我生日快乐该怎么回？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/441238603/answers/updated">现在表白还来得及吗? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/484871280/answers/updated?page=1">同班同学，平常没有什么交集，但是我生日那天晚上他给我发消息祝我生日快乐，是对我有意思吗? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/395184501/answers/updated">喜欢一个人，但许久未曾联系，是自己的高中同学，该怎么做？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/383615012/answers/updated">一个女孩在你生日0.00整给你发生日祝福，什么意思？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/497828504/answers/updated">有必要为很久未联系的异性朋友送生日祝福吗? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/507720488">异性好友每年会祝我生日快乐？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/507720488/answers/updated">好友每年都会会祝我生日快乐？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/530896617/answers/updated">一个女孩在你生日0.00整给你发生日祝福,什么意思？ - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/496929585/answers/updated">请问一个男生很多年了都会记得我的生日，这代表了什么呢? - 知乎 (zhihu.com)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/311497395/answers/updated">认识6年,几乎没什么见面, 但他却每年都12点和我说生日快乐, 然后闲聊几句,我和他有没有发展的可能?</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
